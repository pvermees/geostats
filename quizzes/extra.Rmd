---
title: "Additional (advanced) exercises"
output:
  pdf_document:
    includes:
      keep_tex: yes
    number_sections: yes
---

```{r setup, include = FALSE}
solution <- TRUE
results <- TRUE
multi <- TRUE
knitr::opts_chunk$set(echo=solution,include=results)
set.seed(1)
# to render:
# rmarkdown::render("~/Documents/Programming/R/geostats/quizzes/extra.Rmd")
# then parse into separate PDFs using parsextra.R
```

\newif\ifsol\sol`r ifelse(solution, 'true', 'false')`
\newif\ifmulti\multi`r ifelse(multi, 'true', 'false')`
\ifmulti\pagestyle{empty}\fi

Unlike the exercises in the notes, whose solutions are provided in
Chapter 19, you must attempt these questions yourself before I will
share my answer with you. You can do so during the live sessions,
either in person or via Zoom. Try, struggle, ask questions, and learn!

\ifmulti\clearpage\fi
# Brownian motion \label{sec:Brownian}

Write a function that simulates a random walk:

1. From a starting position of $x=0$ and $y=0$, move a virtual
particle by a distance of 1 in a random
direction. \label{it:Brownian1}

2. Repeat $n=1000$ times and plot the track of the particle as a
line. \label{it:Brownian2}

   ```{r brown, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   # brownian motion for one particle:
   brown <- function(xy0=rep(0,2),n=1000){
      xy <- matrix(rep(xy0,n),ncol=2,byrow=TRUE)
      d <- runif(n,min=0,max=2*pi) # random directions
      for (i in 2:n){
         dx <- cos(d[i])
         dy <- sin(d[i])
         xy[i,1] <- xy[i-1,1] + dx
         xy[i,2] <- xy[i-1,2] + dy
      }
      return(xy)
   }
   xy <- brown()
   plot(xy,type='l',xlab='x',ylab='y')
   ```

3. Repeat steps \ref{it:Brownian1} and \ref{it:Brownian2} for $N=500$
virtual particles and visualise their final positions on a scatter
plot. \label{it:Brownian3}

   ```{r browngauss, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   n <- 1000
   N <- 500
   xyf <- matrix(NA,nrow=N,ncol=2)
   for (i in 1:N){
      xyf[i,] <- brown(n=n)[n,]
   }
   plot(xyf,type='p',pch=16,xlab='x',ylab='y')
   ```

\ifmulti\clearpage\fi
# Diffusion \label{sec:diffusion}

Using the code from exercise \ref{sec:Brownian}:

1. Repeat step \ref{sec:Brownian}.\ref{it:Brownian3} for $n=250$, 500,
1000 and 2000 iterations and visualise the final positions on a
${2}\times{2}$ panel grid of scatter plots. Adjust the axis limits so
that all four panels are plotted at the same scale. \label{it:diffusion1}

   ```{r diffusion, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   cloud <- function(n=1000,N=500,plot=TRUE,...){
      xyf <- matrix(NA,nrow=N,ncol=2)
      for (i in 1:N){
         xyf[i,] <- brown(n=n)[n,]
      }
      if (plot) plot(xyf,pch=16,xlab='x',ylab='y',...)
      invisible(xyf) # returns the values without printing them at the console
   }

   par(mfrow=c(2,2),mar=rep(2,4))
   lims <- c(-100,100)
   ns <- c(250,500,1000,2000)
   for (n in ns){
      cloud(n=n,xlim=lims,ylim=lims)
      legend('topleft',legend=paste('n=',n),bty='n')
   }
```

2. Plot the marginal distributions of the $x$-values as kernel density
estimates and empirical cumulative distribution functions.
\label{it:diffusion2}

   ```{r marginals, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   par(mfrow=c(2,2),mar=rep(2,4))
   for (n in ns){
      xy <- cloud(n=n,plot=FALSE)
      d <- density(xy[,1],from=-100,to=100)
      plot(d,main='')
      rug(xy[,1])
      legend('topleft',legend=paste('n=',n),bty='n')
   }
   for (n in ns){
      xy <- cloud(n=n,plot=FALSE)
      d <- ecdf(xy[,1])
      plot(d,xlim=c(-100,100),main='',verticals=TRUE,pch=NA)
      legend('topleft',legend=paste('n=',n),bty='n')
   }
   ```

3. Visualise the bivariate datasets as 2-dimensional
KDEs. \label{it:diffusion3}

   ```{r kde2d, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   par(mfrow=c(2,2),mar=rep(2,4))
   lims <- c(-100,100)
   for (n in ns){
      xy <- cloud(n=n,plot=FALSE)
      d2d <- MASS::kde2d(x=xy[,1],y=xy[,2],lims=rep(lims,2))
      contour(d2d,xlim=lims,ylim=lims)
      legend('topleft',legend=paste('n=',n),bty='n')
   }
   ```

\ifmulti\clearpage\fi
# Summary statistics \label{sec:summary-statistics}

Using the code from the previous exercises:

1. Construct a table with the means, medians, standard deviations and
interquartile ranges of the synthetic datasets (x-values) of exercise
\ref{sec:diffusion}. \label{it:sumstat1}

   ```{r out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   nn <- length(ns)
   tab <- data.frame(means=rep(0,nn),medians=rep(0,nn),sds=rep(0,nn),iqrs=rep(0,nn))
   for (i in 1:nn){
      x <- cloud(n=ns[i],plot=FALSE)[,1]
      tab[i,'means'] <- mean(x)
      tab[i,'medians'] <- median(x)
      tab[i,'sds'] <- sd(x)
      tab[i,'iqrs'] <- IQR(x)
   }
   tab
   ```

2. Plot the four datasets as box plots. \label{it:sumstat2}

   ```{r box-plots, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   dat <- list()
   for (i in 1:nn){
      dat[[i]] <- cloud(n=ns[i],plot=FALSE)[,1]
   }
   names(dat) <- ns
   boxplot(dat)
   ```

\ifmulti\clearpage\fi
# Probability \label{sec:probability}

Suppose that a password needs to contain exactly 8 characters and must
include at least one lowercase letter, uppercase letter and digit. How
many such passwords are possible?

\ifsol Let $|x|$ stand for `the number of outcomes for $x$'. Then
\[
|\mbox{good passwords}| = |\mbox{all passwords}| - |\mbox{bad passwords}|
\]
where
\[
|\mbox{all passwords}| = (26+26+10)^8
\]

The number of bad passwords requires the \emph{inclusion-exclusion
principle}, which is similar to the additive rule of probability
(Equation 4.4 of the notes). Let $u$, $l$ and $d$ represent outcomes
_lacking_ uppercase letters, lowercase letters and digits,
respectively. Then we are looking for all possible combinations of
those three outcomes:
\[
|\mbox{bad passwords}| = \left| u \cup l \cup d \right|
\]
\[
= \left| u \cup l \right| + \left| d \right| - \left| u \cup l \cap d \right|
\]
\[
= \left| u \right| + \left| l \right| + \left| d \right|
- \left| u \cap l \right| - \left| \left(u \cup l\right) \cap d \right|
\]
\[
= \left| u \right| + \left| l \right| + \left| d \right|
- \left| u \cap l \right| - \left| u \cap d \right| - \left| l \cap d \right|
+ \left| u \cap l \cap d \right|
\]
where $\left| u \cap l \cap d \right| = 0$ so that
\[
|\mbox{good passwords}| = (26+26+10)^8 - (26+26)^8 - (26+10)^8 - (26+10)^8 + 26^8 + 26^8 + 10^8
\]

In \texttt{R}:
\fi

   ```{r prob1}
   len <- 8 # length of the password
   nupper <- 26 # A -> Z
   nlower <- 26 # a -> z
   ndigit <- 10 # 0 -> 9
   # total number of 8-character strings:
   N <- (nupper+nlower+ndigit)^len
   # remove all the passwords with no uppercase letter, lowercase letter, or digit:
   N <- N - (nlower+ndigit)^len - (nupper+ndigit)^len - (nlower+nupper)^len
   # But then you removed some passwords twice. You must add back all passwords with
   # no lowercase AND no uppercase, no lowercase AND no digit, or no uppercase AND no digit:
   N <- N + ndigit^len + nupper^len + nlower^len
   # hence the total number of passwords is
   print(N)
   ```

\ifmulti\clearpage\fi
# Bernoulli variables \label{sec:Bernoulli}

1. Draw a random number from a uniform distribution between 0 and 1
and store this number in a variable ($p$, say). \label{it:Bernoulli1}

   ```{r Bernoulli1}
   set.seed(2) # to get reproducible results (comment out for truly random values)
   p <- runif(1)
   print(p)
   ```

2. draw $n=20$ additional numbers from the same uniform distribution
and count how many of these values are less than $p$. \label{it:Bernoulli2}

   ```{r Bernoulli2}
   n <- 20
   r <- runif(n)
   print(sum(r<p))
   ```

3. Repeat steps \ref{it:Bernoulli1} and \ref{it:Bernoulli2} $N=100$
times, store the results in a vector ($x$, say), and plot the results
as a histogram. \label{it:Bernoulli3}

   ```{r Bernoulli3, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   N <- 100
   m <- matrix(runif(n*N),nrow=N,ncol=n)
   x <- rowSums(m<p)
   h <- hist(x,breaks=seq(from=-0.5,to=20.5,by=1))
   ```

4. Suppose that you did not know the value of $p$, then you could
estimate it (as a new variable $\hat{p}$, say) from the data $x$. To
this end, compute the binomial density (or `likelihood') of all values
in $x$ for $\hat{p}=0.5$ and sample size $n$. \label{it:Bernoulli4}

   ```{r Bernoulli4}
   phat <- 0.5 # example value
   d <- dbinom(x=x, size=n, prob=phat)
   ```

5. Now take the sum of the logarithms of the binomial likelihoods of
$x$. Call this sum $LL$ (for `log-likelihood'). \label{it:Bernoulli5}

   ```{r Bernoulli5}
   d <- dbinom(x=x, size=n, prob=phat, log=TRUE)
   LL <- sum(d)
   print(LL)
   ```

6. Repeat step 5 for a regularly spaced sequence of $\hat{p}$-values.
Then plot the resulting $LL$-values against those
$\hat{p}$-values. \label{it:Bernoulli6}

   ```{r Bernoulli6, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   phat <- seq(from=0.01,to=0.99,by=0.01) # values of 0 and 1 don't work
   np <- length(phat)
   LL <- rep(0,np)
   for (i in 1:np){
   d <- dbinom(x=x, size=n, prob=phat[i], log=TRUE)
   LL[i] <- sum(d)
   }
   plot(phat,LL,type='l')
   ```

7. Approximately which value of $\hat{p}$ corresponds to the maximum
value for $LL$? How does this compare to the true value of $p$?
\label{it:Bernoulli7}

   ```{r Bernoulli7}
   i <- which.max(LL)
   message('phat=',phat[i],', p=',signif(p,2))
   ```

By completing this exercise, you have numerically extended the
procedure described in Section 5.1 of the notes.

\ifmulti\clearpage\fi
# Type-2 errors \label{sec:binomialtest}

1. Draw a random number from a binomial distribution with $n=20$ and
$p=0.52$. \label{it:binomialtest1}

   ```{r binomialtest1}
   n <- 20
   p <- 0.52
   r <- rbinom(n=1,size=n,prob=p)
   ```

2. Apply a binomial test comparing $H_\circ: p=0.5$ with $H_a:
p\neq{0.5}$. Do the data pass the test? \label{it:binomialtest2}

   ```{r binomialtest2}
   alpha <- 0.05
   p0 <- 0.5
   h <- binom.test(x=r,n=n,p=p0)
   message('The data ',ifelse(h$p.value<alpha,'fail','pass'),' the test')
   ```

3. Repeat steps \ref{it:binomialtest1} and \ref{it:binomialtest2}
$N=1000$ times. What percentage of the datasets pass the test?
\label{it:binomialtest3}

   ```{r binomialtest3}
   N <- 1000
   r <- rbinom(n=N,size=n,prob=p)
   npass <- 0
   for (i in 1:N){
      h <- binom.test(x=r[i],n=n,p=p0)
      if (h$p.value>alpha) npass <- npass + 1
   }
   prob <- npass/N
   message(signif(100*prob,2),'%')
   ```

4. Repeat steps \ref{it:binomialtest1} through \ref{it:binomialtest3}
for $n=200$. \label{it:binomialtest4}

   ```{r binomialtest4}
   # write a function to avoid duplicate code:
   btest <- function(n=20,N=1000,p=0.52,p0=0.5,alpha=0.05){
      r <- rbinom(n=N,size=n,prob=p)
      npass <- 0
      for (i in 1:N){
         h <- binom.test(x=r[i],n=n,p=p0)
      	 if (h$p.value>alpha) npass <- npass + 1
      }
      return(npass/N)
   }

   # use the function
   prob <- btest(n=200,p=p)
   message(signif(100*prob,2),'%')
   ```

5. \label{it:binomialtest5} Repeat step \ref{it:binomialtest4} for a
range of values between $n=20$ and $n=2000$. Plot the probability of
rejection against $n$.
   ```{r out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   nt <- 10 # number of tests
   nn <- seq(from=20,to=20000,length.out=nt)
   pp <- rep(0,nt)
   for (i in 1:nt){
      pp[i] <- btest(n=nn[i],p=p)
   }
   plot(x=nn,y=pp,type='b',xlab='sample size',ylab='probability of type-2 error')
   ```

6. Compare the results of step \ref{it:binomialtest5} with a manual
calculation of the probability of committing a Type-2 error as a
function of sample size. \label{it:binomialtest6}
   ```{r binomialtest6, fig.width=10, fig.height=5}
   type2 <- function(n=20,p=0.52,p0=0.5){
      # rejection region of a two-sided binomial test for p=0.5
      ll <- qbinom(p=0.025,size=n,prob=p0) # lower limit
      ul <- qbinom(p=0.975,size=n,prob=p0) # upper limit
      # compute the probabilities of erroneously accepted values
      pu <- pbinom(q=ul,size=n,prob=p)
      pl <- pbinom(q=ll,size=n,prob=p)
      return(pu-pl)
   }
   
   pp2 <- rep(0,nt)
   for (i in 1:nt){
      pp2[i] <- type2(n=nn[i],p=p)
   }
   par(mfrow=c(1,2))
   plot(x=nn,y=pp2,type='b',xlab='sample size',ylab='probability of type-2 error')
   plot(x=pp,y=pp2,xlab='numerical',ylab='analytical')
   lines(range(pp),range(pp))
   ```

\ifmulti\clearpage\fi
# Confidence intervals \label{sec:binomialci}

1. Draw $N=10$ random numbers from a binomial distribution with
$p=0.55$ and $n=20$. Construct 95% confidence intervals for $p$ for
each of these numbers. \label{it:binomialci1}

   ```{r binomialci1}
   # define a function to avoid future duplication of code:
   bci <- function(n=20,N=10,p=0.5){
      out <- matrix(0,nrow=3,ncol=N)
      rownames(out) <- c('r','ll','ul')
      out['r',] <- rbinom(n=N,size=n,prob=p)
      for (i in 1:N){
         h <- binom.test(out['r',i],n=n)
         out[c('ll','ul'),i] <- h$conf.int
      }
      return(out)
   }

   # use the new function:
   ci <- bci(p=0.55)
   print(ci)
   ```

2. Plot these confidence intervals against $n$ as error bars using
\texttt{R}'s \texttt{arrows()} function. \label{it:binomialci2}

   ```{r out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   plotci <- function(ci,...){
      N <- ncol(ci)
      plot(1:N,ci['r',]/n,pch=16,ylim=c(0,1),bty='n',xlab='i',ylab='p',...)
      arrows(x0=1:N,y0=ci['ll',],y1=ci['ul',],length=0.1,angle=90,code=3)
   }
   plotci(ci)
   ```

3. How many of the confidence intervals in step \ref{it:binomialci2}
overlap with $p_0=0.5$?

   ```{r out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   # write function to avoid future duplication:
   bciexplorer <- function(n=20,N=10,p=0.5,p0=0.5){
      ci <- bci(n=n,N=N,p=p)
      overlap <- sum(p0>ci['ll',] & p0<ci['ul',])
      tit <- paste0(overlap,'/',N,' overlapping') # title
      plotci(ci,main=tit)
      lines(x=1:N,y=rep(p0,N),lty=2)
   }

   # use the function:
   bciexplorer(n=20,N=10,p=0.55,p0=0.5)
   ```

4. Repeat step \ref{it:binomialci2} for $n=200$. How many of these new
confidence intervals fall outside $p=0.5$? Do so again for $n=2000$.

   ```{r binomialci4, fig.width=10, fig.height=5}
   par(mfrow=c(1,2))
   bciexplorer(n=200,N=10,p=0.55,p0=0.5)
   bciexplorer(n=2000,N=10,p=0.55,p0=0.5)
   ```

\ifmulti\clearpage\fi
# Radioactive decay \label{sec:FT}

Radioactive decay is the process whereby a _parent_ isotope $P$
transforms to a _daughter_ isotope $D$ at a rate that is controlled by
the amount of parent and a _decay constant_ $\lambda_P$: \[
\partial{D}/\partial{t} = -\partial{P}/\partial{t} = \lambda_P{P} \]
   
The rate of decay ($\partial{D}/\partial{t}$) can be measured with a
Geyger counter. In this exercise, you will simulate radioactive decay
using a finite differences approximation to the decay equation:
\[
\Delta{D}/\Delta{t} = -\Delta{P}/\Delta{t} \approx \lambda_P{P}
\]
   
Then the expected number of parent atoms that are lost (and
daughter atoms that are gained) per time interval $\Delta{t}$
follows a Poisson distribution with parameter $\lambda_P P
\Delta{t}$.

Suppose that you start off with $P=10 000$ atoms of a parent isotope
with decay constant $\lambda_P=1/100$. Then simulate the evolution of
$P$, $D$ and $\Delta{D}$ from as a function of discretised time
from $t={0}\rightarrow{500}$ with $\Delta{t}=1$.

```{r fig.width=8, fig.height=4}
dt <- 1                             # time step
lamp <- 1/100                       # decay constant
tt <- seq(from=0,to=500,by=dt)      # time vector
nt <- length(tt)                    # number of steps
P <- rep(10000,nt)                  # parent atoms
D <- rep(0,nt)                      # daughter atoms
dD <- rep(0,nt)                     # lost daughter atoms per step
for (i in 1:(nt-1)){
   dD[i] <- rpois(n=1,lambda=lamp*P[i]*dt)
   D[i+1] <- D[i] + dD[i]
   P[i+1] <- P[i] - dD[i]
}
dD[nt] <- rpois(n=1,lambda=lamp*P[nt]*dt) # final step
par(mfrow=c(1,2))
cols <- c('red','blue')
plot(x=tt,y=P,type='l',col=cols[1],xlab='time',ylab='atoms')
lines(x=tt,y=D,col=cols[2])
legend('right',legend=c('P','D'),lty=1,col=cols,text.col=cols,bty='n')
plot(tt,dD,type='l',xlab='time',ylab=expression(Delta*'D'))
```

\ifmulti\clearpage\fi
# The normal distribution \label{sec:normal}

1. Modify the Brownian motion code of exercise \ref{sec:Brownian} so
that the random displacements are not defined by a unit circle but by
an ellipse with major axis $a=2$, minor axis $b=0.5$ and rotation
angle $\alpha=\pi/4$. See exercise 18.1.2 of the notes for the
relevant equations.

   ```{r out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   ell <- function(n=1,a=1,b=1,alpha=0){
      beta <- runif(n,min=0,max=2*pi) # random directions
      dx <- a*cos(alpha)*cos(beta) - b*sin(alpha)*sin(beta)
      dy <- a*sin(alpha)*cos(beta) + b*cos(alpha)*sin(beta)
      cbind(dx,dy)
   }

   # brownian motion for one particle:
   brown <- function(xy0=rep(0,2),n=1000,a=1,b=1,alpha=0){
      xy <- matrix(rep(xy0,n),ncol=2,byrow=TRUE)
      dxy <- ell(n=n,a=a,b=b,alpha=alpha)
      for (i in 2:n){
         xy[i,1] <- xy[i-1,1] + dxy[i,1]
         xy[i,2] <- xy[i-1,2] + dxy[i,2]
      }
      return(xy)
   }
   xy <- brown(a=2,b=0.5,alpha=pi/4)
   plot(xy,type='l',xlab='x',ylab='y',asp=1)

   n <- 1000
   N <- 500
   xyf <- matrix(NA,nrow=N,ncol=2)
   for (i in 1:N){
      xyf[i,] <- brown(n=n,a=2,b=0.5,alpha=pi/4)[n,]
   }
   plot(xyf,type='p',pch=16,xlab='x',ylab='y',asp=1)
   ```

2. Explore the effects of different values for $a$, $b$ and $\alpha$.
Brownian motion leads to diffusion, which gives rise to bivariate
normal distributions.  In exercise \ref{sec:diffusion}, this diffusion
was _isotropic_. The elliptical modification is one way to simulate
_anisotropic_ diffusion.

   ```{r out.width="50%", out.height="50%", fig.width=5, fig.height=5}
   cloud <- function(n=1000,N=500,plot=TRUE,a=1,b=1,alpha=0,...){
      xy <- matrix(NA,nrow=N,ncol=2)
      for (i in 1:N){
         xy[i,] <- brown(n=n,a=a,b=b,alpha=alpha)[n,]
      }
      # the optional plot argument will be used in exercise 11 below
      if (plot) plot(xy,pch=16,xlab='x',ylab='y',...)
      invisible(xy)
   }

   par(mfrow=c(2,2),mar=rep(2,4))
   lims <- c(-150,150)
   a <- c(1,2  ,2  ,2)
   b <- c(1,0.5,0.5,0.5)
   alpha <- c(0,pi/4,-pi/4,0)
   n <- 1000
   for (i in 1:4){
      cloud(n=n,xlim=lims,ylim=lims,a=a[i],b=b[i],alpha=alpha[i],asp=1)
      mtext(text=paste0('a=',a[i],', b=',b[i]),line=1)
      adeg <- signif(alpha[i]*180/pi,2)
      mtext(text=substitute(alpha~'='~a~degree,list(a=adeg)),line=0)
   }
   ```

\ifmulti\clearpage\fi
# Error propagation

Consider a bivariate normal distribution with the following
mean vector and covariance matrix:
$$
\mu = 
\left[
\begin{array}{@{}c@{}}
x = -2 \\
y = 3
\end{array}
\right] \mbox{,~}
\Sigma = 
\left[
\begin{array}{@{}cc@{}}
1 & -2\\
-2 & 5
\end{array}
\right]
$$

1. Predict $z = x + 2y$ and estimate its standard error.

   ```{r}
   mu <- c(-2,3)
   Sigma <- matrix(c(1,-2,-2,5),nrow=2,ncol=2)
   x <- mu[1]
   y <- mu[2]
   sx <- sqrt(Sigma[1,1])
   sy <- sqrt(Sigma[2,2])
   sxy <- Sigma[1,2]
   z <- x + 2*y
   # using equation 8.10:
   sz <- sqrt( sx^2 + (2*sy)^2 + 2*2*sxy )
   message('z=',signif(z,2),', s[z]=',signif(sz,2))
   ```

2. Draw $n=1000$ pairs of random numbers from the bivariate normal
distribution. Compute $z$ for each pair and calculate the mean and
standard error of the resulting vector. How does it compare with your
analytical solution?

   ```{r}
   # instead of loading the entire MASS package,
   # you can also call one of its functions like this:
   xy <- MASS::mvrnorm(n=1000,mu=mu,Sigma=Sigma)
   Z <- xy[,1] + 2*xy[,2]
   sZ <- sd(Z)
   message('z=',signif(mean(Z),2),', s[z]=',signif(sZ,2))
   ```

3. Repeat steps 1 and 2 for $z = x^2 y^3$.

   ```{r}
   # step 1:
   z <- (x^2)*(y^3)
   dzdx <- 2*x*y^3
   dzdy <- 3*(x^2)*(y^2)
   # using equation 8.8:
   sz <- sqrt( (dzdx*sx)^2 + (dzdy*sy)^2 + 2*dzdx*dzdy*sxy )
   message('z=',signif(z,2),', s[z]=',signif(sz,2))
   # step 2:
   xy <- MASS::mvrnorm(n=1000,mu=mu,Sigma=Sigma)
   Z <- (xy[,1]^2)*(xy[,2]^3)
   sZ <- sd(Z)
   message('z=',signif(mean(Z),2),', s[z]=',signif(sZ,2))
   ```

4. Repeat step 4 for
   \[
   \Sigma =
   \left[
   \begin{array}{@{}cc@{}}
   0.01 & -0.02\\
   -0.02 & 0.05
   \end{array}
   \right]
   \]

   ```{r}
   Sigma <- matrix(c(.01,-.02,-.02,.05),nrow=2,ncol=2)
   sx <- sqrt(Sigma[1,1])
   sy <- sqrt(Sigma[2,2])
   sxy <- Sigma[1,2]
   # step 1:
   z <- (x^2)*(y^3)
   dzdx <- 2*x*y^3
   dzdy <- 3*(x^2)*(y^2)
   # using equation 8.8:
   sz <- sqrt( (dzdx*sx)^2 + (dzdy*sy)^2 + 2*dzdx*dzdy*sxy )
   message('z=',signif(z,2),', s[z]=',signif(sz,2))
   # step 2:
   xy <- MASS::mvrnorm(n=1000,mu=mu,Sigma=Sigma)
   Z <- (xy[,1]^2)*(xy[,2]^3)
   sZ <- sd(Z)
   message('z=',signif(mean(Z),2),', s[z]=',signif(sZ,2))
   ```

   \ifsol
   Conclusion: the error propagation formulas work well for linear
   functions but not for strongly non-linear ones, unless the measurements
   are precise.
   \fi

\ifmulti\clearpage\fi
# Comparing distributions \label{sec:comparingdistributions}

1. Compare the marginal distributions of exercise \ref{sec:normal}
with a normal distribution using Q-Q plots.

   ```{r}
   a <- c(1,2 ,2 ,2)
   b <- c(1,0.5,0.5,0.5)
   alpha <- c(0,pi/4,-pi/4,0)
   n <- 1000
   N <- 100
   nc <- length(a)
   par(mfcol=c(2,nc),mar=rep(2,4))
   xy <- list() # storing the matrices in a list will save trouble later
   for (i in 1:nc){
      xy[[i]] <- cloud(n=n,N=N,a=a[i],b=b[i],alpha=alpha[i],plot=FALSE)
      qqnorm(xy[[i]][,1],main=paste0('x',i))
      qqnorm(xy[[i]][,2],main=paste0('y',i))
   }
   ```
   
2. Formalise the comparisons using a Kolmogorov-Smirnov test. See the
documentation of the `ks.test()` function for help. Note: you should
_normalise_ the data by specifying the mean and standard deviation of
the marginal distributions to the `ks.test()` function. See the
documentation for the ellipsis (`...`) in said documentation.

   ```{r}
   pvals <- matrix(0,nrow=2,ncol=nc)
   rownames(pvals) <- c('x','y')
   for (i in 1:nc){
      x <- xy[[i]][,1]
      y <- xy[[i]][,2]
      ksx <- ks.test(x=x,y='pnorm',mean=mean(x),sd=sd(x))
      ksy <- ks.test(x=y,y='pnorm',mean=mean(x),sd=sd(y))
      pvals['x',i] <- ksx$p.value
      pvals['y',i] <- ksy$p.value
   }
   signif(pvals,2)
   ```
   \ifsol
   Conclusion: the marginal distributions are Gaussian.
   \fi

\ifmulti\clearpage\fi
# Regression \label{sec:regression}

Consider the following data table:

| $x$ | $s[x]$ | $y$ | $s[y]$ | $r[x,y]$ |
|-----|--------|-----|--------|----------|
| 3 | 1 | 7 | 1 | 0.9 |
| 7 | 1 | 9 | 1 | 0.9 |
| 9 | 1 | 13 | 1 | 0.9 |
| 12 | 1 | 14 | 1 | 0.9 |
| 14 | 1 | 19 | 1 | -0.9 |


1. Fit a straight line through the $x$ and $y$ values, ignoring the
uncertainties ($s[x]$, $s[y]$) and error correlations
($r[x,y]$). Predict a 95% confidence interval for $y$ at $x=20$.

   ```{r fig.width=6,fig.height=5}
   x <- c(3,7,9,12,14)
   y <- c(7,9,13,14,19)
   lmfit <- lm(y ~ x)
   predict(lmfit,newdata=data.frame(x=20),interval='confidence')
   ```

2. Repeat the linear regression taking into account the analytical
uncertainties, using the `geostats` package's `york()` function.
Predict the $y$-value at $x=20$ and estimate its standard
error. Calculate the corresponding 95% confidence interval for $y$
using a t-distribution with $n-2=3$ degree of freedom. Note that
`york()` does not use formula notation. See `?york` for details.

   ```{r out.width='60%', fig.width=6,fig.height=5}
   sx <- rep(1,5)
   sy <- rep(1,5)
   rxy <- c(rep(0.9,4),-0.9)
   tab <- cbind(x,sx,y,sy,rxy)
   yfit <- geostats::york(tab)
   abline(lmfit$coefficients,lty=2)
   xnew <- 20
   ynew <- yfit$coef[1] + yfit$coef[2]*xnew
   # error propagation formula 8.10 of the notes:
   synew <- sqrt( yfit$cov[1,1] + yfit$cov[2,2]*xnew^2 + 2*xnew*yfit$cov[1,2])
   df <- length(x)-2
   ci <- ynew + qt(c(0.025,0.975),df=df)
   out <- signif(c(ynew,ci),5)
   names(out) <- c('fit','lwr','upr')
   out
   ```
   \ifsol
   Conclusion: `York regression' produces more accurate and
   more precise predictions.
   \fi

# Fractals

1. What is the fractal dimension of the following pattern?

   ![Rivera H-I fractal](rivera.jpg){width=50%}

   \ifsol
   ![Box counting](rivera.pdf)
   
   1 square block with side $d=1$ covers the entire pattern,
   and so do 7 square blocks of side $d=1/3$. Hence
   the fractal dimension is
   \[
   f =
   -\frac{\ln(7)-\ln(1)}{\ln(1/3)-\ln(1)} =
   \frac{\ln(7)}{\ln(3)} = 1.77
   \]
   \fi
   ```{r out.width='70%', fig.width=5, fig.height=4}
   d <- c(1,1/3)
   n <- c(1,7)
   fdim <- (log(n[2])-log(n[1]))/(log(d[2])-log(d[1]))
   plot(log(d),log(n),type='b',main=paste('slope=',signif(fdim,3)))
   ```

1. Using your code from exercise \ref{sec:Brownian}:

   (a) Create Brownian walk of 512 steps and plot the x-position of a
   virtual particle against time.

   (b) Subsample the vector of x-positions into $n$ equally spaced
   segments (for $n=2,4,8,16,...,5012$) and add up their respective
   lengths.

   (c) Plot the lengths of the curve against the corresponding number
   of segments on a log-log plot. What is the fractal dimension of the
   curve?

   (d) Repeat steps (a)--(c) several times to assess the robustness of
   the result.
   
   