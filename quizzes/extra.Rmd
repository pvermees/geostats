---
title: "Additional (advanced) exercises"
output:
  pdf_document:
    includes:
      keep_tex: yes
    number_sections: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(include=TRUE, echo=TRUE)
solution <- FALSE
# to render:
# rmarkdown::render("~/Documents/Programming/R/geostats/quizzes/extra.Rmd","pdf_document")
```
\newif\ifsol
\sol`r ifelse(solution, 'true', 'false')`

# Brownian motion \label{sec:Brownian}

Write a function that simulates a random walk:

1. From a starting position of $x=0$ and $y=0$, move a virtual
particle by a distance of 1 in a random
direction. \label{it:Brownian1}

2. Repeat $n=1000$ times and plot the track of the particle as a
line. \label{it:Brownian2}

```{r brown, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
# brownian motion for one particle:
brown <- function(xy0=rep(0,2),n=1000){
  xy <- matrix(rep(xy0,n),ncol=2,byrow=TRUE)
  d <- runif(n,min=0,max=2*pi) # random directions
  for (i in 2:n){
    dx <- cos(d[i])
    dy <- sin(d[i])
    xy[i,1] <- xy[i-1,1] + dx
    xy[i,2] <- xy[i-1,2] + dy
  }
  return(xy)
}
xy <- brown()
plot(xy,type='l',xlab='x',ylab='y')
```

3. Repeat steps \ref{it:Brownian1} and \ref{it:Brownian2} for $N=500$
virtual particles and visualise their final positions on a scatter
plot. \label{it:Brownian3}

```{r browngauss, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5, include=solution}
n <- 1000
N <- 500
xyf <- matrix(NA,nrow=N,ncol=2)
for (i in 1:N){
  xyf[i,] <- brown(n=n)[n,]
}
plot(xyf,type='p',pch=16,xlab='x',ylab='y')
```

# Diffusion \label{sec:diffusion}

Using the code from exercise \ref{sec:Brownian}:

1. Repeat step \ref{sec:Brownian}.\ref{it:Brownian3} for $n=250$, 500,
1000 and 2000 iterations and visualise the final positions on a
${2}\times{2}$ panel grid of scatter plots. Adjust the axis limits so
that all four panels are plotted at the same scale. \label{it:diffusion1}

```{r diffusion, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5, include=solution}
cloud <- function(n=1000,N=500,plot=TRUE,...){
  xyf <- matrix(NA,nrow=N,ncol=2)
  for (i in 1:N){
    xyf[i,] <- brown(n=n)[n,]
  }
  if (plot) plot(xyf,pch=16,xlab='x',ylab='y',...)
  invisible(xyf) # returns the values without printing them at the console
}

par(mfrow=c(2,2),mar=rep(2,4))
lims <- c(-100,100)
ns <- c(250,500,1000,2000)
for (n in ns){
  cloud(n=n,xlim=lims,ylim=lims)
  legend('topleft',legend=paste('n=',n),bty='n')
}
```

2. Plot the marginal distributions of the $x$-values as kernel density
estimates and empirical cumulative distribution functions.
\label{it:diffusion2}

```{r marginals, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5, include=solution}
par(mfrow=c(2,2),mar=rep(2,4))
for (n in ns){
  xy <- cloud(n=n,plot=FALSE)
  d <- density(xy[,1],from=-100,to=100)
  plot(d,main='')
  rug(xy[,1])
  legend('topleft',legend=paste('n=',n),bty='n')
}
for (n in ns){
  xy <- cloud(n=n,plot=FALSE)
  d <- ecdf(xy[,1])
  plot(d,xlim=c(-100,100),main='',verticals=TRUE,pch=NA)
  legend('topleft',legend=paste('n=',n),bty='n')
}
```

3. Visualise the bivariate datasets as 2-dimensional
KDEs. \label{it:diffusion3}

```{r kde2d, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5, include=solution}
par(mfrow=c(2,2),mar=rep(2,4))
lims <- c(-100,100)
for (n in ns){
  xy <- cloud(n=n,plot=FALSE)
  d2d <- MASS::kde2d(x=xy[,1],y=xy[,2],lims=rep(lims,2))
  contour(d2d,xlim=lims,ylim=lims)
  legend('topleft',legend=paste('n=',n),bty='n')
}
```

# Summary statistics \label{sec:summary-statistics}

Using the code from the previous exercises:

1. Construct a table with the means, medians, standard deviations and
interquartile ranges of the synthetic datasets (x-values) of exercise
\ref{sec:diffusion}. \label{it:sumstat1}

```{r summary-table, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5, include=solution}
nn <- length(ns)
tab <- data.frame(means=rep(0,nn),medians=rep(0,nn),sds=rep(0,nn),iqrs=rep(0,nn))
for (i in 1:nn){
  x <- cloud(n=ns[i],plot=FALSE)[,1]
  tab[i,'means'] <- mean(x)
  tab[i,'medians'] <- median(x)
  tab[i,'sds'] <- sd(x)
  tab[i,'iqrs'] <- IQR(x)
}
tab
```

2. Plot the four datasets as box plots. \label{it:sumstat2}

```{r box-plots, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5, include=solution}
dat <- list()
for (i in 1:nn){
  dat[[i]] <- cloud(n=ns[i],plot=FALSE)[,1]
}
names(dat) <- ns
boxplot(dat)
```

# Probability \label{sec:probability}

Suppose that a password needs to contain exactly 8 characters and must
include at least one lowercase letter, uppercase letter and digit. How
many such passwords are possible?

\ifsol The solution to this problem requires the
\emph{inclusion-exclusion principle}, which can be derived from the
additive rule of probability (Equation 4.4 of the notes). Let $u$, $l$
and $d$ represent outcomes containing uppercase letters, lowercase
letters and digits, respectively. Then we are looking for all possible
combinations of those three outcomes:
\[
\left| u \cup l \cup d \right|
\]
\[
= \left| u \cup l \right| + \left| d \right| - \left| u \cup l \cap d \right|
\]
\[
= \left| u \right| + \left| l \right| + \left| d \right|
- \left| u \cap l \right| - \left| \left(u \cup l\right) \cap d \right|
\]
\[
= \left| u \right| + \left| l \right| + \left| d \right|
- \left| u \cap l \right| - \left| u \cap d \right| - \left| l \cap d \right|
+ \left| u \cap l \cap d \right|
\]

In \texttt{R}:
\fi

```{r prob1, include=solution}
len <- 8 # length of the password
nupper <- 26 # A -> Z
nlower <- 26 # a -> z
ndigit <- 10 # 0 -> 9
# total number of 8-character strings:
N <- (nupper+nlower+ndigit)^len
# remove all the passwords with no uppercase letter, lowercase letter, or digit:
N <- N - (nlower+ndigit)^len - (nupper+ndigit)^len - (nlower+nupper)^len
# But then you removed some passwords twice. You must add back all passwords with
# no lowercase AND no uppercase, no lowercase AND no digit, or no uppercase AND no digit:
N <- N + ndigit^len + nupper^len + nlower^len
# hence the total number of passwords is
print(N)
```

# Bernoulli variables \label{sec:Bernoulli}

1. Draw a random number from a uniform distribution between 0 and 1
and store this number in a variable ($p$, say). \label{it:Bernoulli1}

```{r Bernoulli1, include=solution}
set.seed(2) # to get reproducible results (comment out for truly random values)
p <- runif(1)
print(p)
```

2. draw $n=20$ additional numbers from the same uniform distribution
and count how many of these values are less than $p$. \label{it:Bernoulli2}

```{r Bernoulli2, include=solution}
n <- 20
r <- runif(n)
print(sum(r<p))
```

3. Repeat steps \ref{it:Bernoulli1} and \ref{it:Bernoulli2} $N=100$
times, store the results in a vector ($x$, say), and plot the results
as a histogram. \label{it:Bernoulli3}

```{r Bernoulli3, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5, include=solution}
N <- 100
m <- matrix(runif(n*N),nrow=N,ncol=n)
x <- rowSums(m<p)
h <- hist(x,breaks=seq(from=-0.5,to=20.5,by=1))
```

4. Suppose that you did not know the value of $p$, then you could
estimate it (as a new variable $\hat{p}$, say) from the data $x$. To
this end, compute the binomial density (or `likelihood') of all values
in $x$ for $\hat{p}=0.5$ and sample size $n$. \label{it:Bernoulli4}

```{r Bernoulli4, include=solution}
phat <- 0.5 # example value
d <- dbinom(x=x, size=n, prob=phat)
```

5. Now take the sum of the logarithms of the binomial likelihoods of
$x$. Call this sum $LL$ (for `log-likelihood'). \label{it:Bernoulli5}

```{r Bernoulli5, include=solution}
d <- dbinom(x=x, size=n, prob=phat, log=TRUE)
LL <- sum(d)
print(LL)
```

6. Repeat step 5 for a regularly spaced sequence of $\hat{p}$-values.
Then plot the resulting $LL$-values against those
$\hat{p}$-values. \label{it:Bernoulli6}

```{r Bernoulli6, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5, include=solution}
phat <- seq(from=0.01,to=0.99,by=0.01) # values of 0 and 1 don't work
np <- length(phat)
LL <- rep(0,np)
for (i in 1:np){
  d <- dbinom(x=x, size=n, prob=phat[i], log=TRUE)
  LL[i] <- sum(d)
}
plot(phat,LL,type='l')
```

7. Approximately which value of $\hat{p}$ corresponds to the maximum
value for $LL$? How does this compare to the true value of $p$?
\label{it:Bernoulli7}

```{r Bernoulli7, include=solution}
i <- which.max(LL)
message('phat=',phat[i],', p=',signif(p,2))
```

By completing this exercise, you have numerically extended the
procedure described in Section 5.1 of the notes.

# Type-2 errors \label{sec:binomialtest}

1. Draw a random number from a binomial distribution with $n=20$ and
$p=0.52$. \label{it:binomialtest1}

```{r binomialtest1, include=solution}
n <- 20
p <- 0.52
r <- rbinom(n=1,size=n,prob=p)
```

2. Apply a binomial test comparing $H_\circ: p=0.5$ with $H_a:
p\neq{0.5}$. Do the data pass the test? \label{it:binomialtest2}

```{r binomialtest2, include=solution}
alpha <- 0.05
p0 <- 0.5
h <- binom.test(x=r,n=n,p=p0)
message('The data ',ifelse(h$p.value<alpha,'fail','pass'),' the test')
```

3. Repeat steps \ref{it:binomialtest1} and \ref{it:binomialtest2}
$N=1000$ times. What percentage of the datasets pass the test?
\label{it:binomialtest3}

```{r binomialtest3, include=solution}
N <- 1000
r <- rbinom(n=N,size=n,prob=p)
npass <- 0
for (i in 1:N){
  h <- binom.test(x=r[i],n=n,p=p0)
  if (h$p.value>alpha) npass <- npass + 1
}
npass/N
```

4. Repeat steps \ref{it:binomialtest1} through \ref{it:binomialtest3}
for $n=200$. \label{it:binomialtest4}

```{r binomialtest4, include=solution}
# write a function to avoid duplicate code:
btest <- function(n=20,N=1000,p=0.52,p0=0.5,alpha=0.05){
  r <- rbinom(n=N,size=n,prob=p)
  npass <- 0
  for (i in 1:N){
    h <- binom.test(x=r[i],n=n,p=p0)
    if (h$p.value>alpha) npass <- npass + 1
  }
  npass/N
}

# use the function
btest(n=200,p=p)
```

5. Repeat step \ref{it:binomialtest4} for a range of values from
$n=20$ to $n=2000$. Plot the probability of rejection against
$n$. \label{it:binomialtest5}

```{r binomialtest5, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
nt <- 10 # number of tests
nn <- seq(from=20,to=20000,length.out=nt)
pp <- rep(0,nt)
for (i in 1:nt){
  pp[i] <- btest(n=nn[i],p=p)
}
plot(x=nn,y=pp,type='b',xlab='sample size',ylab='probability of type-2 error')
```

6. Compare the results of step \ref{it:binomialtest5} with a manual
calculation of the probability of committing a Type-2 error as a
function of sample size. \label{it:binomialtest6}

```{r binomialtest6, include=solution, fig.width=10, fig.height=5}
type2 <- function(n=20,p=0.52,p0=0.5){
  # rejection region of a two-sided binomial test for p=0.5
  ll <- qbinom(p=0.025,size=n,prob=p0) # lower limit
  ul <- qbinom(p=0.975,size=n,prob=p0) # upper limit
  # compute the probabilities of erroneously accepted values
  return(pbinom(q=ul,size=n,prob=p)-pbinom(q=ll,size=n,prob=p))
}

pp2 <- rep(0,nt)
for (i in 1:nt){
  pp2[i] <- type2(n=nn[i],p=p)
}
par(mfrow=c(1,2))
plot(x=nn,y=pp2,type='b',xlab='sample size',ylab='probability of type-2 error')
plot(x=pp,y=pp2,xlab='numerical',ylab='analytical')
lines(range(pp),range(pp))
```

# Confidence intervals \label{sec:binomialci}

1. Draw $N=10$ random numbers from a binomial distribution with
$p=0.55$ and $n=20$. Construct 95% confidence intervals for $p$ for
each of these numbers. \label{it:binomialci1}

```{r binomialci1, include=solution}
# define a function to avoid future duplication of code:
bci <- function(n=20,N=10,p=0.5){
  out <- matrix(0,nrow=3,ncol=N)
  rownames(out) <- c('r','ll','ul')
  out['r',] <- rbinom(n=N,size=n,prob=p)
  for (i in 1:N){
    h <- binom.test(out['r',i],n=n)
    out[c('ll','ul'),i] <- h$conf.int
  }
  return(out)
}

# use the new function:
rci <- bci(p=0.55)
print(rci)
```

2. Plot these confidence intervals against $n$ as error bars using
\texttt{R}'s \texttt{arrows()} function. \label{it:binomialci2}

```{r binomialci2, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
plotrci <- function(rci,...){
  N <- ncol(rci)
  plot(1:N,rci['r',]/n,pch=16,ylim=c(0,1),bty='n',xlab='i',ylab='p',...)
  arrows(x0=1:N,y0=rci['ll',],y1=rci['ul',],length=0.1,angle=90,code=3)
}
plotrci(rci)
```

3. How many of the confidence intervals in step \ref{it:binomialci2}
overlap with $p_0=0.5$?

```{r binomialci3, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
# write function to avoid future duplication:
bciexplorer <- function(n=20,N=10,p=0.5,p0=0.5){
  rci <- bci(n=n,N=N,p=p)
  overlap <- sum(p0>rci['ll',] & p0<rci['ul',])
  tit <- paste0(overlap,'/',N,' overlapping') # title
  plotrci(rci,main=tit)
  lines(x=1:N,y=rep(p0,N),lty=2)
}

# use the function:
bciexplorer(n=20,N=10,p=0.55,p0=0.5)
```

4. Repeat step \ref{it:binomialci2} for $n=200$. How many of these new
confidence intervals fall outside $p=0.5$? Do so again for $n=2000$.

```{r binomialci4, include=solution, fig.width=10, fig.height=5}
par(mfrow=c(1,2))
bciexplorer(n=200,N=10,p=0.55,p0=0.5)
bciexplorer(n=2000,N=10,p=0.55,p0=0.5)
```

# Random sampling in 2D \label{sec:FT}

1. Generate two vectors ($x$ and $y$, say) of 1000 random numbers
between 0 and 500 and visualise them on a scatter plot. Add a grid of
lines at every 20 units of $x$ and $y$. \label{it:FT1}

```{r FT1, include=solution, out.width="70%", out.height="70%", fig.width=10, fig.height=10}
randgrid <- function(n=1000,m=0,M=500,bw=20){
  x <- runif(n,min=m,max=M)
  y <- runif(n,min=m,max=M)
  plot(x=x,y=y,pch=16)
  g <- seq(from=m,to=M,by=bw)
  ng <- length(g)
  matlines(x=rbind(rep(m,ng),rep(M,ng)),y=rbind(g,g),lty=3,col='black')
  matlines(x=rbind(g,g),y=rbind(rep(m,ng),rep(M,ng)),lty=3,col='black')
  xy <- cbind(x,y)
  invisible(xy)
}
randgrid()
```

2. Count the number of items falling in each square bin of the
graticule contained in the interval from $x=0$ to $x=100$ and from
$y=0$ to $y=100$. Plot these numbers as a histogram. \label{it:FT2}

```{r FT2, include=solution, out.width="50%", out.height="50%", fig.width=5, fig.height=5}
# this function solves the problem with a for-loop
# this is not the fastest solution, but it is the easiest:
bingrid <- function(xy,bw=20){
  breaks <- seq(from=0,to=100,by=bw)
  nbins <- length(breaks)-1
  counts <- rep(NA,nbins^2)
  for (i in 1:nbins){
    for (j in 1:nbins){
      xinbin <- (xy[,1]>=breaks[i] & xy[,1]<breaks[i+1])
      yinbin <- (xy[,2]>=breaks[j] & xy[,2]<breaks[j+1])
      ii <- (i-1)*nbins + j
      counts[ii] <- sum(xinbin & yinbin)
    }
  }
  hist(counts,breaks=max(counts)+1)
  invisible(counts)
}
bingrid(xy)
```

3. Calculate the mean and variance of the resulting dataset of
counts. Repeat steps \ref{it:FT1} and \ref{it:FT2} several times
before drawing conclusions.

```{r FT3, include=solution, fig.width=10, fig.height=5}
par(mfcol=c(2,3))
for (i in 1:3){
  counts <- bingrid(xy=randgrid())
  message('mean=',mean(counts),', var=',var(counts))
}
```

# 