\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Plotting data}{9}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:plotting}{{2}{9}{Plotting data}{chapter.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Anscombe's quartet of bivariate data pairs.\relax }}{9}{table.2.1}\protected@file@percent }
\newlabel{tab:anscombe}{{2.1}{9}{Anscombe's quartet of bivariate data pairs.\relax }{table.2.1}{}}
\newlabel{pg:anscombe}{{2}{9}{Plotting data}{table.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Anscombe's quartet shown as bivariate scatter plots. \relax }}{10}{figure.2.1}\protected@file@percent }
\newlabel{fig:anscombe}{{2.1}{10}{Anscombe's quartet shown as bivariate scatter plots.\\\relax }{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Categorical data}{10}{section.2.1}\protected@file@percent }
\newlabel{sec:categorical}{{2.1}{10}{Categorical data}{section.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Bar chart of clast counts. The vertical axis labels the number of objects counted in each category. The order of the categories along the horizontal axis is completely arbitrary and can be changed without loss of information.\relax }}{10}{figure.2.2}\protected@file@percent }
\newlabel{fig:clasts}{{2.2}{10}{Bar chart of clast counts. The vertical axis labels the number of objects counted in each category. The order of the categories along the horizontal axis is completely arbitrary and can be changed without loss of information.\relax }{figure.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Count data}{10}{section.2.2}\protected@file@percent }
\newlabel{sec:counts}{{2.2}{10}{Count data}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Histogram of magnitude $\geq {5.0}$ earthquakes per year between 1917 and 2016. The vertical axis labels the number of years. The horizontal axis shows the number of earthquakes. In contrast with Figure\nobreakspace  {}\ref  {fig:clasts}, the order of the four categories along the horizontal axis matters and cannot be changed without loss of information. Categorical data whose order matters are also known as \emph  {ordinal} data.\relax }}{11}{figure.2.3}\protected@file@percent }
\newlabel{fig:quakecounts}{{2.3}{11}{Histogram of magnitude $\geq {5.0}$ earthquakes per year between 1917 and 2016. The vertical axis labels the number of years. The horizontal axis shows the number of earthquakes. In contrast with Figure~\ref {fig:clasts}, the order of the four categories along the horizontal axis matters and cannot be changed without loss of information. Categorical data whose order matters are also known as \emph {ordinal} data.\relax }{figure.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Continuous data}{11}{section.2.3}\protected@file@percent }
\newlabel{sec:continuous}{{2.3}{11}{Continuous data}{section.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Two histograms of the same pH data, with the individual measurements marked as vertical ticks underneath. This is also known as a \emph  {rug plot}, and allows us to better assess the effect of bin width on the appearance of histograms. Histogram a) uses a bin width of 1\nobreakspace  {}pH unit whereas histogram b) uses a bin width of 0.5\nobreakspace  {}pH units. The two histograms look considerably different and it is not immediately clear which choice of bin width is best.\relax }}{12}{figure.2.4}\protected@file@percent }
\newlabel{fig:binwidth}{{2.4}{12}{Two histograms of the same pH data, with the individual measurements marked as vertical ticks underneath. This is also known as a \emph {rug plot}, and allows us to better assess the effect of bin width on the appearance of histograms. Histogram a) uses a bin width of 1~pH unit whereas histogram b) uses a bin width of 0.5~pH units. The two histograms look considerably different and it is not immediately clear which choice of bin width is best.\relax }{figure.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Two histograms of the pH data whose bin widths are the same, but whose bins have been offset by 0.25 pH units. This arbitrary decision strongly affects the appearance of the histogram.\relax }}{12}{figure.2.5}\protected@file@percent }
\newlabel{fig:binpos}{{2.5}{12}{Two histograms of the pH data whose bin widths are the same, but whose bins have been offset by 0.25 pH units. This arbitrary decision strongly affects the appearance of the histogram.\relax }{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The rug plot along the bottom axis represents three data points. The grey dashed lines mark rectangular boxes (`kernels') that are centred around each of these data points. The black step function is obtained by taking the sum of these boxes. This procedure removes the need to choose bin locations.\relax }}{13}{figure.2.6}\protected@file@percent }
\newlabel{fig:rectangles}{{2.6}{13}{The rug plot along the bottom axis represents three data points. The grey dashed lines mark rectangular boxes (`kernels') that are centred around each of these data points. The black step function is obtained by taking the sum of these boxes. This procedure removes the need to choose bin locations.\relax }{figure.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Rectangular KDE of the pH data, constructed using the same procedure as shown in Figure\nobreakspace  {}\ref  {fig:rectangles}. The area under this curve has been normalised to unity.\relax }}{13}{figure.2.7}\protected@file@percent }
\newlabel{fig:pHrectKDE}{{2.7}{13}{Rectangular KDE of the pH data, constructed using the same procedure as shown in Figure~\ref {fig:rectangles}. The area under this curve has been normalised to unity.\relax }{figure.2.7}{}}
\newlabel{eq:gaussiankernel}{{2.5}{13}{ii. Where to place the bins?}{equation.2.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Using a Gaussian kernel instead of a rectangular kernel on the three data points of Figure\nobreakspace  {}\ref  {fig:rectangles}. This produces a smooth KDE.\relax }}{14}{figure.2.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Gaussian KDE of the pH data. The continuous curve does more justice to the continuous data than the discrete step function of Figures\nobreakspace  {}\ref  {fig:binwidth}, \ref  {fig:binpos} or \ref  {fig:pHrectKDE}.\relax }}{14}{figure.2.9}\protected@file@percent }
\newlabel{fig:pHgaussKDE}{{2.9}{14}{Gaussian KDE of the pH data. The continuous curve does more justice to the continuous data than the discrete step function of Figures~\ref {fig:binwidth}, \ref {fig:binpos} or \ref {fig:pHrectKDE}.\relax }{figure.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Rug plots of the pH data with a) a kernel bandwidth of $h=0.1$; and b) a bandwidth of $h=1$. Using a narrow bandwidth undermooths the data, whereas a wide bandwidth produces an oversmoothed distribution.\relax }}{14}{figure.2.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Data transformations}{15}{section.2.4}\protected@file@percent }
\newlabel{sec:transformations}{{2.4}{15}{Data transformations}{section.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Rug plot of 20 clast size measurements. Even though all the measurements are strictly positive, the KDE extends into negative data space.\relax }}{15}{figure.2.11}\protected@file@percent }
\newlabel{fig:negativeKDE}{{2.11}{15}{Rug plot of 20 clast size measurements. Even though all the measurements are strictly positive, the KDE extends into negative data space.\relax }{figure.2.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces a) KDE of the clast size measurements, after applying a (natural) logarithmic transformation. Note how the distribution has become more symmetric compared to the linear scale of Figure\nobreakspace  {}\ref  {fig:negativeKDE}. b) The same KDE mapped back to linear scale. Unlike Figure\nobreakspace  {}\ref  {fig:negativeKDE}, the mapped distribution does not cross over into negative values.\relax }}{15}{figure.2.12}\protected@file@percent }
\newlabel{fig:logKDE}{{2.12}{15}{a) KDE of the clast size measurements, after applying a (natural) logarithmic transformation. Note how the distribution has become more symmetric compared to the linear scale of Figure~\ref {fig:negativeKDE}. b) The same KDE mapped back to linear scale. Unlike Figure~\ref {fig:negativeKDE}, the mapped distribution does not cross over into negative values.\relax }{figure.2.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Rug plot of 20 porosity measurements. Even though all the measurements are between 0 and 1, the KDE extends beyond these hard limits.\relax }}{16}{figure.2.13}\protected@file@percent }
\newlabel{fig:porosityKDE}{{2.13}{16}{Rug plot of 20 porosity measurements. Even though all the measurements are between 0 and 1, the KDE extends beyond these hard limits.\relax }{figure.2.13}{}}
\newlabel{eq:logit}{{2.6}{16}{Data transformations}{equation.2.4.6}{}}
\newlabel{eq:invlogit}{{2.7}{16}{Data transformations}{equation.2.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces a) KDE of the porosity data, after applying a logistic transformation. Note the two horizonal axes. The top axis marks the transformed values on a linear scale that extends from $-\infty $ to $+\infty $. The bottom axis is labeled by the actual porosity values on a non-linear scale that extends from 0 to 1. b) The same distribution mapped back to the 0 -- 1 interval.\relax }}{16}{figure.2.14}\protected@file@percent }
\newlabel{fig:logitKDE}{{2.14}{16}{a) KDE of the porosity data, after applying a logistic transformation. Note the two horizonal axes. The top axis marks the transformed values on a linear scale that extends from $-\infty $ to $+\infty $. The bottom axis is labeled by the actual porosity values on a non-linear scale that extends from 0 to 1. b) The same distribution mapped back to the 0 -- 1 interval.\relax }{figure.2.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Multivariate distributions}{17}{section.2.5}\protected@file@percent }
\newlabel{sec:multivariate}{{2.5}{17}{Multivariate distributions}{section.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Old Faithful eruption measurements. The dataset records 272 observations of 2 variables: the duration of each eruption, and the waiting time between them. Both variables are expressed in minutes. The lower left panel shows the bivariate measurements as grey circles. The contour lines represent a 2-dimensional KDE. The marginal distributions of the waiting times (top) and eruption durations (right) are shown as 1-dimensional KDEs.\relax }}{17}{figure.2.15}\protected@file@percent }
\newlabel{fig:KDE2D}{{2.15}{17}{Old Faithful eruption measurements. The dataset records 272 observations of 2 variables: the duration of each eruption, and the waiting time between them. Both variables are expressed in minutes. The lower left panel shows the bivariate measurements as grey circles. The contour lines represent a 2-dimensional KDE. The marginal distributions of the waiting times (top) and eruption durations (right) are shown as 1-dimensional KDEs.\relax }{figure.2.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Empirical cumulative distribution fuctions}{17}{section.2.6}\protected@file@percent }
\newlabel{sec:ECDF}{{2.6}{17}{Empirical cumulative distribution fuctions}{section.2.6}{}}
\newlabel{eq:ECDF}{{2.8}{17}{Empirical cumulative distribution fuctions}{equation.2.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Empirical cumulative distribution functions (ECDFs) of, from left to right: a) the pH data (whose KDE is shown in Figure\nobreakspace  {}\ref  {fig:pHgaussKDE}); b) the clast size data of Figure\nobreakspace  {}\ref  {fig:negativeKDE}; c) the porosity data of Figure\nobreakspace  {}\ref  {fig:porosityKDE}; and d) the eruption time data of Figure\nobreakspace  {}\ref  {fig:KDE2D}. Note that ECDFs are only applicable to 1-dimensional datasets. \relax }}{18}{figure.2.16}\protected@file@percent }
\newlabel{fig:ECDFs}{{2.16}{18}{Empirical cumulative distribution functions (ECDFs) of, from left to right: a) the pH data (whose KDE is shown in Figure~\ref {fig:pHgaussKDE}); b) the clast size data of Figure~\ref {fig:negativeKDE}; c) the porosity data of Figure~\ref {fig:porosityKDE}; and d) the eruption time data of Figure~\ref {fig:KDE2D}. Note that ECDFs are only applicable to 1-dimensional datasets.\\\relax }{figure.2.16}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Summary statistics}{19}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:summary-statistics}{{3}{19}{Summary statistics}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Location}{19}{section.3.1}\protected@file@percent }
\newlabel{sec:location}{{3.1}{19}{Location}{section.3.1}{}}
\newlabel{eq:mean}{{3.1}{19}{Location}{equation.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Rug plot (top) and ECDF (bottom) of the pH data. The mean (solid vertical line) is 5.00, the median (dashed line) is 5.10 and the mode (dotted line) is 5.25. The dash-dot line on the bottom panel marks halfway mark of the ECDF. The intersection of this line with the ECDF marks the median. All three measures of location are closely spaced together in the densest part of the dataset.\relax }}{20}{figure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Rug plot and ECDF of the clast size data. The mean (solid vertical line) is 3.19, the median (dashed line) is 2.1, and the mode (dotted line) is 0.45. There is a factor 7 difference between the smallest and largest measure of location for this dataset. The mean is strongly affected by the long `tail' of large outliers. Only 6 out of 20 clasts (30\%) are larger than the mean of the distribution and only 1 is (5\%) is smaller than the mode.\relax }}{21}{figure.3.2}\protected@file@percent }
\newlabel{fig:clastslocation}{{3.2}{21}{Rug plot and ECDF of the clast size data. The mean (solid vertical line) is 3.19, the median (dashed line) is 2.1, and the mode (dotted line) is 0.45. There is a factor 7 difference between the smallest and largest measure of location for this dataset. The mean is strongly affected by the long `tail' of large outliers. Only 6 out of 20 clasts (30\%) are larger than the mean of the distribution and only 1 is (5\%) is smaller than the mode.\relax }{figure.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Rug plot and ECDF of a) the porosity data (mean = 0.51, median = 0.55, mode = 0.97); and b) the geyser eruption data (mean = 3.49, median = 4.0, mode = 4.37). Both of these distributions are `bimodal', meaning that they have two `peaks' in the KDE, corresponding to two steep segments in the ECDFs. The dotted lines mark the highest one of them and ignores the other one. The mean and median fall in between the two modes and are not representative of the data. \relax }}{21}{figure.3.3}\protected@file@percent }
\newlabel{fig:porositylocation}{{3.3}{21}{Rug plot and ECDF of a) the porosity data (mean = 0.51, median = 0.55, mode = 0.97); and b) the geyser eruption data (mean = 3.49, median = 4.0, mode = 4.37). Both of these distributions are `bimodal', meaning that they have two `peaks' in the KDE, corresponding to two steep segments in the ECDFs. The dotted lines mark the highest one of them and ignores the other one. The mean and median fall in between the two modes and are not representative of the data.\\\relax }{figure.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Dispersion}{22}{section.3.2}\protected@file@percent }
\newlabel{sec:dispersion}{{3.2}{22}{Dispersion}{section.3.2}{}}
\newlabel{eq:stdev}{{3.3}{22}{Dispersion}{equation.3.2.3}{}}
\newlabel{eq:MAD}{{3.4}{22}{Dispersion}{equation.3.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces ECDF of the pH data with indication of the three quantiles, namely the 25 percentile, median and 75 percentile. The Interquartile Range (IQR) is defined as the difference between the 75 and 25 percentiles. This is 1 pH unit in this example. The standard deviation and Median Absolute Deviation are $s[x] = 0.7$ and MAD = 0.5 pH units, respectively.\relax }}{23}{figure.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Shape}{23}{section.3.3}\protected@file@percent }
\newlabel{sec:shape}{{3.3}{23}{Shape}{section.3.3}{}}
\newlabel{eq:skew}{{3.5}{23}{Shape}{equation.3.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces a) the frequency distributions of pH data is symmetric and is characterised by a near zero (but ever so slightly negative) skewness; b) the clast size measurements are positively skewed, i.e. they heavily lean towards small values with a heavy `tail' of higher values; c) finally, the distribution of Covid-19 death rates in the UK is negatively skewed: old people are much more likely to die of covid than young people. \relax }}{24}{figure.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Box-and-whisker plots}{24}{section.3.4}\protected@file@percent }
\newlabel{sec:boxplots}{{3.4}{24}{Box-and-whisker plots}{section.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces KDE (top) and box-and-whisker plot (bottom) of the clast size data. The width of the box marks the IQR. The whiskers extend to the minimum and maximum value, excluding a single outlier which, at $\sim $11cm, is more than 1.5 IQR larger than the third quartile (75 percentile). The median is offset towards the left hand side of the box, indicating the positive skewness of the datasets.\relax }}{24}{figure.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Probability}{25}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:probability}{{4}{25}{Probability}{chapter.4}{}}
\newlabel{eq:2H1T}{{4.2}{25}{Probability}{equation.4.0.2}{}}
\newlabel{eq:1116}{{4.3}{25}{Probability}{equation.4.0.3}{}}
\newlabel{page:multiplication}{{4}{25}{Probability}{equation.4.0.3}{}}
\newlabel{page:addition}{{4}{26}{Probability}{equation.4.0.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Permutations}{26}{section.4.1}\protected@file@percent }
\newlabel{sec:permutations}{{4.1}{26}{Permutations}{section.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Combinations}{27}{section.4.2}\protected@file@percent }
\newlabel{sec:combinations}{{4.2}{27}{Combinations}{section.4.2}{}}
\newlabel{eq:nchoosek}{{4.6}{28}{Combinations}{equation.4.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Conditional probability}{28}{section.4.3}\protected@file@percent }
\newlabel{sec:conditionalprobability}{{4.3}{28}{Conditional probability}{section.4.3}{}}
\newlabel{eq:PA&B}{{4.8}{28}{Conditional probability}{equation.4.3.8}{}}
\newlabel{eq:totalprob}{{4.9}{29}{Conditional probability}{equation.4.3.9}{}}
\newlabel{eq:Bayes}{{4.10}{29}{Conditional probability}{equation.4.3.10}{}}
\newlabel{eq:totalBayes}{{4.11}{29}{Conditional probability}{equation.4.3.11}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}The binomial distribution}{31}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:binomial}{{5}{31}{The binomial distribution}{chapter.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The probability mass function (PMF) for a binomial experiment with a 2/3 chance of success (and a 1/3 chance of failure) for five gold prospecting claims. The horizontal axis is labelled with the number of claims that produce gold. The vertical axis shows the probability of these respective outcomes.\relax }}{32}{figure.5.1}\protected@file@percent }
\newlabel{fig:goldbar}{{5.1}{32}{The probability mass function (PMF) for a binomial experiment with a 2/3 chance of success (and a 1/3 chance of failure) for five gold prospecting claims. The horizontal axis is labelled with the number of claims that produce gold. The vertical axis shows the probability of these respective outcomes.\relax }{figure.5.1}{}}
\newlabel{eq:binom}{{5.1}{32}{The binomial distribution}{equation.5.0.1}{}}
\newlabel{eq:CDF}{{5.2}{32}{The binomial distribution}{equation.5.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The cumulative distribution function (CDF) of the binomial distribution. This is the running sum of Figure\nobreakspace  {}\ref  {fig:goldbar}. The horizontal axis is labelled with the number of claims that produce gold. The vertical axis shows the cumulative probability of these respective outcomes. For example, the probability that two or fewer prospectors find gold is 21\%.\relax }}{32}{figure.5.2}\protected@file@percent }
\newlabel{fig:goldCDF}{{5.2}{32}{The cumulative distribution function (CDF) of the binomial distribution. This is the running sum of Figure~\ref {fig:goldbar}. The horizontal axis is labelled with the number of claims that produce gold. The vertical axis shows the cumulative probability of these respective outcomes. For example, the probability that two or fewer prospectors find gold is 21\%.\relax }{figure.5.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Parameter estimation}{33}{section.5.1}\protected@file@percent }
\newlabel{sec:binompar}{{5.1}{33}{Parameter estimation}{section.5.1}{}}
\newlabel{eq:Lbinom}{{5.3}{33}{Parameter estimation}{equation.5.1.3}{}}
\newlabel{eq:phat}{{5.4}{33}{Parameter estimation}{equation.5.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Hypothesis tests}{34}{section.5.2}\protected@file@percent }
\newlabel{sec:binomH}{{5.2}{34}{Hypothesis tests}{section.5.2}{}}
\newlabel{it:rejection}{{5}{34}{Hypothesis tests}{Item.42}{}}
\newlabel{it:decision}{{6}{34}{Hypothesis tests}{Item.43}{}}
\newlabel{pag:notaccepted}{{6}{35}{Hypothesis tests}{Item.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces a) PMF and b) CDF of a binomial null distribution with $p=2/3$ and $n=5$. The rejection region is marked in black on a). The horizontal dotted line in b) shows the $\alpha =0.05$ cutoff mark. The horizontal dashed line in b) marks the p-value for $k=2$, which is greater than 0.05. Therefore, the one-sided null hypothesis cannot be rejected.\relax }}{35}{figure.5.3}\protected@file@percent }
\newlabel{fig:1sidedbinomialrejection5}{{5.3}{35}{a) PMF and b) CDF of a binomial null distribution with $p=2/3$ and $n=5$. The rejection region is marked in black on a). The horizontal dotted line in b) shows the $\alpha =0.05$ cutoff mark. The horizontal dashed line in b) marks the p-value for $k=2$, which is greater than 0.05. Therefore, the one-sided null hypothesis cannot be rejected.\relax }{figure.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces a) the same PMF and b) CDF as Figure\nobreakspace  {}\ref  {fig:1sidedbinomialrejection5}. The black bar marks the rejection region for a two sided hypothesis test of $H_\circ : p=2/3$. Horizontal dotted lines mark the $\alpha /2=0.025$ and $(1-\alpha /2)=0.975$ cutoff marks. Their intersections with the CDF are shown as two vertical dotted lines. The horizontal dashed line marks $P(k\leq {2}|p=2/3)=0.2099$. This value falls between $\alpha /2$ and $(1-\alpha /2)$. Therefore $H_\circ $ cannot be rejected. \relax }}{36}{figure.5.4}\protected@file@percent }
\newlabel{fig:2sidedbinomialrejection5}{{5.4}{36}{a) the same PMF and b) CDF as Figure~\ref {fig:1sidedbinomialrejection5}. The black bar marks the rejection region for a two sided hypothesis test of $H_\circ : p=2/3$. Horizontal dotted lines mark the $\alpha /2=0.025$ and $(1-\alpha /2)=0.975$ cutoff marks. Their intersections with the CDF are shown as two vertical dotted lines. The horizontal dashed line marks $P(k\leq {2}|p=2/3)=0.2099$. This value falls between $\alpha /2$ and $(1-\alpha /2)$. Therefore $H_\circ $ cannot be rejected.\\\relax }{figure.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Statistical power}{36}{section.5.3}\protected@file@percent }
\newlabel{sec:power}{{5.3}{36}{Statistical power}{section.5.3}{}}
\newlabel{eq:1sidedbinomtest15}{{5.5}{37}{Statistical power}{equation.5.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces a) PMF and b) CDF of a binomial distribution with $p={2/3}$ and $n=15$. The $\alpha =0.05$ cutoff is shown as a horizontal dotted line and intersects the CDF at a point marked by a vertical dotted line. The vertical dashed lines mark the observation ($k=6$), which falls in the black area of the bar chart, and to the left of the dotted line in the CDF. Therefore, $H_\circ $ is rejected.\relax }}{37}{figure.5.5}\protected@file@percent }
\newlabel{fig:1sidedbinomialrejection15}{{5.5}{37}{a) PMF and b) CDF of a binomial distribution with $p={2/3}$ and $n=15$. The $\alpha =0.05$ cutoff is shown as a horizontal dotted line and intersects the CDF at a point marked by a vertical dotted line. The vertical dashed lines mark the observation ($k=6$), which falls in the black area of the bar chart, and to the left of the dotted line in the CDF. Therefore, $H_\circ $ is rejected.\relax }{figure.5.5}{}}
\newlabel{eq:2sidedbinomtest15}{{5.6}{37}{Statistical power}{equation.5.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces a) PMF and b) CDF of the binomial distribution with $p=2/3$ and $n=15$. The dotted lines mark the $\alpha /2=0.025$ and $(1-\alpha /2)=0.975$ levels and quantiles. The dashed lines mark the observed value ($k=6$, vertical) and its cumulative probability (0.0308, horizontal). $k=6$ falls outside the rejection region and $0.0308>\alpha /2$. Therefore $H_\circ $ cannot be rejected.\relax }}{38}{figure.5.6}\protected@file@percent }
\newlabel{fig:2sidedbinomialrejection15}{{5.6}{38}{a) PMF and b) CDF of the binomial distribution with $p=2/3$ and $n=15$. The dotted lines mark the $\alpha /2=0.025$ and $(1-\alpha /2)=0.975$ levels and quantiles. The dashed lines mark the observed value ($k=6$, vertical) and its cumulative probability (0.0308, horizontal). $k=6$ falls outside the rejection region and $0.0308>\alpha /2$. Therefore $H_\circ $ cannot be rejected.\relax }{figure.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces The PMF (a \& b) and CDF (c \& d) for a binomial distribution with $p=2/3$ and $n=30$. The vertical dashed lines mark the observation $k=12$, which falls outside the cutoff limits defined by the vertical dotted lines, and inside the rejection regions marked in black. The p-value of the one-sided test (a \& c) is 0.0025 and the p-value of the two-sided test (b \& d) is 0.005. Both of these values are less than $\alpha $ and therefore both null hypotheses are rejected.\relax }}{38}{figure.5.7}\protected@file@percent }
\newlabel{fig:binomialrejection30}{{5.7}{38}{The PMF (a \& b) and CDF (c \& d) for a binomial distribution with $p=2/3$ and $n=30$. The vertical dashed lines mark the observation $k=12$, which falls outside the cutoff limits defined by the vertical dotted lines, and inside the rejection regions marked in black. The p-value of the one-sided test (a \& c) is 0.0025 and the p-value of the two-sided test (b \& d) is 0.005. Both of these values are less than $\alpha $ and therefore both null hypotheses are rejected.\relax }{figure.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Type-I and type-II errors}{39}{section.5.4}\protected@file@percent }
\newlabel{sec:typeI&II}{{5.4}{39}{Type-I and type-II errors}{section.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces a) -- c) PMFs of the binomial null distribution with $p=2/3$ ($H_\circ $, grey) and alternative distributions with a) $p=2/5$, b) $p=1/5$ and c) $p=0$. Sample size is $n=5$ for all cases. d) -- f) CDFs for the null distribution (grey) and the alternative distribution (black). The horizontal dotted lines mark $\alpha =0.05$. Their intersections with the CDF of the null distribution are marked by vertical dotted lines. The areas to the left of these lines define the rejection region and are marked in black in the bar chart. The larger the sample size, the easier it is to reject $H_\circ $. The dashed horizontal lines mark the intersection of the rejection region with the CDF of the alternative distribution. These mark the power of the statistical test ($1-\beta $). Power clearly increases as the alternative distribution drifts away from the null distribution.\relax }}{40}{figure.5.8}\protected@file@percent }
\newlabel{fig:binompower1}{{5.8}{40}{a) -- c) PMFs of the binomial null distribution with $p=2/3$ ($H_\circ $, grey) and alternative distributions with a) $p=2/5$, b) $p=1/5$ and c) $p=0$. Sample size is $n=5$ for all cases. d) -- f) CDFs for the null distribution (grey) and the alternative distribution (black). The horizontal dotted lines mark $\alpha =0.05$. Their intersections with the CDF of the null distribution are marked by vertical dotted lines. The areas to the left of these lines define the rejection region and are marked in black in the bar chart. The larger the sample size, the easier it is to reject $H_\circ $. The dashed horizontal lines mark the intersection of the rejection region with the CDF of the alternative distribution. These mark the power of the statistical test ($1-\beta $). Power clearly increases as the alternative distribution drifts away from the null distribution.\relax }{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces a) -- c) PMFs of binomial distributions with $p=2/3$ ($H_\circ $, grey) and $p=2/5$ ($H_a$, black and white), for sample sizes of a) $n=5$, b) $n=15$ and c) $n=30$. d) -- f) CDFs for the null distribution (grey) and the alternative distribution (black). The horizontal dotted lines mark $\alpha =0.05$. Their intersections with the CDF of the null distribution are marked by vertical dotted lines. The area to the left of these lines define the rejection region and are marked in black in the bar chart. The larger the sample size, the easier it is to reject $H_\circ $. The dashed horizontal lines mark the intersection of the rejection region with the CDF of the alternative distribution. These mark the power of the statistical test ($1-\beta $). Power clearly increases with sample size.\relax }}{41}{figure.5.9}\protected@file@percent }
\newlabel{fig:binompower2}{{5.9}{41}{a) -- c) PMFs of binomial distributions with $p=2/3$ ($H_\circ $, grey) and $p=2/5$ ($H_a$, black and white), for sample sizes of a) $n=5$, b) $n=15$ and c) $n=30$. d) -- f) CDFs for the null distribution (grey) and the alternative distribution (black). The horizontal dotted lines mark $\alpha =0.05$. Their intersections with the CDF of the null distribution are marked by vertical dotted lines. The area to the left of these lines define the rejection region and are marked in black in the bar chart. The larger the sample size, the easier it is to reject $H_\circ $. The dashed horizontal lines mark the intersection of the rejection region with the CDF of the alternative distribution. These mark the power of the statistical test ($1-\beta $). Power clearly increases with sample size.\relax }{figure.5.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Pitfalls of statistical hypothesis testing}{41}{section.5.5}\protected@file@percent }
\newlabel{sec:pitfalls}{{5.5}{41}{Pitfalls of statistical hypothesis testing}{section.5.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces The most likely outcome of a binomial experiment with $p=2/3$ is $k/n=2/3(=0.67)$. This figure shows the p-values of six slightly different outcomes for a range of different sample sizes. The horizontal dotted line marks the 5\% significance level. No matter how little the observed $k/n$-ratio differs from 2/3, this difference always becomes `significant' given a large enough sample size.\relax }}{42}{figure.5.10}\protected@file@percent }
\newlabel{fig:binomnvsp}{{5.10}{42}{The most likely outcome of a binomial experiment with $p=2/3$ is $k/n=2/3(=0.67)$. This figure shows the p-values of six slightly different outcomes for a range of different sample sizes. The horizontal dotted line marks the 5\% significance level. No matter how little the observed $k/n$-ratio differs from 2/3, this difference always becomes `significant' given a large enough sample size.\relax }{figure.5.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Confidence intervals}{43}{section.5.6}\protected@file@percent }
\newlabel{sec:binomCI}{{5.6}{43}{Confidence intervals}{section.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces 95\% confidence interval for the binomial parameter $p$ given $k=2$ successful claims (dashed lines) out of $n=5$. The lower ($p=0.053$, grey) and upper ($p=0.85$) limits of the confidence interval are shown as a) PMFs and b) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels. \relax }}{44}{figure.5.11}\protected@file@percent }
\newlabel{fig:binomcik2n5}{{5.11}{44}{95\% confidence interval for the binomial parameter $p$ given $k=2$ successful claims (dashed lines) out of $n=5$. The lower ($p=0.053$, grey) and upper ($p=0.85$) limits of the confidence interval are shown as a) PMFs and b) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels.\\\relax }{figure.5.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces 95\% confidence interval for the binomial parameter $p$ given $k=4$ successful claims (dashed lines) out of $n=5$ trials. The lower ($p=0.284$, grey) and upper ($p=0.995$) limits of the confidence interval are shown as a) PMFs and b) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels. \relax }}{44}{figure.5.12}\protected@file@percent }
\newlabel{fig:binomcik4n5}{{5.12}{44}{95\% confidence interval for the binomial parameter $p$ given $k=4$ successful claims (dashed lines) out of $n=5$ trials. The lower ($p=0.284$, grey) and upper ($p=0.995$) limits of the confidence interval are shown as a) PMFs and b) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels.\\\relax }{figure.5.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces 95\% confidence interval for the binomial parameter $p$ given $k=12$ successful claims (dashed lines) out of $n=30$ trials. The lower ($p=0.23$, grey) and upper ($p=0.59$) limits of the confidence interval are shown as a) PMFs and b) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels. \relax }}{45}{figure.5.13}\protected@file@percent }
\newlabel{fig:binomcik12n30}{{5.13}{45}{95\% confidence interval for the binomial parameter $p$ given $k=12$ successful claims (dashed lines) out of $n=30$ trials. The lower ($p=0.23$, grey) and upper ($p=0.59$) limits of the confidence interval are shown as a) PMFs and b) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels.\\\relax }{figure.5.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces 95\% confidence intervals for a) $\hat  {p}=2/3$ and b) $\hat  {p}=1/5$ for different sample sizes $n$. The horizontal dotted lines mark the maximum likelihood estimates. Note the asymmetry of the confidence intervals, which always fall within the 0 to 1 range of the parameter.\relax }}{45}{figure.5.14}\protected@file@percent }
\newlabel{fig:binomcivsn}{{5.14}{45}{95\% confidence intervals for a) $\hat {p}=2/3$ and b) $\hat {p}=1/5$ for different sample sizes $n$. The horizontal dotted lines mark the maximum likelihood estimates. Note the asymmetry of the confidence intervals, which always fall within the 0 to 1 range of the parameter.\relax }{figure.5.14}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The Poisson distribution}{47}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:poisson}{{6}{47}{The Poisson distribution}{chapter.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The number of US earthquakes of magnitude 5.0 or greater per year between 1917 and 2016, with aftershocks removed. \relax }}{47}{figure.6.1}\protected@file@percent }
\newlabel{fig:declusteredquakes}{{6.1}{47}{The number of US earthquakes of magnitude 5.0 or greater per year between 1917 and 2016, with aftershocks removed.\\\relax }{figure.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces  Histogram of the earthquake counts shown in Figure\nobreakspace  {}\ref  {fig:declusteredquakes}. \nobreakspace  {}  \textbf  {mean: 5.43}  standard deviation: 2.50  \textbf  {variance: 6.24} \relax }}{47}{figure.6.2}\protected@file@percent }
\newlabel{fig:declusteredquakesperyear}{{6.2}{47}{Histogram of the earthquake counts shown in Figure~\ref {fig:declusteredquakes}.\\~\\ \textbf {mean: 5.43}\\ standard deviation: 2.50\\ \textbf {variance: 6.24} \relax }{figure.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Point-counting results for zircon in sand. Black squares mark zircons and grey circles other minerals. \relax }}{48}{figure.6.3}\protected@file@percent }
\newlabel{fig:zircons}{{6.3}{48}{Point-counting results for zircon in sand. Black squares mark zircons and grey circles other minerals.\\\relax }{figure.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces The number of zircons counted in each graticule of Figure\nobreakspace  {}\ref  {fig:zircons}. \relax }}{48}{figure.6.4}\protected@file@percent }
\newlabel{fig:zirconcounts}{{6.4}{48}{The number of zircons counted in each graticule of Figure~\ref {fig:zircons}.\\\relax }{figure.6.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces  Histogram of the zircon counts shown in Figure\nobreakspace  {}\ref  {fig:zirconcounts}. \nobreakspace  {}  \textbf  {mean: 3.50}  standard deviation: 1.85  \textbf  {variance: 3.40} \relax }}{48}{figure.6.5}\protected@file@percent }
\newlabel{fig:zirconhist}{{6.5}{48}{Histogram of the zircon counts shown in Figure~\ref {fig:zirconcounts}.\\~\\ \textbf {mean: 3.50}\\ standard deviation: 1.85\\ \textbf {variance: 3.40} \relax }{figure.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Probability mass function}{49}{section.6.1}\protected@file@percent }
\newlabel{sec:PMF}{{6.1}{49}{Probability mass function}{section.6.1}{}}
\newlabel{eq:poispmf}{{6.1}{49}{Probability mass function}{equation.6.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces PMF (a -- d) and CDF (e -- h) of the Poisson distribution for $\lambda =1$ (a, e); $\lambda =2$ (b, f); $\lambda =5$ (c, g); and $\lambda =10$ (d, h); as marked by the dashed line. \relax }}{49}{figure.6.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Binomial probability of 2 successes after $n$ trials for different values of $p$, where $np=5$. In the limit of $n \rightarrow {\infty }$ and $p \rightarrow {0}$, the cumulative probability $P(k\leq {2})$ converges to a value of 0.1246. This equals the probability of 2 successes under a Poisson distribution with $\lambda = 5$.\relax }}{49}{table.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Parameter estimation}{50}{section.6.2}\protected@file@percent }
\newlabel{sec:poispar}{{6.2}{50}{Parameter estimation}{section.6.2}{}}
\newlabel{eq:poislik}{{6.2}{50}{Parameter estimation}{equation.6.2.2}{}}
\newlabel{eq:poisLL}{{6.4}{50}{Parameter estimation}{equation.6.2.4}{}}
\newlabel{eq:lambda=k}{{6.8}{50}{Parameter estimation}{equation.6.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Hypothesis tests}{51}{section.6.3}\protected@file@percent }
\newlabel{sec:poishyp}{{6.3}{51}{Hypothesis tests}{section.6.3}{}}
\newlabel{it:poisl351sided}{{6}{51}{Hypothesis tests}{Item.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces a) PMF and b) CDF of a Poissonian null distribution with $\lambda =3.5$. The rejection region is marked in black on a). The horizontal dotted line in b) shows the $1-\alpha =0.95$ mark. The horizontal dashed line in the CDF marks the cumulative probability for $k=9$, which is greater than the 0.95 cutoff. Therefore the p-value is less than 0.05 and the one-sided null hypothesis is rejected.\relax }}{51}{figure.6.7}\protected@file@percent }
\newlabel{fig:poishyp}{{6.7}{51}{a) PMF and b) CDF of a Poissonian null distribution with $\lambda =3.5$. The rejection region is marked in black on a). The horizontal dotted line in b) shows the $1-\alpha =0.95$ mark. The horizontal dashed line in the CDF marks the cumulative probability for $k=9$, which is greater than the 0.95 cutoff. Therefore the p-value is less than 0.05 and the one-sided null hypothesis is rejected.\relax }{figure.6.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Multiple testing}{52}{section.6.4}\protected@file@percent }
\newlabel{sec:multipletesting}{{6.4}{52}{Multiple testing}{section.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Confidence intervals}{52}{section.6.5}\protected@file@percent }
\newlabel{sec:poisCI}{{6.5}{52}{Confidence intervals}{section.6.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces 95\% confidence interval for the Poisson parameter $\lambda $ given a single observation of $k=5$ events. The lower ($p=1.62$, grey) and upper ($p=11.67$) limits of the confidence interval are shown as a) PMFs and c) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels.\relax }}{53}{figure.6.8}\protected@file@percent }
\newlabel{fig:poisci}{{6.8}{53}{95\% confidence interval for the Poisson parameter $\lambda $ given a single observation of $k=5$ events. The lower ($p=1.62$, grey) and upper ($p=11.67$) limits of the confidence interval are shown as a) PMFs and c) CDFs. Dotted horizontal lines mark the 0.025 and 0.975 confidence levels.\relax }{figure.6.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces 95\% Poisson confidence intervals for all the years in the declustered earthquake database. The horizontal dotted line marks the average of all the years ($\hat  {\lambda }=5.43$). `outliers' are marked in black. \relax }}{53}{figure.6.9}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}The normal distribution}{55}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:normal}{{7}{55}{The normal distribution}{chapter.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces The time of day for all 543 magnitude 5.0 or greater earthquakes of Figure\nobreakspace  {}\ref  {fig:declusteredquakes}.\relax }}{56}{figure.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.1}The Central Limit Theorem}{56}{section.7.1}\protected@file@percent }
\newlabel{sec:CLT}{{7.1}{56}{The Central Limit Theorem}{section.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces The KDE and rug plot of 272 Old Faithful eruption durations is the marginal distribution of Figure\nobreakspace  {}\ref  {fig:KDE2D}. This distribution has two modes at 2 and 4.5 minutes.\relax }}{56}{figure.7.2}\protected@file@percent }
\newlabel{fig:CLTfaithful1}{{7.2}{56}{The KDE and rug plot of 272 Old Faithful eruption durations is the marginal distribution of Figure~\ref {fig:KDE2D}. This distribution has two modes at 2 and 4.5 minutes.\relax }{figure.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Collect $n=2$ randomly chosen events from the original dasaset of geyser eruptions and add their durations together. Repeat to create a new dataset of 500 values. The KDE of this distribution has not two but three modes at 4 ($=2\times {2}$), 6.5 ($=2+4.5$), and 9 ($=2\times {4.5}$) minutes, respectively.\relax }}{57}{figure.7.3}\protected@file@percent }
\newlabel{fig:CLTfaithful2}{{7.3}{57}{Collect $n=2$ randomly chosen events from the original dasaset of geyser eruptions and add their durations together. Repeat to create a new dataset of 500 values. The KDE of this distribution has not two but three modes at 4 ($=2\times {2}$), 6.5 ($=2+4.5$), and 9 ($=2\times {4.5}$) minutes, respectively.\relax }{figure.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Collect $n=3$ randomly chosen events from the geyser eruption dataset and add their durations together. Repeat 500 times to create a third dataset. The KDE of this distributions has four visible modes, including peaks at 6 ($=3\times {2}$), 8.5 ($=2\times {2}+4.5$), 11 ($=2+2\times {4.5}$) and 13.5 ($=3\times {4.5}$) minutes. \relax }}{57}{figure.7.4}\protected@file@percent }
\newlabel{fig:CLTfaithful3}{{7.4}{57}{Collect $n=3$ randomly chosen events from the geyser eruption dataset and add their durations together. Repeat 500 times to create a third dataset. The KDE of this distributions has four visible modes, including peaks at 6 ($=3\times {2}$), 8.5 ($=2\times {2}+4.5$), 11 ($=2+2\times {4.5}$) and 13.5 ($=3\times {4.5}$) minutes. \relax }{figure.7.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Taking 500 samples of $n=10$ randomly selected eruptions and summing their durations produces a fourth dataset whose KDE has a single mode with symmetric tails towards lower and higher values.\relax }}{57}{figure.7.5}\protected@file@percent }
\newlabel{fig:CLTfaithful10}{{7.5}{57}{Taking 500 samples of $n=10$ randomly selected eruptions and summing their durations produces a fourth dataset whose KDE has a single mode with symmetric tails towards lower and higher values.\relax }{figure.7.5}{}}
\newlabel{eq:gauss}{{7.6}{57}{The Central Limit Theorem}{equation.7.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Galton's bean machine is a mechanical device that simulates additive physical processes. It consists of a triangular arrangement of pegs above a linear array of containers. When a bead enters the machine from the top, it bounces off the pegs on its way down to the containers. The probability of bouncing to the left is the same as the probability of bouncing to the right. After $n$ bounces, the bead lands in one of the containers, forming a bell shaped (binomial) distribution. With increasing $n$, this distribution converges to a Gaussian form.\relax }}{58}{figure.7.6}\protected@file@percent }
\newlabel{fig:galtonsbeanmachine}{{7.6}{58}{Galton's bean machine is a mechanical device that simulates additive physical processes. It consists of a triangular arrangement of pegs above a linear array of containers. When a bead enters the machine from the top, it bounces off the pegs on its way down to the containers. The probability of bouncing to the left is the same as the probability of bouncing to the right. After $n$ bounces, the bead lands in one of the containers, forming a bell shaped (binomial) distribution. With increasing $n$, this distribution converges to a Gaussian form.\relax }{figure.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}The multivariate normal distribution}{58}{section.7.2}\protected@file@percent }
\newlabel{sec:multinorm}{{7.2}{58}{The multivariate normal distribution}{section.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Four synthetic, bivariate, continuous distributions, defined by black areas and lines. White areas are excluded from the distributions. \relax }}{58}{figure.7.7}\protected@file@percent }
\newlabel{fig:pop2d}{{7.7}{58}{Four synthetic, bivariate, continuous distributions, defined by black areas and lines. White areas are excluded from the distributions.\\\relax }{figure.7.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces It is easy to recognise the probability distributions of Figure\nobreakspace  {}\ref  {fig:pop2d} in the scatter plots of 100 random points selected from them. \relax }}{59}{figure.7.8}\protected@file@percent }
\newlabel{fig:rand2d}{{7.8}{59}{It is easy to recognise the probability distributions of Figure~\ref {fig:pop2d} in the scatter plots of 100 random points selected from them.\\\relax }{figure.7.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.9}{\ignorespaces Four scatter plots with 200 points, each of which represents the sum of a random sample of 100 points drawn from Figure\nobreakspace  {}\ref  {fig:pop2d}. \relax }}{59}{figure.7.9}\protected@file@percent }
\newlabel{fig:rand2sum}{{7.9}{59}{Four scatter plots with 200 points, each of which represents the sum of a random sample of 100 points drawn from Figure~\ref {fig:pop2d}.\\\relax }{figure.7.9}{}}
\newlabel{eq:2dgauss}{{7.7}{59}{The multivariate normal distribution}{equation.7.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.10}{\ignorespaces The main panel shows population 4 of Figure\nobreakspace  {}\ref  {fig:rand2sum} as grey circles, and the best fittin bivariate Gaussian distribution as contours. The side panels show the marginal distributions of the $X$- and $Y$-variable, which are both univariate Gaussian. The means and standard deviations of the marginal distributions equal the means and the standard deviations of the bivariate distribution. The bivariate distribution has a fifth parameter, the covariance $\sigma _{x,y}$, which controls the angle at which the elliptical contours are rotated relative to the axes of the diagram. The significance of these parameters is further explored in Section\nobreakspace  {}\ref  {sec:normalproperties}.\relax }}{60}{figure.7.10}\protected@file@percent }
\newlabel{fig:norm2dmarginal}{{7.10}{60}{The main panel shows population 4 of Figure~\ref {fig:rand2sum} as grey circles, and the best fittin bivariate Gaussian distribution as contours. The side panels show the marginal distributions of the $X$- and $Y$-variable, which are both univariate Gaussian. The means and standard deviations of the marginal distributions equal the means and the standard deviations of the bivariate distribution. The bivariate distribution has a fifth parameter, the covariance $\sigma _{x,y}$, which controls the angle at which the elliptical contours are rotated relative to the axes of the diagram. The significance of these parameters is further explored in Section~\ref {sec:normalproperties}.\relax }{figure.7.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Properties}{60}{section.7.3}\protected@file@percent }
\newlabel{sec:normalproperties}{{7.3}{60}{Properties}{section.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.11}{\ignorespaces PDFs of the univariate normal distribution for different values of $\mu $ and $\sigma $. $\mu $ controls the position and $\sigma $ the width of the distribution. By definition, the area under the PDF always remains the same (i.e. $\DOTSI \intop \ilimits@ _{-\infty }^{+\infty }f(x)\nobreakspace  {}dx = 1$). \relax }}{60}{figure.7.11}\protected@file@percent }
\newlabel{fig:musigma}{{7.11}{60}{PDFs of the univariate normal distribution for different values of $\mu $ and $\sigma $. $\mu $ controls the position and $\sigma $ the width of the distribution. By definition, the area under the PDF always remains the same (i.e. $\int _{-\infty }^{+\infty }f(x)~dx = 1$).\\\relax }{figure.7.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.12}{\ignorespaces PDF (a) and CDF (b) of the normal distribution. The $\mu \pm \sigma $ and $\mu \pm {2}\sigma $ intervals cover $\sim $68\% and $\sim $95\% of the distribution, respectively. \relax }}{61}{figure.7.12}\protected@file@percent }
\newlabel{fig:2sigma}{{7.12}{61}{PDF (a) and CDF (b) of the normal distribution. The $\mu \pm \sigma $ and $\mu \pm {2}\sigma $ intervals cover $\sim $68\% and $\sim $95\% of the distribution, respectively. \relax }{figure.7.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.13}{\ignorespaces The standard deviations $\sigma _x$ and $\sigma _y$, and covariance $\sigma _{x,y}$ control the shape and dispersion of the bivariate normal distribution. \relax }}{61}{figure.7.13}\protected@file@percent }
\newlabel{fig:cov}{{7.13}{61}{The standard deviations $\sigma _x$ and $\sigma _y$, and covariance $\sigma _{x,y}$ control the shape and dispersion of the bivariate normal distribution.\\\relax }{figure.7.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Parameter estimation}{61}{section.7.4}\protected@file@percent }
\newlabel{sec:normalparameters}{{7.4}{61}{Parameter estimation}{section.7.4}{}}
\newlabel{eq:Lnorm}{{7.8}{61}{Parameter estimation}{equation.7.4.8}{}}
\newlabel{eq:LLnorm}{{7.9}{62}{Parameter estimation}{equation.7.4.9}{}}
\newlabel{eq:stdevgivenmu}{{7.11}{62}{Parameter estimation}{equation.7.4.11}{}}
\newlabel{eq:stdevrepeat}{{7.12}{62}{Parameter estimation}{equation.7.4.12}{}}
\newlabel{eq:sxy}{{7.14}{63}{Parameter estimation}{equation.7.4.14}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Error propagation}{65}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:errorprop}{{8}{65}{Error propagation}{chapter.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Darts board illustration of accuracy (closeness to the centre) and precision (closeness of the measurements). The best case scenario combines high precision with high accuracy. The worst case scenario combines high precision with low accuracy. This is the worst possible situation because the high precision gives false confidence in the data. \relax }}{65}{figure.8.1}\protected@file@percent }
\newlabel{fig:darts}{{8.1}{65}{Darts board illustration of accuracy (closeness to the centre) and precision (closeness of the measurements). The best case scenario combines high precision with high accuracy. The worst case scenario combines high precision with low accuracy. This is the worst possible situation because the high precision gives false confidence in the data.\\\relax }{figure.8.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Linear approximation}{66}{section.8.1}\protected@file@percent }
\newlabel{sec:linearerrorprop}{{8.1}{66}{Linear approximation}{section.8.1}{}}
\newlabel{eq:zgx}{{8.1}{66}{Linear approximation}{equation.8.1.1}{}}
\newlabel{eq:varz}{{8.2}{66}{Linear approximation}{equation.8.1.2}{}}
\newlabel{eq:zi-z}{{8.3}{66}{Linear approximation}{equation.8.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Error propagation of two linear functions, $g_1$ (black line) and $g_2$ (grey line). The input data $\{x_i\}$ are normally distributed around some average value $\bar  {x}$. Error propagation estimates the dispersion of the inferred quantity $z_i$ from the dispersion of the measurements $x_i$. Deviations $(x_i-\bar  {x})$ from the average $x$-value are reduced (for function $g_1$) or magnified (for function $g_2$) depending on the slope of the functions, resulting in deviations of the dependent variable that are smaller $(z_{i1}-\bar  {z}_1)$ or greater $(z_{i2}-\bar  {z}_i)$ than $(x_i-\bar  {x})$. \relax }}{66}{figure.8.2}\protected@file@percent }
\newlabel{fig:errorprop2d}{{8.2}{66}{Error propagation of two linear functions, $g_1$ (black line) and $g_2$ (grey line). The input data $\{x_i\}$ are normally distributed around some average value $\bar {x}$. Error propagation estimates the dispersion of the inferred quantity $z_i$ from the dispersion of the measurements $x_i$. Deviations $(x_i-\bar {x})$ from the average $x$-value are reduced (for function $g_1$) or magnified (for function $g_2$) depending on the slope of the functions, resulting in deviations of the dependent variable that are smaller $(z_{i1}-\bar {z}_1)$ or greater $(z_{i2}-\bar {z}_i)$ than $(x_i-\bar {x})$. \relax }{figure.8.2}{}}
\newlabel{eq:errorprop1d}{{8.4}{66}{Linear approximation}{equation.8.1.4}{}}
\newlabel{eq:zfxy}{{8.5}{67}{Linear approximation}{equation.8.1.5}{}}
\newlabel{eq:dg}{{8.6}{67}{Linear approximation}{equation.8.1.6}{}}
\newlabel{eq:errorprop2d}{{8.8}{67}{Linear approximation}{equation.8.1.8}{}}
\newlabel{eq:errorprop2dmatrix}{{8.9}{67}{Linear approximation}{equation.8.1.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces Error propagation of a bivariate function $z = g(x,y)$. The uncertainty in $z$ is \emph  {approximately} proportional to the slope of the surface $g$ w.r.t. the measurements $x$ and $y$. If the curvature of the 3D surface is minor relative to the uncertainties, then said surface can be \emph  {locally} approximated by a plane. Using such a linear approximation, the size of the error ellipses ($s[x]$, $s[y]$) around the mean measurements (black dots) linearly scales with corresponding deviations in the inferred quantity ($s[z]$).\relax }}{68}{figure.8.3}\protected@file@percent }
\newlabel{fig:errorprop3d}{{8.3}{68}{Error propagation of a bivariate function $z = g(x,y)$. The uncertainty in $z$ is \emph {approximately} proportional to the slope of the surface $g$ w.r.t. the measurements $x$ and $y$. If the curvature of the 3D surface is minor relative to the uncertainties, then said surface can be \emph {locally} approximated by a plane. Using such a linear approximation, the size of the error ellipses ($s[x]$, $s[y]$) around the mean measurements (black dots) linearly scales with corresponding deviations in the inferred quantity ($s[z]$).\relax }{figure.8.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Examples}{68}{section.8.2}\protected@file@percent }
\newlabel{sec:errorpropexamples}{{8.2}{68}{Examples}{section.8.2}{}}
\newlabel{eq:addition}{{8.10}{68}{Examples}{equation.8.2.10}{}}
\newlabel{eq:subtraction}{{8.11}{69}{Examples}{equation.8.2.11}{}}
\newlabel{eq:multiplication}{{8.12}{69}{Examples}{equation.8.2.12}{}}
\newlabel{eq:division}{{8.13}{70}{Examples}{equation.8.2.13}{}}
\newlabel{eq:exponentiation}{{8.14}{70}{Examples}{equation.8.2.14}{}}
\newlabel{eq:logarithms}{{8.15}{71}{Examples}{equation.8.2.15}{}}
\newlabel{eq:power}{{8.16}{71}{Examples}{equation.8.2.16}{}}
\newlabel{eq:newtons2nd}{{8.17}{71}{Examples}{equation.8.2.17}{}}
\newlabel{eq:xvt}{{8.18}{71}{Examples}{equation.8.2.18}{}}
\newlabel{eq:ygt2}{{8.19}{72}{Examples}{equation.8.2.19}{}}
\newlabel{eq:sx}{{8.20}{72}{Examples}{equation.8.2.20}{}}
\newlabel{eq:sy}{{8.21}{72}{Examples}{equation.8.2.21}{}}
\newlabel{eq:sdnewton}{{8.22}{72}{Examples}{equation.8.2.22}{}}
\newlabel{eq:snowy}{{8.23}{72}{Examples}{equation.8.2.23}{}}
\newlabel{eq:haddock}{{8.24}{73}{Examples}{equation.8.2.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Standard deviation vs. standard error}{73}{section.8.3}\protected@file@percent }
\newlabel{sec:stderr}{{8.3}{73}{Standard deviation vs. standard error}{section.8.3}{}}
\newlabel{eq:stderrmean}{{8.25}{73}{Standard deviation vs. standard error}{equation.8.3.25}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Fisher Information}{74}{section.8.4}\protected@file@percent }
\newlabel{sec:FisherInformation}{{8.4}{74}{Fisher Information}{section.8.4}{}}
\newlabel{eq:Fisher}{{8.26}{74}{Fisher Information}{equation.8.4.26}{}}
\newlabel{eq:multidimFisher}{{8.27}{74}{Fisher Information}{equation.8.4.27}{}}
\newlabel{eq:d2LLdl2}{{8.28}{75}{Fisher Information}{equation.8.4.28}{}}
\newlabel{eq:poisvar}{{8.29}{75}{Fisher Information}{equation.8.4.29}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Comparing distributions}{77}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:comparingdistributions}{{9}{77}{Comparing distributions}{chapter.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Q-Q plots}{77}{section.9.1}\protected@file@percent }
\newlabel{sec:q-q}{{9.1}{77}{Q-Q plots}{section.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Quantile-quantile (Q-Q) plot of Old Faithful eruption durations. The horizontal axis marks the theoretical quantiles of a normal distribution with the same mean and standard deviation as the data. The vertical axis marks the quantiles of the actual data. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. Otherwise they will not. In this example the distribution of eruption durations clearly does not follow a normal distribution.\relax }}{77}{figure.9.1}\protected@file@percent }
\newlabel{fig:qqfaithful1}{{9.1}{77}{Quantile-quantile (Q-Q) plot of Old Faithful eruption durations. The horizontal axis marks the theoretical quantiles of a normal distribution with the same mean and standard deviation as the data. The vertical axis marks the quantiles of the actual data. If the two distributions being compared are similar, the points in the Q-Q plot will approximately lie on the line y = x. Otherwise they will not. In this example the distribution of eruption durations clearly does not follow a normal distribution.\relax }{figure.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Q-Q plot of the dataset of 500 sums of $n = 2$ randomly selected eruption durations shown in Figure\nobreakspace  {}\ref  {fig:CLTfaithful2}. The resulting trimodal distribution plots closer to the 1:1 line than the original dataset of Figure\nobreakspace  {}\ref  {fig:qqfaithful1} but is still far from normal.\relax }}{78}{figure.9.2}\protected@file@percent }
\newlabel{fig:qqfaithful2}{{9.2}{78}{Q-Q plot of the dataset of 500 sums of $n = 2$ randomly selected eruption durations shown in Figure~\ref {fig:CLTfaithful2}. The resulting trimodal distribution plots closer to the 1:1 line than the original dataset of Figure~\ref {fig:qqfaithful1} but is still far from normal.\relax }{figure.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces Q-Q plot of the dataset of 500 sums of $n = 10$ randomly selected eruption durations shown in Figure\nobreakspace  {}\ref  {fig:CLTfaithful10}. The data plot very close to the 1:1 line, visually confirming that they follow a normal distribution. The sample distribution only deviates from the theoretical distribution at the most extreme quantiles. This indicates that the sample distribution has heavier tails than the normal distribution. This phenomenon will be discussed further in the next section. Increasing $n$ further would remove this effect near the tails and bring the sample distribution even closer to the normal distribution.\relax }}{78}{figure.9.3}\protected@file@percent }
\newlabel{fig:qqfaithful10}{{9.3}{78}{Q-Q plot of the dataset of 500 sums of $n = 10$ randomly selected eruption durations shown in Figure~\ref {fig:CLTfaithful10}. The data plot very close to the 1:1 line, visually confirming that they follow a normal distribution. The sample distribution only deviates from the theoretical distribution at the most extreme quantiles. This indicates that the sample distribution has heavier tails than the normal distribution. This phenomenon will be discussed further in the next section. Increasing $n$ further would remove this effect near the tails and bring the sample distribution even closer to the normal distribution.\relax }{figure.9.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces  Q-Q plot comparing datasets $X_1$ and $X_2$ of Figure\nobreakspace  {}\ref  {fig:rand2sum}. Even though the two datasets have markedly different means ($\bar  {X}_1=50.0$ and $\bar  {X}_2=59.9$) and slightly different standard deviations ($s[X_1]=3.3$ and $s[X_2]=3.1$), the quantiles of the two datasets plot along a straight line. This means that their distributions are identical in shape. \relax }}{78}{figure.9.4}\protected@file@percent }
\newlabel{fig:qqfaithful12}{{9.4}{78}{Q-Q plot comparing datasets $X_1$ and $X_2$ of Figure~\ref {fig:rand2sum}. Even though the two datasets have markedly different means ($\bar {X}_1=50.0$ and $\bar {X}_2=59.9$) and slightly different standard deviations ($s[X_1]=3.3$ and $s[X_2]=3.1$), the quantiles of the two datasets plot along a straight line. This means that their distributions are identical in shape. \relax }{figure.9.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}The t-test}{78}{section.9.2}\protected@file@percent }
\newlabel{sec:t}{{9.2}{78}{The t-test}{section.9.2}{}}
\newlabel{tab:coins}{{9.2}{78}{The t-test}{section.9.2}{}}
\newlabel{eq:z}{{9.1}{79}{The t-test}{equation.9.2.1}{}}
\newlabel{eq:t}{{9.2}{79}{The t-test}{equation.9.2.2}{}}
\newlabel{eq:t1}{{9.3}{79}{The t-test}{equation.9.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces a) PDF and b) CDF of a t-distribution with 4 degrees of freedom. The one-sided rejection region (for $\alpha =0.05$) is marked in black. The vertical dashed line marks the observed value of $t=-3.2091$. This line plots in the rejection region, leading to the conclusion that $\mu <{19.30}$\nobreakspace  {}g/cm\textsuperscript  {3}. The horizontal dashed line marks the p-value ($0.0163<\alpha $).\relax }}{80}{figure.9.5}\protected@file@percent }
\newlabel{fig:1samplettest}{{9.5}{80}{a) PDF and b) CDF of a t-distribution with 4 degrees of freedom. The one-sided rejection region (for $\alpha =0.05$) is marked in black. The vertical dashed line marks the observed value of $t=-3.2091$. This line plots in the rejection region, leading to the conclusion that $\mu <{19.30}$~g/cm\textsuperscript {3}. The horizontal dashed line marks the p-value ($0.0163<\alpha $).\relax }{figure.9.5}{}}
\newlabel{tab:2setcoins}{{9.2}{80}{The t-test}{figure.9.5}{}}
\newlabel{eq:t2}{{9.4}{80}{The t-test}{equation.9.2.4}{}}
\newlabel{eq:sp}{{9.5}{80}{The t-test}{equation.9.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces a) PDF and b) CDF of a t-distribution with 7 degrees of freedom. The two-sided rejection region (for $\alpha =0.05$) is marked in black. The vertical dashed line marks the observed value of $t=-2.014$ and plots outside the rejection region. Therefore the test does not allow us to conclude that $\mu _1\neq  \mu _2$. \relax }}{81}{figure.9.6}\protected@file@percent }
\newlabel{fig:2samplettest}{{9.6}{81}{a) PDF and b) CDF of a t-distribution with 7 degrees of freedom. The two-sided rejection region (for $\alpha =0.05$) is marked in black. The vertical dashed line marks the observed value of $t=-2.014$ and plots outside the rejection region. Therefore the test does not allow us to conclude that $\mu _1\neq \mu _2$. \relax }{figure.9.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}Confidence intervals}{81}{section.9.3}\protected@file@percent }
\newlabel{eq:tconf}{{9.3}{81}{Confidence intervals}{section.9.3}{}}
\newlabel{eq:tci}{{9.6}{82}{Confidence intervals}{equation.9.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces a) PDFs and b) CDFs of the t-distribution for three different degrees of freedom ($df$). For small sample sizes (low $df$), the t-distribution has long tails towards low and high values. With increasing sample size, the tails become shorter and the t-distribution sharper. When $df>30$, the t-distribution is indistinguishable from a standard normal distribution with $\mu =0$ and $\sigma =1$.\relax }}{83}{figure.9.7}\protected@file@percent }
\newlabel{fig:tdof}{{9.7}{83}{a) PDFs and b) CDFs of the t-distribution for three different degrees of freedom ($df$). For small sample sizes (low $df$), the t-distribution has long tails towards low and high values. With increasing sample size, the tails become shorter and the t-distribution sharper. When $df>30$, the t-distribution is indistinguishable from a standard normal distribution with $\mu =0$ and $\sigma =1$.\relax }{figure.9.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces The grey and black lines mark the PDFs (a) and CDFs (b) of two normal distributions whose means (vertical dotted lines) are offset by 2 standard errors from the sample average (vertical dashed line). They mark a 95\% confidence interval for $\bar  {x}$. However this simple procedure only works if sample size is large enough for the Central Limit Theorem to apply.\relax }}{83}{figure.9.8}\protected@file@percent }
\newlabel{fig:normconf}{{9.8}{83}{The grey and black lines mark the PDFs (a) and CDFs (b) of two normal distributions whose means (vertical dotted lines) are offset by 2 standard errors from the sample average (vertical dashed line). They mark a 95\% confidence interval for $\bar {x}$. However this simple procedure only works if sample size is large enough for the Central Limit Theorem to apply.\relax }{figure.9.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}The $\chi ^2$-test}{83}{section.9.4}\protected@file@percent }
\newlabel{sec:chi2}{{9.4}{83}{The $\chi ^2$-test}{section.9.4}{}}
\newlabel{eq:chi2}{{9.7}{84}{The $\chi ^2$-test}{equation.9.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces a) PDF and b) CDF of the $\chi ^2$-distribution with 5 degrees of freedom. The rejection region is marked in black. The observed value for the earthquake data ($\chi ^2=5.14$) is shown as a vertical dashed line. It plots outside the rejection region, indicating that the histogram of the data falls within the expected range of the hypothesised (Poisson) distribution. \relax }}{85}{figure.9.9}\protected@file@percent }
\newlabel{fig:chi2}{{9.9}{85}{a) PDF and b) CDF of the $\chi ^2$-distribution with 5 degrees of freedom. The rejection region is marked in black. The observed value for the earthquake data ($\chi ^2=5.14$) is shown as a vertical dashed line. It plots outside the rejection region, indicating that the histogram of the data falls within the expected range of the hypothesised (Poisson) distribution. \relax }{figure.9.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Comparing two or more samples}{85}{section.9.5}\protected@file@percent }
\newlabel{sec:contingency}{{9.5}{85}{Comparing two or more samples}{section.9.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Observed clast counts for two sets of cobbles.\relax }}{86}{table.9.1}\protected@file@percent }
\newlabel{tab:observedclasts}{{9.1}{86}{Observed clast counts for two sets of cobbles.\relax }{table.9.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Expected clast counts for two sets of cobbles.\relax }}{86}{table.9.2}\protected@file@percent }
\newlabel{tab:expectedclasts}{{9.2}{86}{Expected clast counts for two sets of cobbles.\relax }{table.9.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces a) PDF and b) CDF of the $\chi ^2$-distribution with 3 degrees of freedom. The rejection region is marked in black. The observed value ($\chi ^2=0.86$) is shown as a vertical dashed line. It plots outside the rejection region, indicating that the histogram of the data falls within the expected range of the hypothesised (Poisson) distribution. \relax }}{87}{figure.9.10}\protected@file@percent }
\newlabel{fig:chi22}{{9.10}{87}{a) PDF and b) CDF of the $\chi ^2$-distribution with 3 degrees of freedom. The rejection region is marked in black. The observed value ($\chi ^2=0.86$) is shown as a vertical dashed line. It plots outside the rejection region, indicating that the histogram of the data falls within the expected range of the hypothesised (Poisson) distribution. \relax }{figure.9.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Cherry picking (Type-I errors revisited)}{87}{section.9.6}\protected@file@percent }
\newlabel{sec:cherrypicking}{{9.6}{87}{Cherry picking (Type-I errors revisited)}{section.9.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces a) predicted frequency distribution for the zircon count data of example\nobreakspace  {}2 in chapter\nobreakspace  {}\ref  {ch:poisson}, following a Poisson distribution with $\lambda =3.50$ and $n=48$; b) -- d) three sample distributions with the $\chi ^2$ statistic and p-values for comparison with distribution a; e) $\chi ^2$-distribution with 5 degrees of freedom. The $\chi ^2$-test would flag sample d as being `significantly different' from the predicted histogram a. Sample c also looks somewhat different from the predicted distribution a, but this difference falls within the expected range of random sampling variability of the Poisson distribution. Sample b is identical to the prediction a. This should raise suspicion. It is extremely unlikely for a sample to fit the prediction so well. \relax }}{88}{figure.9.11}\protected@file@percent }
\newlabel{fig:cherrypicking}{{9.11}{88}{a) predicted frequency distribution for the zircon count data of example~2 in chapter~\ref {ch:poisson}, following a Poisson distribution with $\lambda =3.50$ and $n=48$; b) -- d) three sample distributions with the $\chi ^2$ statistic and p-values for comparison with distribution a; e) $\chi ^2$-distribution with 5 degrees of freedom. The $\chi ^2$-test would flag sample d as being `significantly different' from the predicted histogram a. Sample c also looks somewhat different from the predicted distribution a, but this difference falls within the expected range of random sampling variability of the Poisson distribution. Sample b is identical to the prediction a. This should raise suspicion. It is extremely unlikely for a sample to fit the prediction so well.\\\relax }{figure.9.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.7}Effect size (Type-II errors revisited)}{88}{section.9.7}\protected@file@percent }
\newlabel{sec:effectsize}{{9.7}{88}{Effect size (Type-II errors revisited)}{section.9.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.3}{\ignorespaces Point counting data for two samples of sand.\relax }}{89}{table.9.3}\protected@file@percent }
\newlabel{tab:observedsand}{{9.3}{89}{Point counting data for two samples of sand.\relax }{table.9.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.4}{\ignorespaces Predicted point counts.\relax }}{89}{table.9.4}\protected@file@percent }
\newlabel{tab:predictedsand}{{9.4}{89}{Predicted point counts.\relax }{table.9.4}{}}
\newlabel{eq:effectsize}{{9.10}{90}{Effect size (Type-II errors revisited)}{equation.9.7.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9.8}Non-parametric tests}{91}{section.9.8}\protected@file@percent }
\newlabel{sec:nonparametric}{{9.8}{91}{Non-parametric tests}{section.9.8}{}}
\newlabel{tab:mannwhitneygeneric}{{9.8}{91}{Non-parametric tests}{section.9.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9.5}{\ignorespaces The same data as table\nobreakspace  {}\ref  {tab:2setcoins} but with the second sample marked in bold for future use.\relax }}{92}{table.9.5}\protected@file@percent }
\newlabel{tab:mannwhitneycoins}{{9.5}{92}{The same data as table~\ref {tab:2setcoins} but with the second sample marked in bold for future use.\relax }{table.9.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces a) PMF and b) CDF of the Wilcoxon test statistic for comparison of two samples containing 5 and 4 items, respectively. The two-sided rejection region is marked in black. The observed value ($W=26$) is shown as a vertical dashed line. It plots outside the rejection region, leaving open the possibility that the two samples might have come from the same distribution.\relax }}{93}{figure.9.12}\protected@file@percent }
\newlabel{fig:wilcox}{{9.12}{93}{a) PMF and b) CDF of the Wilcoxon test statistic for comparison of two samples containing 5 and 4 items, respectively. The two-sided rejection region is marked in black. The observed value ($W=26$) is shown as a vertical dashed line. It plots outside the rejection region, leaving open the possibility that the two samples might have come from the same distribution.\relax }{figure.9.12}{}}
\newlabel{eq:KS}{{9.11}{93}{Non-parametric tests}{equation.9.8.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces The two-sample Kolmogorov-Smirnov statistic is the maximum vertical distance between two ECDFs. This example compares the cumulative distributions of 121 detrital zircon U--Pb ages from the Yellow River with 116 detrital zircon U--Pb ages from a sand dune in the adjacent Mu Us desert. The KS-distance is 0.3006.\relax }}{93}{figure.9.13}\protected@file@percent }
\newlabel{fig:KS}{{9.13}{93}{The two-sample Kolmogorov-Smirnov statistic is the maximum vertical distance between two ECDFs. This example compares the cumulative distributions of 121 detrital zircon U--Pb ages from the Yellow River with 116 detrital zircon U--Pb ages from a sand dune in the adjacent Mu Us desert. The KS-distance is 0.3006.\relax }{figure.9.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.14}{\ignorespaces a) PMF and b) CDF of the Kolmogorov-Smirnov statistic for comparison of two samples containing 116 and 121 items, respectively. The vertical dashed lines mark the test statistic for the two sand samples of Figure\nobreakspace  {}\ref  {fig:KS}. The rejection region is marked in black on the PMF and groups all values of the test statistic that exceed the 95 percentile ($D=0.174$). $H_\circ $ is rejected.\relax }}{94}{figure.9.14}\protected@file@percent }
\newlabel{fig:KSdens}{{9.14}{94}{a) PMF and b) CDF of the Kolmogorov-Smirnov statistic for comparison of two samples containing 116 and 121 items, respectively. The vertical dashed lines mark the test statistic for the two sand samples of Figure~\ref {fig:KS}. The rejection region is marked in black on the PMF and groups all values of the test statistic that exceed the 95 percentile ($D=0.174$). $H_\circ $ is rejected.\relax }{figure.9.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.15}{\ignorespaces Kolmogorov-Smirnov statistic for the comparison of a sample with the normal distribution with the same mean and standard deviation. In this case $D=0.27$, which can be compared with a lookup table for a \textit  {one sample} Kolmogorov-Smirnov test. The outcome of this test (which is not elaborated in these notes) is a rejection of the null hypothesis.\relax }}{95}{figure.9.15}\protected@file@percent }
\newlabel{fig:KSnorm}{{9.15}{95}{Kolmogorov-Smirnov statistic for the comparison of a sample with the normal distribution with the same mean and standard deviation. In this case $D=0.27$, which can be compared with a lookup table for a \textit {one sample} Kolmogorov-Smirnov test. The outcome of this test (which is not elaborated in these notes) is a rejection of the null hypothesis.\relax }{figure.9.15}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Regression}{97}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:regression}{{10}{97}{Regression}{chapter.10}{}}
\newlabel{eq:Rb-Sr}{{10.1}{97}{Regression}{equation.10.0.1}{}}
\newlabel{eq:y=a+bx}{{10.2}{97}{Regression}{equation.10.0.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Rb--Sr composition of eight aliquots ($1\leq {i}\leq {8}$) of the same sample.\relax }}{97}{table.10.1}\protected@file@percent }
\newlabel{tab:Rb-Sr}{{10.1}{97}{Rb--Sr composition of eight aliquots ($1\leq {i}\leq {8}$) of the same sample.\relax }{table.10.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Isochron plot for the Rb--Sr data. The scatter plot of eight \textsuperscript  {87}Rb/\textsuperscript  {86}Sr- and \textsuperscript  {87}Sr/\textsuperscript  {86}Sr-ratios forms an array of points along a line whose intercept marks the initial \textsuperscript  {87}Sr/\textsuperscript  {86}Sr-composition, and whose intercept is a function of the age ($t = \qopname  \relax o{ln}[1+\beta _1]/\lambda $). The linear trend is not perfect due to analytical uncertainty, which has dispersed the data. \relax }}{98}{figure.10.1}\protected@file@percent }
\newlabel{fig:Rb-Sr}{{10.1}{98}{Isochron plot for the Rb--Sr data. The scatter plot of eight \textsuperscript {87}Rb/\textsuperscript {86}Sr- and \textsuperscript {87}Sr/\textsuperscript {86}Sr-ratios forms an array of points along a line whose intercept marks the initial \textsuperscript {87}Sr/\textsuperscript {86}Sr-composition, and whose intercept is a function of the age ($t = \ln [1+\beta _1]/\lambda $). The linear trend is not perfect due to analytical uncertainty, which has dispersed the data. \relax }{figure.10.1}{}}
\newlabel{eq:y=a+bx+e}{{10.3}{98}{Regression}{equation.10.0.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}The correlation coefficient}{98}{section.10.1}\protected@file@percent }
\newlabel{sec:corrcoef}{{10.1}{98}{The correlation coefficient}{section.10.1}{}}
\newlabel{eq:rho}{{10.4}{98}{The correlation coefficient}{equation.10.1.4}{}}
\newlabel{eq:r}{{10.5}{98}{The correlation coefficient}{equation.10.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Four synthetic bivariate normal datasets exhibiting different degrees of correlation between the x- and y-variable. Panel a) displays no correlation, b) a weak positive correlation, c) a strong positive correlation, and d) a strong negative correlation. \relax }}{99}{figure.10.2}\protected@file@percent }
\newlabel{fig:r}{{10.2}{99}{Four synthetic bivariate normal datasets exhibiting different degrees of correlation between the x- and y-variable. Panel a) displays no correlation, b) a weak positive correlation, c) a strong positive correlation, and d) a strong negative correlation.\\\relax }{figure.10.2}{}}
\newlabel{eq:tr}{{10.6}{99}{The correlation coefficient}{equation.10.1.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces a) PDF and b) CDF of a t-distribution with 3 degrees of freedom. The two-sided rejection region (for $\alpha = 0.05$) is marked in black. The vertical dashed line marks the observed value of t = 13.98 and plots inside the rejection region. Therefore the test rejects the null hypothesis that $\rho =0$, leading to the conclusion that the data are significantly correlated.\relax }}{100}{figure.10.3}\protected@file@percent }
\newlabel{fig:tr}{{10.3}{100}{a) PDF and b) CDF of a t-distribution with 3 degrees of freedom. The two-sided rejection region (for $\alpha = 0.05$) is marked in black. The vertical dashed line marks the observed value of t = 13.98 and plots inside the rejection region. Therefore the test rejects the null hypothesis that $\rho =0$, leading to the conclusion that the data are significantly correlated.\relax }{figure.10.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Least Squares}{100}{section.10.2}\protected@file@percent }
\newlabel{sec:leastsquares}{{10.2}{100}{Least Squares}{section.10.2}{}}
\newlabel{eq:ss}{{10.7}{100}{Least Squares}{equation.10.2.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.2}{\ignorespaces The same data as Table\nobreakspace  {}\ref  {tab:Rb-Sr}. $y_i$ are the observed and $\beta _0 + \beta _1 x_i$ the \emph  {fitted} values of the dependent variable. The residuals $\epsilon _i$ are the differences between these two sets of numbers.\relax }}{100}{table.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Three guesses for the intercept ($\beta _0$) and slope ($\beta _1$) of the Rb--Sr isochron data of Figure\nobreakspace  {}\ref  {fig:Rb-Sr}. Dashed lines mark the residuals. The lines in panels a) and b) are too steep and too shallow, respectively. Consequently, the sum of their squared residuals ($ss$) is non-zero. Panel c) shows a better fit and a lower sum of squares. \relax }}{101}{figure.10.4}\protected@file@percent }
\newlabel{eq:fitab}{{10.9}{101}{Least Squares}{equation.10.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Maximum Likelihood}{101}{section.10.3}\protected@file@percent }
\newlabel{sec:MLregression}{{10.3}{101}{Maximum Likelihood}{section.10.3}{}}
\newlabel{eq:normalresid}{{10.10}{102}{Maximum Likelihood}{equation.10.3.10}{}}
\newlabel{eq:Lregression}{{10.11}{102}{Maximum Likelihood}{equation.10.3.11}{}}
\newlabel{eq:sigmabeta}{{10.13}{102}{Maximum Likelihood}{equation.10.3.13}{}}
\newlabel{eq:sigmahatregression}{{10.14}{102}{Maximum Likelihood}{equation.10.3.14}{}}
\newlabel{eq:ciregression}{{10.15}{103}{Maximum Likelihood}{equation.10.3.15}{}}
\newlabel{eq:envelope}{{10.16}{103}{Maximum Likelihood}{equation.10.3.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces The grey area represents a 95\% confidence envelope for the least squares regression of the Rb--Sr data. The curvature of this envelope reflects the increased uncertainty that is caused by \textbf  {extrapolating} the data. The width of the confidence envelope is the smallest near the average of the measurements ($\bar  {x}$, $\bar  {y}$) and increases indefinitely beyond that.\relax }}{103}{figure.10.5}\protected@file@percent }
\newlabel{fig:envelope}{{10.5}{103}{The grey area represents a 95\% confidence envelope for the least squares regression of the Rb--Sr data. The curvature of this envelope reflects the increased uncertainty that is caused by \textbf {extrapolating} the data. The width of the confidence envelope is the smallest near the average of the measurements ($\bar {x}$, $\bar {y}$) and increases indefinitely beyond that.\relax }{figure.10.5}{}}
\newlabel{eq:prediction-interval}{{10.17}{103}{Maximum Likelihood}{equation.10.3.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Confidence envelopes (grey) and prediction intervals (dashed lines) for three synthetic datasets of increasing size. Whereas the width of the confidence interval approaches zero for large datasets, the width of the prediction interval only decreases slightly. \relax }}{104}{figure.10.6}\protected@file@percent }
\newlabel{fig:prediction-interval}{{10.6}{104}{Confidence envelopes (grey) and prediction intervals (dashed lines) for three synthetic datasets of increasing size. Whereas the width of the confidence interval approaches zero for large datasets, the width of the prediction interval only decreases slightly. \relax }{figure.10.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Common mistakes}{104}{section.10.4}\protected@file@percent }
\newlabel{sec:regression-caveats}{{10.4}{104}{Common mistakes}{section.10.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces 18 scatter plots of random uniform bivariate data, with indication of the coefficient of determination ($r^2$) and the p-value for correlation. The one `significant' result (p-value = 0.00063) is a Type-I error.\relax }}{104}{figure.10.7}\protected@file@percent }
\newlabel{fig:regression-p-hacking}{{10.7}{104}{18 scatter plots of random uniform bivariate data, with indication of the coefficient of determination ($r^2$) and the p-value for correlation. The one `significant' result (p-value = 0.00063) is a Type-I error.\relax }{figure.10.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces A synthetic dataset of ten clustered points around $\{x=1,y=1\}$ plus one outlier at $\{x=5,y=5\}$. The cluster consists of the same values as the first panel of Figure\nobreakspace  {}\ref  {fig:regression-p-hacking}. But whereas the latter dataset was characterised by a coefficient of determination of almost zero ($r^2=0.0014$), the new dataset has a coefficient of determination that is close to one ($r^2=0.9$). Such is the disproportionate effect of the additional data point. \relax }}{105}{figure.10.8}\protected@file@percent }
\newlabel{fig:regression-outlier}{{10.8}{105}{A synthetic dataset of ten clustered points around $\{x=1,y=1\}$ plus one outlier at $\{x=5,y=5\}$. The cluster consists of the same values as the first panel of Figure~\ref {fig:regression-p-hacking}. But whereas the latter dataset was characterised by a coefficient of determination of almost zero ($r^2=0.0014$), the new dataset has a coefficient of determination that is close to one ($r^2=0.9$). Such is the disproportionate effect of the additional data point. \relax }{figure.10.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces a--c) Three random normal datasets of 50 points each, drawn from normal distributions with means $\mu _x=\mu _y=\mu _z=100$ and standard deviations $\sigma _x=\sigma _y=1$ and $\sigma _z=10$, respectively. The three datasets are independent, so $\rho _{x,y}=\rho _{x,z}=\rho _{y,z}=0$. However, the ratio of $x/z$ is strongly correlated with d) $y/z$, and with e) $z$. This correlation is entirely spurious has no scientific value. \relax }}{105}{figure.10.9}\protected@file@percent }
\newlabel{fig:spurious}{{10.9}{105}{a--c) Three random normal datasets of 50 points each, drawn from normal distributions with means $\mu _x=\mu _y=\mu _z=100$ and standard deviations $\sigma _x=\sigma _y=1$ and $\sigma _z=10$, respectively. The three datasets are independent, so $\rho _{x,y}=\rho _{x,z}=\rho _{y,z}=0$. However, the ratio of $x/z$ is strongly correlated with d) $y/z$, and with e) $z$. This correlation is entirely spurious has no scientific value.\\\relax }{figure.10.9}{}}
\newlabel{eq:spurious}{{10.18}{105}{Common mistakes}{equation.10.4.18}{}}
\newlabel{eq:spuriousxzyz}{{10.19}{106}{Common mistakes}{equation.10.4.19}{}}
\newlabel{eq:spuriouszxz}{{10.20}{106}{Common mistakes}{equation.10.4.20}{}}
\newlabel{eq:spuriousxzyzindepxyz}{{10.21}{106}{Common mistakes}{equation.10.4.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Weighted regression}{106}{section.10.5}\protected@file@percent }
\newlabel{sec:weightedregression}{{10.5}{106}{Weighted regression}{section.10.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10.3}{\ignorespaces Synthetic three-aliquot dataset. $X$ and $Y$ the \emph  {true} values; $x$ and $y$ are three \emph  {measurements}, $s[x]$ and $s[y]$ their respective standard errors, and $s[x,y]$ their covariance. The uncertainties differ between the three samples, which are therefore heteroscedastic.\relax }}{107}{table.10.3}\protected@file@percent }
\newlabel{tab:york}{{10.3}{107}{Synthetic three-aliquot dataset. $X$ and $Y$ the \emph {true} values; $x$ and $y$ are three \emph {measurements}, $s[x]$ and $s[y]$ their respective standard errors, and $s[x,y]$ their covariance. The uncertainties differ between the three samples, which are therefore heteroscedastic.\relax }{table.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.10}{\ignorespaces Synthetic data of Table\nobreakspace  {}\ref  {tab:york}. The white squares are the true population values ($X$ and $Y$) of three aliquots . The black squares are three measurements ($x$ and $y$). The ellipses represent 95\% confidence regions for bivariate normal distributions with means $x$ and $y$, and (co)variances $s[x]$, $s[y]$ and $s[x,y]$. The true values fall on a line with intercept $\beta _0=10$ and slope $\beta _1=1$ (dotted line). The unweighted least squares fit (dashed line) has an intercept of $\beta _0=1.9$ and slope $\beta _1=1.63$. This poor result is entirely due to the third data point, whose disproportionally large uncertainties are not properly accounted for by the ordinary least squares regression algorithm. \relax }}{107}{figure.10.10}\protected@file@percent }
\newlabel{fig:errorfit}{{10.10}{107}{Synthetic data of Table~\ref {tab:york}. The white squares are the true population values ($X$ and $Y$) of three aliquots . The black squares are three measurements ($x$ and $y$). The ellipses represent 95\% confidence regions for bivariate normal distributions with means $x$ and $y$, and (co)variances $s[x]$, $s[y]$ and $s[x,y]$. The true values fall on a line with intercept $\beta _0=10$ and slope $\beta _1=1$ (dotted line). The unweighted least squares fit (dashed line) has an intercept of $\beta _0=1.9$ and slope $\beta _1=1.63$. This poor result is entirely due to the third data point, whose disproportionally large uncertainties are not properly accounted for by the ordinary least squares regression algorithm. \relax }{figure.10.10}{}}
\newlabel{eq:york}{{10.22}{107}{Weighted regression}{equation.10.5.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.11}{\ignorespaces a) the same as Figure\nobreakspace  {}\ref  {fig:errorfit}, but with the weighted regression result added as a solid line. Its intercept is $\beta _0=9.4$ and its slope is $\beta _1=1.05$. These values are much closer to the true values (dotted line) than the ordinary least squares solution (dashed line) is. b) Zooming into aliquot\nobreakspace  {}3 shows the true value of the independent variable ($X_3$), its measured value ($x_3$), and the fitted value $\boldsymbol  {x}_3$. \relax }}{108}{figure.10.11}\protected@file@percent }
\newlabel{fig:yorkfit}{{10.11}{108}{a) the same as Figure~\ref {fig:errorfit}, but with the weighted regression result added as a solid line. Its intercept is $\beta _0=9.4$ and its slope is $\beta _1=1.05$. These values are much closer to the true values (dotted line) than the ordinary least squares solution (dashed line) is. b) Zooming into aliquot~3 shows the true value of the independent variable ($X_3$), its measured value ($x_3$), and the fitted value $\boldsymbol {x}_3$. \relax }{figure.10.11}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Fractals and chaos}{109}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:fractals}{{11}{109}{Fractals and chaos}{chapter.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Histogram for 20,000 recent earthquakes of magnitude $\geq {4.5}$ from the USGS earthquake catalog.\relax }}{109}{figure.11.1}\protected@file@percent }
\newlabel{fig:recentquakes}{{11.1}{109}{Histogram for 20,000 recent earthquakes of magnitude $\geq {4.5}$ from the USGS earthquake catalog.\relax }{figure.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.2}{\ignorespaces  The histogram of the logarithm of the 20,000 earthquakes in Figure\nobreakspace  {}\ref  {fig:recentquakes} is still negatively skewed.\relax }}{110}{figure.11.2}\protected@file@percent }
\newlabel{fig:recentlogquakes}{{11.2}{110}{The histogram of the logarithm of the 20,000 earthquakes in Figure~\ref {fig:recentquakes} is still negatively skewed.\relax }{figure.11.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Power law distributions}{110}{section.11.1}\protected@file@percent }
\newlabel{sec:power-law}{{11.1}{110}{Power law distributions}{section.11.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.3}{\ignorespaces  Bivariate scatter plot of $y=\qopname  \relax o{log}_{10}[N/N_\circ ]$ against earthquake magnitude, where $N$ is the number of earthquakes exceeding a given magnitude and $N_\circ $ is the total number of earthquakes, which is 20,000 for the dataset of Figure\nobreakspace  {}\ref  {fig:recentquakes}. \relax }}{110}{figure.11.3}\protected@file@percent }
\newlabel{fig:gutenberg}{{11.3}{110}{Bivariate scatter plot of $y=\log _{10}[N/N_\circ ]$ against earthquake magnitude, where $N$ is the number of earthquakes exceeding a given magnitude and $N_\circ $ is the total number of earthquakes, which is 20,000 for the dataset of Figure~\ref {fig:recentquakes}. \relax }{figure.11.3}{}}
\newlabel{eq:gutenberg}{{11.1}{110}{Power law distributions}{equation.11.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.4}{\ignorespaces A map of central Finland (axis labels mark latitude and longitude), with water marked in black and land marked in white. Finland is also known as ``the land of a thousand lakes''. But in fact there are far more than 1000 lakes in Finland. The small area shown in this figure already contains 2327 of them. Most of these lakes are small, but there are also a few big ones that cover an area of more than 1000\nobreakspace  {}km\textsuperscript  {2}.\relax }}{111}{figure.11.4}\protected@file@percent }
\newlabel{fig:Finland}{{11.4}{111}{A map of central Finland (axis labels mark latitude and longitude), with water marked in black and land marked in white. Finland is also known as ``the land of a thousand lakes''. But in fact there are far more than 1000 lakes in Finland. The small area shown in this figure already contains 2327 of them. Most of these lakes are small, but there are also a few big ones that cover an area of more than 1000~km\textsuperscript {2}.\relax }{figure.11.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.5}{\ignorespaces Plotting the number of lakes exceeding a certain size against that size on a log-log scale yields a linear array of points similar to the Gutenberg-Richter Law of Figure\nobreakspace  {}\ref  {fig:gutenberg}. Extrapolating this trend towards the left would reveal that there are millions of puddles in Finland.\relax }}{111}{figure.11.5}\protected@file@percent }
\newlabel{fig:Finlandpowerlaw}{{11.5}{111}{Plotting the number of lakes exceeding a certain size against that size on a log-log scale yields a linear array of points similar to the Gutenberg-Richter Law of Figure~\ref {fig:gutenberg}. Extrapolating this trend towards the left would reveal that there are millions of puddles in Finland.\relax }{figure.11.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.2}How long is the coast of Britain?}{112}{section.11.2}\protected@file@percent }
\newlabel{sec:britain}{{11.2}{112}{How long is the coast of Britain?}{section.11.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.6}{\ignorespaces Five attempts to measure the length of Britain's coastline. Different results are obtained depending on the length of the measuring rod used for the measurements. The shorter the yardstick (shown as error bars), the longer the estimate. \relax }}{112}{figure.11.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11.7}{\ignorespaces Setting out the length of the British coast line against the length of the measuring rod on a log-log scale produces a linear trend with a slope of -0.253. This line can be extrapolated to lower values to estimate the length that would be measured with even smaller measuring rods. For example, if we were to measure the British coast with a 30\nobreakspace  {}cm long ruler, then this would produce a result of  $\qopname  \relax o{exp}(9.18-0.253\qopname  \relax o{ln}[3\times {10}^{-4}])=42,060$\nobreakspace  {}km! \relax }}{112}{figure.11.7}\protected@file@percent }
\newlabel{fig:loglogbritain}{{11.7}{112}{Setting out the length of the British coast line against the length of the measuring rod on a log-log scale produces a linear trend with a slope of -0.253. This line can be extrapolated to lower values to estimate the length that would be measured with even smaller measuring rods. For example, if we were to measure the British coast with a 30~cm long ruler, then this would produce a result of\\ $\exp (9.18-0.253\ln [3\times {10}^{-4}])=42,060$~km! \relax }{figure.11.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.8}{\ignorespaces The same data as Figure\nobreakspace  {}\ref  {fig:loglogbritain} but plotting the number of polygonal segments on the y-axis instead of the length of those segments. Note how the slope of the best fit line equals 1 + the slope of Figure\nobreakspace  {}\ref  {fig:loglogbritain}.\relax }}{113}{figure.11.8}\protected@file@percent }
\newlabel{fig:fractaldimbritain}{{11.8}{113}{The same data as Figure~\ref {fig:loglogbritain} but plotting the number of polygonal segments on the y-axis instead of the length of those segments. Note how the slope of the best fit line equals 1 + the slope of Figure~\ref {fig:loglogbritain}.\relax }{figure.11.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.9}{\ignorespaces Box counting of the British coast using (from left to right) a $16\times {16}$, $32\times {32}$, $64\times {64}$, and $128\times {128}$ grid. Black squares overlap with the coastline, white squares do not. The legends in the upper right corner of each subpanel specify the number of black squares relative to the total number of squares. \relax }}{113}{figure.11.9}\protected@file@percent }
\newlabel{fig:Britainboxes}{{11.9}{113}{Box counting of the British coast using (from left to right) a $16\times {16}$, $32\times {32}$, $64\times {64}$, and $128\times {128}$ grid. Black squares overlap with the coastline, white squares do not. The legends in the upper right corner of each subpanel specify the number of black squares relative to the total number of squares.\\\relax }{figure.11.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.10}{\ignorespaces Plotting the number of boxes against their size on a log-log diagram yields a linear array with a slope of -1.28. This is similar to the value obtained by the polygonal line segment method of Figure\nobreakspace  {}\ref  {fig:fractaldimbritain}. \relax }}{113}{figure.11.10}\protected@file@percent }
\newlabel{fig:Britainboxcounts}{{11.10}{113}{Plotting the number of boxes against their size on a log-log diagram yields a linear array with a slope of -1.28. This is similar to the value obtained by the polygonal line segment method of Figure~\ref {fig:fractaldimbritain}. \relax }{figure.11.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.11}{\ignorespaces Box counting of the river network on the island of Corsica (France). Black boxes overlap with rivers, white boxes do not. Legends are as in Figure\nobreakspace  {}\ref  {fig:Britainboxes}. \relax }}{114}{figure.11.11}\protected@file@percent }
\newlabel{fig:Corsica}{{11.11}{114}{Box counting of the river network on the island of Corsica (France). Black boxes overlap with rivers, white boxes do not. Legends are as in Figure~\ref {fig:Britainboxes}.\\\relax }{figure.11.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.12}{\ignorespaces Log-log plot of the box counting results of Figure\nobreakspace  {}\ref  {fig:Corsica}. The power law appears to be a law of nature. \relax }}{114}{figure.11.12}\protected@file@percent }
\newlabel{fig:Corsicaboxcounts}{{11.12}{114}{Log-log plot of the box counting results of Figure~\ref {fig:Corsica}. The power law appears to be a law of nature. \relax }{figure.11.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.3}Fractals}{114}{section.11.3}\protected@file@percent }
\newlabel{sec:fractals}{{11.3}{114}{Fractals}{section.11.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.13}{\ignorespaces A 1\textsuperscript  {st} order Koch curve is constructed by (1) dividing a straight line segment into three segments of equal length; (2) drawing an equilateral triangle that has the middle segment from step 1 as its base and points outward; and (3) removing the line segment that is the base of the triangle from step 2. \relax }}{114}{figure.11.13}\protected@file@percent }
\newlabel{fig:koch1}{{11.13}{114}{A 1\textsuperscript {st} order Koch curve is constructed by (1) dividing a straight line segment into three segments of equal length; (2) drawing an equilateral triangle that has the middle segment from step 1 as its base and points outward; and (3) removing the line segment that is the base of the triangle from step 2. \relax }{figure.11.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.14}{\ignorespaces A 2\textsuperscript  {nd} order Koch curve is derived from a 1\textsuperscript  {st} order Koch curve by replacing each straight line segment in the 1\textsuperscript  {st} order curve with a scaled down version of that 1\textsuperscript  {st} order curve.\relax }}{115}{figure.11.14}\protected@file@percent }
\newlabel{fig:koch2}{{11.14}{115}{A 2\textsuperscript {nd} order Koch curve is derived from a 1\textsuperscript {st} order Koch curve by replacing each straight line segment in the 1\textsuperscript {st} order curve with a scaled down version of that 1\textsuperscript {st} order curve.\relax }{figure.11.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.15}{\ignorespaces A 3\textsuperscript  {rd} order Koch curve is derived from a 2\textsuperscript  {nd} order Koch curve by replacing each straight line segment in the 2\textsuperscript  {nd} order curve with a scaled down version of the 1\textsuperscript  {st} order Koch curve.\relax }}{115}{figure.11.15}\protected@file@percent }
\newlabel{fig:koch3}{{11.15}{115}{A 3\textsuperscript {rd} order Koch curve is derived from a 2\textsuperscript {nd} order Koch curve by replacing each straight line segment in the 2\textsuperscript {nd} order curve with a scaled down version of the 1\textsuperscript {st} order Koch curve.\relax }{figure.11.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.16}{\ignorespaces This is a 6\textsuperscript  {th} order Koch curve, which is generated by replacing each straight line segment in a 5\textsuperscript  {th} order curve with a scaled down version of the 1\textsuperscript  {st} order curve.\relax }}{115}{figure.11.16}\protected@file@percent }
\newlabel{fig:koch6}{{11.16}{115}{This is a 6\textsuperscript {th} order Koch curve, which is generated by replacing each straight line segment in a 5\textsuperscript {th} order curve with a scaled down version of the 1\textsuperscript {st} order curve.\relax }{figure.11.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.17}{\ignorespaces Box counting of the 6\textsuperscript  {th} order Koch curve. The larger the boxes, the fewer of them are needed to cover the entire curve. The recursive order of the Koch curve can be increased indefinitely, and so does the number of small boxes needed to cover them. \relax }}{115}{figure.11.17}\protected@file@percent }
\newlabel{fig:kochboxes}{{11.17}{115}{Box counting of the 6\textsuperscript {th} order Koch curve. The larger the boxes, the fewer of them are needed to cover the entire curve. The recursive order of the Koch curve can be increased indefinitely, and so does the number of small boxes needed to cover them.\\\relax }{figure.11.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.18}{\ignorespaces Log-log plot setting out the number of boxes needed to cover the 6\textsuperscript  {th} order Koch curve of Figure\nobreakspace  {}\ref  {fig:koch6} against the size of those boxes. The best fitting line has a slope of 1.21, which is similar to the slope of the box-counting results for the British coastline (Figure\nobreakspace  {}\ref  {fig:Britainboxcounts}). \relax }}{116}{figure.11.18}\protected@file@percent }
\newlabel{fig:kochboxcounts}{{11.18}{116}{Log-log plot setting out the number of boxes needed to cover the 6\textsuperscript {th} order Koch curve of Figure~\ref {fig:koch6} against the size of those boxes. The best fitting line has a slope of 1.21, which is similar to the slope of the box-counting results for the British coastline (Figure~\ref {fig:Britainboxcounts}). \relax }{figure.11.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.19}{\ignorespaces The \textbf  {Sierpinski carpet} is generated using a recursive algorithm that is built on a grid of eight black squares surrounding a white square. Each level of recursion replaces each black square by the same pattern. From left to right, this figure shows the first four levels of recursion for this algorithm. The end result is an arrangement of small and large holes that shares many characteristics with the size distribution of Finnish lakes shown in Figure\nobreakspace  {}\ref  {fig:Finland}.  \relax }}{116}{figure.11.19}\protected@file@percent }
\newlabel{fig:sierpinski}{{11.19}{116}{The \textbf {Sierpinski carpet} is generated using a recursive algorithm that is built on a grid of eight black squares surrounding a white square. Each level of recursion replaces each black square by the same pattern. From left to right, this figure shows the first four levels of recursion for this algorithm. The end result is an arrangement of small and large holes that shares many characteristics with the size distribution of Finnish lakes shown in Figure~\ref {fig:Finland}.\\ \relax }{figure.11.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.20}{\ignorespaces Log-log plot of the box counting results for the Sierpinski carpet. This pattern in ideally suited for box counting, resulting in a perfect linear fit. Note how the slope of the best fitting line (1.92) is higher than that of the Koch curve (slope=1.21). It is similar to the slope that would be obtained by box-counting the Finnish lakes of Figure\nobreakspace  {}\ref  {fig:Finland}, which similar to $1-x$ where $x$ is the slope of the size-frequency plot of the Finnish lakes (Figure\nobreakspace  {}\ref  {fig:Finlandpowerlaw}).\relax }}{117}{figure.11.20}\protected@file@percent }
\newlabel{fig:sierpinskiboxcounts}{{11.20}{117}{Log-log plot of the box counting results for the Sierpinski carpet. This pattern in ideally suited for box counting, resulting in a perfect linear fit. Note how the slope of the best fitting line (1.92) is higher than that of the Koch curve (slope=1.21). It is similar to the slope that would be obtained by box-counting the Finnish lakes of Figure~\ref {fig:Finland}, which similar to $1-x$ where $x$ is the slope of the size-frequency plot of the Finnish lakes (Figure~\ref {fig:Finlandpowerlaw}).\relax }{figure.11.20}{}}
\newlabel{eq:fractaldim}{{11.2}{117}{Fractals}{equation.11.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.21}{\ignorespaces The Cantor set is generated using a recursive algorithm that is built on a line segment whose middle third is removed. Each level of recursion replaces each black line by the same pattern. From top to bottom, this figure shows the first five levels of recursion for this algorithm. \relax }}{117}{figure.11.21}\protected@file@percent }
\newlabel{fig:cantor}{{11.21}{117}{The Cantor set is generated using a recursive algorithm that is built on a line segment whose middle third is removed. Each level of recursion replaces each black line by the same pattern. From top to bottom, this figure shows the first five levels of recursion for this algorithm. \relax }{figure.11.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.22}{\ignorespaces The y-axis shows the number of linear segments in the Cantor set that exceed the length shown on the x-axis. The values form a power law with a fractal dimension of $D=0.68$. In fact it can be shown that the exact value is $D=\qopname  \relax o{ln}[2]/\qopname  \relax o{ln}[3]$. With a fractal dimension between zero and one, the Cantor set falls somewhere between a point and a line. \relax }}{118}{figure.11.22}\protected@file@percent }
\newlabel{fig:cantorloglog}{{11.22}{118}{The y-axis shows the number of linear segments in the Cantor set that exceed the length shown on the x-axis. The values form a power law with a fractal dimension of $D=0.68$. In fact it can be shown that the exact value is $D=\ln [2]/\ln [3]$. With a fractal dimension between zero and one, the Cantor set falls somewhere between a point and a line. \relax }{figure.11.22}{}}
\@writefile{toc}{\contentsline {section}{\numberline {11.4}Chaos}{118}{section.11.4}\protected@file@percent }
\newlabel{sec:chaos}{{11.4}{118}{Chaos}{section.11.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.23}{\ignorespaces A pendulum is swinging above three magnets. The force ($F_m$) exerted on the pendulum scales with the square of its bob's distance ($|d|$) to the magnets ($F_m(i) \propto 1/|d(i)|^2$, where $1\leq {i}\leq {3}$ marks each of the magnets). The pendulum slows down due to friction ($F_f(i) \propto v$ where $v$ is the velocity of the bob) and eventually comes to a standstill above one of the magnets. On this figure it has done so above the first magnet.\relax }}{118}{figure.11.23}\protected@file@percent }
\newlabel{fig:pendulum}{{11.23}{118}{A pendulum is swinging above three magnets. The force ($F_m$) exerted on the pendulum scales with the square of its bob's distance ($|d|$) to the magnets ($F_m(i) \propto 1/|d(i)|^2$, where $1\leq {i}\leq {3}$ marks each of the magnets). The pendulum slows down due to friction ($F_f(i) \propto v$ where $v$ is the velocity of the bob) and eventually comes to a standstill above one of the magnets. On this figure it has done so above the first magnet.\relax }{figure.11.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.24}{\ignorespaces This figure shows the three magnet configuration of Figure\nobreakspace  {}\ref  {fig:pendulum} in map view. The black line marks the trajectory of the pendulum after it was pushed southward from a position towards the northeast of the three magnets. After describing a circular motion, the bob of the pendulum accelerates towards the second magnet, gets deflected by it, and slows down. It then heads towards the third magnet and the first magnet before returning to the second magnet and coming to a standstill there. \relax }}{118}{figure.11.24}\protected@file@percent }
\newlabel{fig:3magnets1}{{11.24}{118}{This figure shows the three magnet configuration of Figure~\ref {fig:pendulum} in map view. The black line marks the trajectory of the pendulum after it was pushed southward from a position towards the northeast of the three magnets. After describing a circular motion, the bob of the pendulum accelerates towards the second magnet, gets deflected by it, and slows down. It then heads towards the third magnet and the first magnet before returning to the second magnet and coming to a standstill there. \relax }{figure.11.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.25}{\ignorespaces The experimental setup shown in this figure is nearly identical to that of Figure\nobreakspace  {}\ref  {fig:3magnets1}. The only difference is a slight offset of the initial position. The first stage of the resulting trajectory is nearly identical to that of Figure\nobreakspace  {}\ref  {fig:3magnets1}. The bob makes a circular motion towards the second magnet and decelerates. But after passing the second magnet, its course diverges from the first experiment. It moves towards the first magnet, to the third magnet and then back to the first magnet before coming to a standstill. Thus, the slight difference in initial position has produced a completely different end result.\relax }}{119}{figure.11.25}\protected@file@percent }
\newlabel{fig:3magnets2}{{11.25}{119}{The experimental setup shown in this figure is nearly identical to that of Figure~\ref {fig:3magnets1}. The only difference is a slight offset of the initial position. The first stage of the resulting trajectory is nearly identical to that of Figure~\ref {fig:3magnets1}. The bob makes a circular motion towards the second magnet and decelerates. But after passing the second magnet, its course diverges from the first experiment. It moves towards the first magnet, to the third magnet and then back to the first magnet before coming to a standstill. Thus, the slight difference in initial position has produced a completely different end result.\relax }{figure.11.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11.26}{\ignorespaces This intricate picture colour codes the initial positions of the magnetic pendulum experiment according to its outcomes. White, grey and black pixels in this $512\times {512}$ image mark initial positions that resulted in a final position at the first, second and third magnet, respectively. The resulting pattern is simple in the immediate vicinity of the magnets, but complex at a further distance. It has all the characteristics of a fractal, exhibiting the same level of complexity regardless of scale. The pattern is determinisitic in the sense that the same grid of initial conditions produces exactly the same pattern. But it is chaotic because even tiny changes in the initial positions or velocity may produce completely different patterns. \relax }}{119}{figure.11.26}\protected@file@percent }
\newlabel{fig:3magnets}{{11.26}{119}{This intricate picture colour codes the initial positions of the magnetic pendulum experiment according to its outcomes. White, grey and black pixels in this $512\times {512}$ image mark initial positions that resulted in a final position at the first, second and third magnet, respectively. The resulting pattern is simple in the immediate vicinity of the magnets, but complex at a further distance. It has all the characteristics of a fractal, exhibiting the same level of complexity regardless of scale. The pattern is determinisitic in the sense that the same grid of initial conditions produces exactly the same pattern. But it is chaotic because even tiny changes in the initial positions or velocity may produce completely different patterns. \relax }{figure.11.26}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Unsupervised learning}{121}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:unsupervised}{{12}{121}{Unsupervised learning}{chapter.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Principal Component Analysis}{121}{section.12.1}\protected@file@percent }
\newlabel{sec:PCA}{{12.1}{121}{Principal Component Analysis}{section.12.1}{}}
\newlabel{eq:X}{{12.1}{121}{Principal Component Analysis}{equation.12.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.1}{\ignorespaces Simple toy example of three samples that can be visualised on a two-dimensional scatter plot. There is no need to use unsupervised learning in this case. But we will use this simple dataset as a toy example to understand how Principal Component Analysis works. \relax }}{121}{figure.12.1}\protected@file@percent }
\newlabel{fig:PCA2Ddata}{{12.1}{121}{Simple toy example of three samples that can be visualised on a two-dimensional scatter plot. There is no need to use unsupervised learning in this case. But we will use this simple dataset as a toy example to understand how Principal Component Analysis works.\\\relax }{figure.12.1}{}}
\newlabel{eq:PCA}{{12.2}{122}{Principal Component Analysis}{equation.12.1.2}{}}
\newlabel{eq:P}{{12.3}{122}{Principal Component Analysis}{equation.12.1.3}{}}
\newlabel{eq:L}{{12.4}{122}{Principal Component Analysis}{equation.12.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.2}{\ignorespaces PCA decomposition of Figure\nobreakspace  {}\ref  {fig:PCA2Ddata}. The data $X$ are shown as numbers, $C$ as a yellow square, and $1_{2,1}C \pm L$ as a cross. The first principal direction (running from the upper left to the lower right) has been stretched by a factor of $(3.67/0.71) = 5.2$ w.r.t the second principal direction, which runs perpendicular to it. \relax }}{122}{figure.12.2}\protected@file@percent }
\newlabel{fig:PCA2D1}{{12.2}{122}{PCA decomposition of Figure~\ref {fig:PCA2Ddata}. The data $X$ are shown as numbers, $C$ as a yellow square, and $1_{2,1}C \pm L$ as a cross. The first principal direction (running from the upper left to the lower right) has been stretched by a factor of $(3.67/0.71) = 5.2$ w.r.t the second principal direction, which runs perpendicular to it.\\\relax }{figure.12.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.3}{\ignorespaces Projection of the three data points on the two principal directions yields two principal components ($P$ in Equation\nobreakspace  {}\ref  {eq:P}), representing a one dimensional representation of the two-dimensional data \relax }}{122}{figure.12.3}\protected@file@percent }
\newlabel{fig:PCA2D2}{{12.3}{122}{Projection of the three data points on the two principal directions yields two principal components ($P$ in Equation~\ref {eq:P}), representing a one dimensional representation of the two-dimensional data\\\relax }{figure.12.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.4}{\ignorespaces A biplot of both principal components along with the loadings of the two variables shown as arrows. The first principal component shows that the most important difference is that between sample\nobreakspace  {}1 (which is rich in $b$ and poor in $a$) and samples 2 \& 3 (which are poor in $b$ and rich in $a$). The second principal component captures the remaining variance, with sample\nobreakspace  {}3 begin slightly richer in $a$ and $b$ than sample\nobreakspace  {}2. \relax }}{123}{figure.12.4}\protected@file@percent }
\newlabel{fig:PCA2D3}{{12.4}{123}{A biplot of both principal components along with the loadings of the two variables shown as arrows. The first principal component shows that the most important difference is that between sample~1 (which is rich in $b$ and poor in $a$) and samples 2 \& 3 (which are poor in $b$ and rich in $a$). The second principal component captures the remaining variance, with sample~3 begin slightly richer in $a$ and $b$ than sample~2.\\\relax }{figure.12.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12.1}{\ignorespaces \texttt  {USArrests} is a dataset that is built into the \texttt  {R} programming environment. It contains crime statistics (in arrests per 100,000 residents) for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percentage of the population living in urban areas. Thus, \texttt  {USArrests} is a four-column table that cannot readily be visualised on a two-dimensional surface.\relax }}{123}{table.12.1}\protected@file@percent }
\newlabel{tab:USArrests}{{12.1}{123}{\texttt {USArrests} is a dataset that is built into the \texttt {R} programming environment. It contains crime statistics (in arrests per 100,000 residents) for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percentage of the population living in urban areas. Thus, \texttt {USArrests} is a four-column table that cannot readily be visualised on a two-dimensional surface.\relax }{table.12.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.5}{\ignorespaces  PCA biplot of American crime statistics. The grey labels mark the different states, whilst the black vectors mark the crimes and the percentage of the population that lives in urban areas. States that have a lot of crime plot on the left hand side of the diagram, states with little crime plot towards the right. Heavily urbanised states plot at the bottom of the diagram, rural states plot near the top. \relax }}{124}{figure.12.5}\protected@file@percent }
\newlabel{fig:USArrests}{{12.5}{124}{PCA biplot of American crime statistics. The grey labels mark the different states, whilst the black vectors mark the crimes and the percentage of the population that lives in urban areas. States that have a lot of crime plot on the left hand side of the diagram, states with little crime plot towards the right. Heavily urbanised states plot at the bottom of the diagram, rural states plot near the top.\\\relax }{figure.12.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Multidimensional Scaling}{124}{section.12.2}\protected@file@percent }
\newlabel{sec:MDS}{{12.2}{124}{Multidimensional Scaling}{section.12.2}{}}
\newlabel{eq:euclidean}{{12.5}{124}{Multidimensional Scaling}{equation.12.2.5}{}}
\newlabel{eq:d}{{12.6}{124}{Multidimensional Scaling}{equation.12.2.6}{}}
\newlabel{eq:m}{{12.7}{125}{Multidimensional Scaling}{equation.12.2.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.6}{\ignorespaces Table of road distances (in km) between European cities. The full dataset comprises 21 cities.\relax }}{125}{figure.12.6}\protected@file@percent }
\newlabel{tab:eurodist}{{12.6}{125}{Table of road distances (in km) between European cities. The full dataset comprises 21 cities.\relax }{figure.12.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.7}{\ignorespaces MDS configuration of the European city distance data (Table\nobreakspace  {}\ref  {tab:eurodist}). Cities (such as Lyon and Geneva) that are close together in the real world plot close together on the MDS configuration. And cities (such as Stockholm and Athens) that are far apart in the real world plot on opposite ends of the MDS configuration. But whilst the MDS configuration preserves the distances, it does not preserve the orientation of the cities. In this figure, the y-axis has been flipped, and the city locations are rotated $\sim 15^\circ $ in a clockwise sense compared to the real map of Europe. \relax }}{126}{figure.12.7}\protected@file@percent }
\newlabel{fig:eurodist}{{12.7}{126}{MDS configuration of the European city distance data (Table~\ref {tab:eurodist}). Cities (such as Lyon and Geneva) that are close together in the real world plot close together on the MDS configuration. And cities (such as Stockholm and Athens) that are far apart in the real world plot on opposite ends of the MDS configuration. But whilst the MDS configuration preserves the distances, it does not preserve the orientation of the cities. In this figure, the y-axis has been flipped, and the city locations are rotated $\sim 15^\circ $ in a clockwise sense compared to the real map of Europe.\\\relax }{figure.12.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.8}{\ignorespaces The Shepard plot of the European city distances shows a good agreement between the true input distances from Table\nobreakspace  {}\ref  {tab:eurodist} (x-axis) and the fitted distances measured on the MDS configuration of Figure\nobreakspace  {}\ref  {fig:eurodist} (y-axis). There are 21 cities in the dataset, resulting in $21\times {20}/2=210$ pairwise distances. Hence there are 210 data points on this scatter plot. Most of the scatter of the data around the best fit line is caused by the fact that the input data are \emph  {road distances}, which do not perfectly agree with the straight line map distances. \relax }}{126}{figure.12.8}\protected@file@percent }
\newlabel{fig:Shepard}{{12.8}{126}{The Shepard plot of the European city distances shows a good agreement between the true input distances from Table~\ref {tab:eurodist} (x-axis) and the fitted distances measured on the MDS configuration of Figure~\ref {fig:eurodist} (y-axis). There are 21 cities in the dataset, resulting in $21\times {20}/2=210$ pairwise distances. Hence there are 210 data points on this scatter plot. Most of the scatter of the data around the best fit line is caused by the fact that the input data are \emph {road distances}, which do not perfectly agree with the straight line map distances.\\\relax }{figure.12.8}{}}
\newlabel{eq:stress}{{12.8}{126}{Multidimensional Scaling}{equation.12.2.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12.2}{\ignorespaces Rule of thumb for interpreting the goodness of fit of an MDS configuration.\relax }}{126}{table.12.2}\protected@file@percent }
\newlabel{tab:S}{{12.2}{126}{Rule of thumb for interpreting the goodness of fit of an MDS configuration.\relax }{table.12.2}{}}
\newlabel{eq:DZd}{{12.9}{127}{Multidimensional Scaling}{equation.12.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.9}{\ignorespaces MDS configuration of the detrital zircon U--Pb data. Samples that have similar age distributions (such as `Y' and `T') are characterised by low K-S statistics (e.g., $d[Y,T]=0.07$) and plot close together. Samples that have greatly differing age distributions (such as `Y' and `8') are characterised by high K-S statistics (e.g., $d[Y,5]=0.62$) and plot far apart on the MDS map. \relax }}{127}{figure.12.9}\protected@file@percent }
\newlabel{fig:DZmds}{{12.9}{127}{MDS configuration of the detrital zircon U--Pb data. Samples that have similar age distributions (such as `Y' and `T') are characterised by low K-S statistics (e.g., $d[Y,T]=0.07$) and plot close together. Samples that have greatly differing age distributions (such as `Y' and `8') are characterised by high K-S statistics (e.g., $d[Y,5]=0.62$) and plot far apart on the MDS map.\\\relax }{figure.12.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.3}K-means clustering}{128}{section.12.3}\protected@file@percent }
\newlabel{sec:kmeans}{{12.3}{128}{K-means clustering}{section.12.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.10}{\ignorespaces A two-dimensional dataset to illustrate the k-means clustering algorithm. There are 150 data points. In this first exercise we will try to classify them into three groups.\relax }}{128}{figure.12.10}\protected@file@percent }
\newlabel{fig:kmeans1}{{12.10}{128}{A two-dimensional dataset to illustrate the k-means clustering algorithm. There are 150 data points. In this first exercise we will try to classify them into three groups.\relax }{figure.12.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.11}{\ignorespaces Data points 48, 100 and 130 were randomly selected from the dataset and assigned as the centroids of clusters 1 (circle), 2 (triangle) and 3 (cross).\relax }}{128}{figure.12.11}\protected@file@percent }
\newlabel{fig:kmeans2}{{12.11}{128}{Data points 48, 100 and 130 were randomly selected from the dataset and assigned as the centroids of clusters 1 (circle), 2 (triangle) and 3 (cross).\relax }{figure.12.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.12}{\ignorespaces Replace each of the question marks in Figure\nobreakspace  {}\ref  {fig:kmeans2} with the symbol (circles, triangles or crosses) that is closest to it, using the Euclidean distance of Equation\nobreakspace  {}\ref  {eq:euclidean}.\relax }}{128}{figure.12.12}\protected@file@percent }
\newlabel{fig:kmeans3}{{12.12}{128}{Replace each of the question marks in Figure~\ref {fig:kmeans2} with the symbol (circles, triangles or crosses) that is closest to it, using the Euclidean distance of Equation~\ref {eq:euclidean}.\relax }{figure.12.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.13}{\ignorespaces The grey symbols are the same as the black symbols in Figure\nobreakspace  {}\ref  {fig:kmeans3}. The black symbols are the average $\{x,y\}$-positions of all the samples within each cluster. These are different than the previous values shown in Figure\nobreakspace  {}\ref  {fig:kmeans2}. The new values form the centroid of the clusters that will be used in the next iteration of the k-means algorithm.\relax }}{129}{figure.12.13}\protected@file@percent }
\newlabel{fig:kmeans4}{{12.13}{129}{The grey symbols are the same as the black symbols in Figure~\ref {fig:kmeans3}. The black symbols are the average $\{x,y\}$-positions of all the samples within each cluster. These are different than the previous values shown in Figure~\ref {fig:kmeans2}. The new values form the centroid of the clusters that will be used in the next iteration of the k-means algorithm.\relax }{figure.12.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.14}{\ignorespaces Final classification of the data. Each of the 150 data points has been assigned to a particular cluster whose centroids are marked as large and bold symbols.\relax }}{129}{figure.12.14}\protected@file@percent }
\newlabel{fig:kmeans5}{{12.14}{129}{Final classification of the data. Each of the 150 data points has been assigned to a particular cluster whose centroids are marked as large and bold symbols.\relax }{figure.12.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.15}{\ignorespaces Two-dimensional marginal distributions of R.A. Fisher's iris dataset, which comprises four variables measured in 150 different flowers belonging to three different species: \emph  {setosa} (circles), \emph  {versicolor} (triangles) and \emph  {virginica} (crosses). The two-dimensional dataset of Figures\nobreakspace  {}\ref  {fig:kmeans1}--\ref  {fig:kmeans5} was derived from panel (4,2), which sets out Petal Width against Sepal Width. The classification shown in Figure\nobreakspace  {}\ref  {fig:kmeans5} does a decent job at classifying the 150 flowers into three groups but the classification is not perfect. For example, the flower in the lower left corner of panel (4,2) belongs to \emph  {setosa} but was incorrectly classified as \emph  {versicolor} in Figure\nobreakspace  {}\ref  {fig:kmeans5}.  \relax }}{130}{figure.12.15}\protected@file@percent }
\newlabel{fig:Iris}{{12.15}{130}{Two-dimensional marginal distributions of R.A. Fisher's iris dataset, which comprises four variables measured in 150 different flowers belonging to three different species: \emph {setosa} (circles), \emph {versicolor} (triangles) and \emph {virginica} (crosses). The two-dimensional dataset of Figures~\ref {fig:kmeans1}--\ref {fig:kmeans5} was derived from panel (4,2), which sets out Petal Width against Sepal Width. The classification shown in Figure~\ref {fig:kmeans5} does a decent job at classifying the 150 flowers into three groups but the classification is not perfect. For example, the flower in the lower left corner of panel (4,2) belongs to \emph {setosa} but was incorrectly classified as \emph {versicolor} in Figure~\ref {fig:kmeans5}. \\\relax }{figure.12.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12.3}{\ignorespaces Classification results of the k-means algorithm applied to Fisher's iris data.\relax }}{130}{table.12.3}\protected@file@percent }
\newlabel{tab:kmeansIris}{{12.3}{130}{Classification results of the k-means algorithm applied to Fisher's iris data.\relax }{table.12.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Hierarchical clustering}{130}{section.12.4}\protected@file@percent }
\newlabel{sec:hierarchical}{{12.4}{130}{Hierarchical clustering}{section.12.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.16}{\ignorespaces A simple bivariate dataset that will be used to illustrate the hierarchical clustering algorithm:\relax }}{131}{figure.12.16}\protected@file@percent }
\newlabel{fig:hierarchical1}{{12.16}{131}{A simple bivariate dataset that will be used to illustrate the hierarchical clustering algorithm:\relax }{figure.12.16}{}}
\newlabel{it:hierachical1}{{1}{131}{Hierarchical clustering}{Item.168}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.17}{\ignorespaces First step of the hierarchical clustering process. The closest two samples (1 and 3) have been grouped together into a new cluster (grey line, left). The results can also be visualised as a tree or \textbf  {dendrogram} (right). \relax }}{131}{figure.12.17}\protected@file@percent }
\newlabel{fig:hierarchical2}{{12.17}{131}{First step of the hierarchical clustering process. The closest two samples (1 and 3) have been grouped together into a new cluster (grey line, left). The results can also be visualised as a tree or \textbf {dendrogram} (right).\\\relax }{figure.12.17}{}}
\newlabel{it:hierarchical2}{{3}{131}{Hierarchical clustering}{Item.170}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.18}{\ignorespaces The second step of the hierarchical clustering process shown as a scatter plot (left) and a dendrogram (right). The first order cluster is nested inside the second order one. \relax }}{132}{figure.12.18}\protected@file@percent }
\newlabel{fig:hierarchical3}{{12.18}{132}{The second step of the hierarchical clustering process shown as a scatter plot (left) and a dendrogram (right). The first order cluster is nested inside the second order one.\\\relax }{figure.12.18}{}}
\newlabel{it:hierarchical3}{{5}{132}{Hierarchical clustering}{Item.172}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.19}{\ignorespaces The third step of the hierarchical clustering process shown as a scatter plot (left) and a dendrogram (right). The third cluster does not share any elements with the first two clusters. \relax }}{132}{figure.12.19}\protected@file@percent }
\newlabel{fig:hierarchical4}{{12.19}{132}{The third step of the hierarchical clustering process shown as a scatter plot (left) and a dendrogram (right). The third cluster does not share any elements with the first two clusters.\\\relax }{figure.12.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12.20}{\ignorespaces Final results of the hierarchical cluster analysis. The tree consists of four nested clusters. The y-axis has units of distance: the longer the branch, the greater the difference between the corresponding clusters.\relax }}{132}{figure.12.20}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.21}{\ignorespaces Hierarchical clustering tree of R.A. Fisher's iris data. Labels have been omitted to reduce clutter. \relax }}{133}{figure.12.21}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12.22}{\ignorespaces Cutting the tree at height=3.6 (dashed line) produces a simple tree with three branches, which can be used to classify the iris flowers into three groups. \relax }}{133}{figure.12.22}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {12.4}{\ignorespaces Classification results of the hierarchical clustering algorithm applied to Fisher's iris data.\relax }}{133}{table.12.4}\protected@file@percent }
\newlabel{tab:hclustIris}{{12.4}{133}{Classification results of the hierarchical clustering algorithm applied to Fisher's iris data.\relax }{table.12.4}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Supervised learning}{135}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:supervised}{{13}{135}{Supervised learning}{chapter.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Discriminant Analysis}{135}{section.13.1}\protected@file@percent }
\newlabel{sec:LDA}{{13.1}{135}{Discriminant Analysis}{section.13.1}{}}
\newlabel{eq:bayesRule}{{13.1}{135}{Discriminant Analysis}{equation.13.1.1}{}}
\newlabel{eq:bayesTheorem}{{13.2}{135}{Discriminant Analysis}{equation.13.1.2}{}}
\newlabel{eq:multiNorm}{{13.3}{136}{Discriminant Analysis}{equation.13.1.3}{}}
\newlabel{eq:quadRule}{{13.4}{136}{Discriminant Analysis}{equation.13.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces Quadratic discriminant analysis of synthetic data: a) the bivariate training data comprise three classes; b) fitting three bivariate normal distributions to the data; c) decision boundaries between the three distributions. \relax }}{136}{figure.13.1}\protected@file@percent }
\newlabel{fig:QDA}{{13.1}{136}{Quadratic discriminant analysis of synthetic data: a) the bivariate training data comprise three classes; b) fitting three bivariate normal distributions to the data; c) decision boundaries between the three distributions.\\\relax }{figure.13.1}{}}
\newlabel{eq:linRule}{{13.5}{136}{Discriminant Analysis}{equation.13.1.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.2}{\ignorespaces Linear discriminant analysis of a second synthetic dataset: a) the bivariate training data comprise three classes; b) fitting three bivariate normal distributions to the data with the same covariance matrix; c) decision boundaries between the three distributions. \relax }}{137}{figure.13.2}\protected@file@percent }
\newlabel{fig:LDA}{{13.2}{137}{Linear discriminant analysis of a second synthetic dataset: a) the bivariate training data comprise three classes; b) fitting three bivariate normal distributions to the data with the same covariance matrix; c) decision boundaries between the three distributions.\\\relax }{figure.13.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.3}{\ignorespaces a) an equal mixture of two bivariate normal datasets; b) PCA extracts the major axis of the best fitting error ellipse to the merged dataset as the first principal component; c) LDA fits two error ellipses to the data and extracts a function (the first linear discriminant) that maximises the distance between them. In this example, this produces a line that is perpendicular to the first principal component. \relax }}{137}{figure.13.3}\protected@file@percent }
\newlabel{fig:PCAvsLDA}{{13.3}{137}{a) an equal mixture of two bivariate normal datasets; b) PCA extracts the major axis of the best fitting error ellipse to the merged dataset as the first principal component; c) LDA fits two error ellipses to the data and extracts a function (the first linear discriminant) that maximises the distance between them. In this example, this produces a line that is perpendicular to the first principal component.\\\relax }{figure.13.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.4}{\ignorespaces The first two linear discriminants of the four-dimensional iris data represent a two-dimensional projection of these data that maximises the differences between the three different species of flowers contained in them. They are defined as  LD1 = 0.83 $\times $ (sepal length - 5.84) + 1.53 $\times $ (sepal width - 3.06) - 2.20 $\times $ (petal length - 3.76) - 2.81 $\times $ (petal width - 1.20); and  LD2 = 0.024 $\times $ (sepal length - 5.85) + 2.16 $\times $ (sepal width - 3.06) - 0.93 $\times $ (petal length - 3.76) + 2.84 $\times $ (petal width - 1.20). \relax }}{138}{figure.13.4}\protected@file@percent }
\newlabel{fig:LDAiris}{{13.4}{138}{The first two linear discriminants of the four-dimensional iris data represent a two-dimensional projection of these data that maximises the differences between the three different species of flowers contained in them. They are defined as\\ LD1 = 0.83 $\times $ (sepal length - 5.84) + 1.53 $\times $ (sepal width - 3.06) - 2.20 $\times $ (petal length - 3.76) - 2.81 $\times $ (petal width - 1.20); and\\ LD2 = 0.024 $\times $ (sepal length - 5.85) + 2.16 $\times $ (sepal width - 3.06) - 0.93 $\times $ (petal length - 3.76) + 2.84 $\times $ (petal width - 1.20).\\\relax }{figure.13.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.5}{\ignorespaces  Using the two discriminant functions of Figure\nobreakspace  {}\ref  {fig:LDAiris}:  LD1 = $0.83\times (6.0-5.84)+1.53\times (3.0-3.06)-2.20\times (5.0-3.76)-2.81\times (1.5-1.20)=-3.53$; and  LD2 = $0.024\times (6.0-5.85)+2.16\times (3.0-3.06)-0.93\times (5.0-3.76)+2.84\times (1.5-1.20)=-0.42$.  Plotting these two coordinates on the discrimination diagram of Figure\nobreakspace  {}\ref  {fig:LDAiris} suggests that the new flower belongs to the \textit  {versicolor} species.  \relax }}{138}{figure.13.5}\protected@file@percent }
\newlabel{fig:LDAnewiris}{{13.5}{138}{Using the two discriminant functions of Figure~\ref {fig:LDAiris}:\\ LD1 = $0.83\times (6.0-5.84)+1.53\times (3.0-3.06)-2.20\times (5.0-3.76)-2.81\times (1.5-1.20)=-3.53$; and\\ LD2 = $0.024\times (6.0-5.85)+2.16\times (3.0-3.06)-0.93\times (5.0-3.76)+2.84\times (1.5-1.20)=-0.42$.\\ Plotting these two coordinates on the discrimination diagram of Figure~\ref {fig:LDAiris} suggests that the new flower belongs to the \textit {versicolor} species. \\\relax }{figure.13.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13.2}Decision trees}{138}{section.13.2}\protected@file@percent }
\newlabel{sec:CART}{{13.2}{138}{Decision trees}{section.13.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.6}{\ignorespaces Synthetic dataset of bivariate data that belong to two classes. The grey circles are split into two data clouds. The white circles form a `c'-shape surrounding one of the modes of the grey population. Discriminant analysis is unable to deal with this situation. \relax }}{139}{figure.13.6}\protected@file@percent }
\newlabel{fig:CARTdata}{{13.6}{139}{Synthetic dataset of bivariate data that belong to two classes. The grey circles are split into two data clouds. The white circles form a `c'-shape surrounding one of the modes of the grey population. Discriminant analysis is unable to deal with this situation.\\\relax }{figure.13.6}{}}
\newlabel{eq:Q}{{13.6}{139}{Decision trees}{equation.13.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.7}{\ignorespaces Evaluation of three candidate split points using the Gini index $Q$. The best first order split corresponds to $x=-4.902$ (panel c). \relax }}{139}{figure.13.7}\protected@file@percent }
\newlabel{fig:Qtries}{{13.7}{139}{Evaluation of three candidate split points using the Gini index $Q$. The best first order split corresponds to $x=-4.902$ (panel c).\\\relax }{figure.13.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.8}{\ignorespaces Second order partition of the right hand side of Figure\nobreakspace  {}\ref  {fig:Qtries}.c). The results can be visualised as a dendrogram or tree (right). The `leaves' of this tree are annotated as $n_\circ /n_\bullet $ where $n_\circ $ is the number of training data of class $\circ $ and $n_\bullet $ is the number of training data of class $\bullet $. \relax }}{140}{figure.13.8}\protected@file@percent }
\newlabel{fig:Q2}{{13.8}{140}{Second order partition of the right hand side of Figure~\ref {fig:Qtries}.c). The results can be visualised as a dendrogram or tree (right). The `leaves' of this tree are annotated as $n_\circ /n_\bullet $ where $n_\circ $ is the number of training data of class $\circ $ and $n_\bullet $ is the number of training data of class $\bullet $.\\\relax }{figure.13.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.9}{\ignorespaces A recursive binary partition tree that perfectly classifies all the samples in Figure\nobreakspace  {}\ref  {fig:CARTdata}. The annotations of the dendrogram have been removed to reduce clutter.  \relax }}{140}{figure.13.9}\protected@file@percent }
\newlabel{fig:overfittedCARTscatter}{{13.9}{140}{A recursive binary partition tree that perfectly classifies all the samples in Figure~\ref {fig:CARTdata}. The annotations of the dendrogram have been removed to reduce clutter. \\\relax }{figure.13.9}{}}
\newlabel{it:cv1}{{2}{140}{Decision trees}{Item.176}{}}
\newlabel{it:cv2}{{3}{140}{Decision trees}{Item.177}{}}
\newlabel{eq:cp}{{13.7}{141}{Decision trees}{equation.13.2.7}{}}
\newlabel{eq:Ta}{{13.8}{141}{Decision trees}{equation.13.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.10}{\ignorespaces A plot of cross validated (CV) prediction error versus the number of nodes in the collection of nested subtrees shows a minimum at five splits. The CV error of small trees is caused by bias, while the CV error of large tree is a result of variance. There typically exist several trees with CV errors close to the minimum. Therefore, a `1-SE rule' is used, i.e., choosing the smallest tree whose misclassification rate does not exceed the minimum CV error plus one standard error of the smallest CV error (dotted line). For the example dataset, this supports an optimal tree with four splits. \relax }}{141}{figure.13.10}\protected@file@percent }
\newlabel{fig:cvCART}{{13.10}{141}{A plot of cross validated (CV) prediction error versus the number of nodes in the collection of nested subtrees shows a minimum at five splits. The CV error of small trees is caused by bias, while the CV error of large tree is a result of variance. There typically exist several trees with CV errors close to the minimum. Therefore, a `1-SE rule' is used, i.e., choosing the smallest tree whose misclassification rate does not exceed the minimum CV error plus one standard error of the smallest CV error (dotted line). For the example dataset, this supports an optimal tree with four splits.\\\relax }{figure.13.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.11}{\ignorespaces The optimal tree for the bivariate test data. This tree misclassifies 19 of the 300 samples in the training data (6.3\%). The 10-fold cross validation error is 18\%. \relax }}{141}{figure.13.11}\protected@file@percent }
\newlabel{fig:optimalCART}{{13.11}{141}{The optimal tree for the bivariate test data. This tree misclassifies 19 of the 300 samples in the training data (6.3\%). The 10-fold cross validation error is 18\%.\\\relax }{figure.13.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13.12}{\ignorespaces The optimal tree for Fisher's iris data. There are four variables (instead of two for the previous example) and three classes (instead of two for the previous example). The optimal tree misclassifies 6 of the 150 flowers in the training data (4\%). The 10-fold cross validation error is 6\%. \relax }}{142}{figure.13.12}\protected@file@percent }
\newlabel{fig:irisCART}{{13.12}{142}{The optimal tree for Fisher's iris data. There are four variables (instead of two for the previous example) and three classes (instead of two for the previous example). The optimal tree misclassifies 6 of the 150 flowers in the training data (4\%). The 10-fold cross validation error is 6\%.\\\relax }{figure.13.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Compositional data}{143}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:compositional}{{14}{143}{Compositional data}{chapter.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Ratio data}{143}{section.14.1}\protected@file@percent }
\newlabel{sec:ratios}{{14.1}{143}{Ratio data}{section.14.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Logratio transformations}{144}{section.14.2}\protected@file@percent }
\newlabel{sec:logratios}{{14.2}{144}{Logratio transformations}{section.14.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces The A--CN--K diagram is a widely used graphical device in chemical weathering studies. It is a ternary diagram of Al\textsubscript  {2}O\textsubscript  {3}, CaO + Na\textsubscript  {2}O and K\textsubscript  {2}O, where the CaO refers to the silicate component of the sediment only (ignoring carbonates). The composition on the diagram results from the competing effects of initial starting composition and chemical weathering. With increasing weathering intensity, A--CN--K compositions get pulled towards the Al\textsubscript  {2}O\textsubscript  {3} apex of the ternary diagram. This figure shows a synthetic dataset of 20 A--CN--K measurements that have been affected by variable weathering intensities. \relax }}{145}{figure.14.1}\protected@file@percent }
\newlabel{fig:ACNK}{{14.1}{145}{The A--CN--K diagram is a widely used graphical device in chemical weathering studies. It is a ternary diagram of Al\textsubscript {2}O\textsubscript {3}, CaO + Na\textsubscript {2}O and K\textsubscript {2}O, where the CaO refers to the silicate component of the sediment only (ignoring carbonates). The composition on the diagram results from the competing effects of initial starting composition and chemical weathering. With increasing weathering intensity, A--CN--K compositions get pulled towards the Al\textsubscript {2}O\textsubscript {3} apex of the ternary diagram. This figure shows a synthetic dataset of 20 A--CN--K measurements that have been affected by variable weathering intensities.\\\relax }{figure.14.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces  The black square represents the average A--CN--K composition of the 20 samples of Figure\nobreakspace  {}\ref  {fig:ACNK}. It is obtained by taking the arithmetic mean of all the Al\textsubscript  {2}O\textsubscript  {3}, (CaO + Na\textsubscript  {2}O) and K\textsubscript  {2}O concentrations using Equation\nobreakspace  {}\ref  {eq:mean}, and plotting the resulting 3-element vector as a new sample. This average composition plots outside the sample cloud, which is a meaningless result not unlike the porosity example of Figure\nobreakspace  {}\ref  {fig:porositylocation}.a. \relax }}{145}{figure.14.2}\protected@file@percent }
\newlabel{fig:ACNKarithmeticmean}{{14.2}{145}{The black square represents the average A--CN--K composition of the 20 samples of Figure~\ref {fig:ACNK}. It is obtained by taking the arithmetic mean of all the Al\textsubscript {2}O\textsubscript {3}, (CaO + Na\textsubscript {2}O) and K\textsubscript {2}O concentrations using Equation~\ref {eq:mean}, and plotting the resulting 3-element vector as a new sample. This average composition plots outside the sample cloud, which is a meaningless result not unlike the porosity example of Figure~\ref {fig:porositylocation}.a.\\\relax }{figure.14.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces The black polygon represents a `95\% confidence region' for the arithmetic mean of Figure\nobreakspace  {}\ref  {fig:ACNKarithmeticmean}. It is obtained by 1) calculating the standard deviations of the Al\textsubscript  {2}O\textsubscript  {3}, (CaO + Na\textsubscript  {2}O) and K\textsubscript  {2}O concentrations using Equation\nobreakspace  {}\ref  {eq:stdev}; 2) multiplying these values by two; and 3) subtracting or adding these values to the arithmetic mean of Figure\nobreakspace  {}\ref  {fig:ACNKarithmeticmean}. The resulting `2-sigma' confidence polygon plots outside the ternary diagram, in physically impossible negative data space. \relax }}{146}{figure.14.3}\protected@file@percent }
\newlabel{fig:ACNKnaive}{{14.3}{146}{The black polygon represents a `95\% confidence region' for the arithmetic mean of Figure~\ref {fig:ACNKarithmeticmean}. It is obtained by 1) calculating the standard deviations of the Al\textsubscript {2}O\textsubscript {3}, (CaO + Na\textsubscript {2}O) and K\textsubscript {2}O concentrations using Equation~\ref {eq:stdev}; 2) multiplying these values by two; and 3) subtracting or adding these values to the arithmetic mean of Figure~\ref {fig:ACNKarithmeticmean}. The resulting `2-sigma' confidence polygon plots outside the ternary diagram, in physically impossible negative data space.\\\relax }{figure.14.3}{}}
\newlabel{eq:alr}{{14.1}{146}{Logratio transformations}{equation.14.2.1}{}}
\newlabel{eq:inverse-logratio-transformation}{{14.2}{147}{Logratio transformations}{equation.14.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.4}{\ignorespaces The additive logratio transformation (Equation\nobreakspace  {}\ref  {eq:alr}) maps data from an $n$-dimensional compositional space to an $(n-1)$-dimensional Euclidean space. For the A--CN--K data, it maps the data from a ternary diagram ($n=3$) to a bivariate ($n-1=2$) dataspace using Equation\nobreakspace  {}\ref  {eq:alr}. In this transformed space, it is safe to calculate the arithmetic mean (black square) and confidence regions (black ellipse). After completion of these calculations, the result can be mapped back to the ternary diagram using the inverse logratio transformation (Equation\nobreakspace  {}\ref  {eq:inverse-logratio-transformation}). \relax }}{147}{figure.14.4}\protected@file@percent }
\newlabel{fig:alr}{{14.4}{147}{The additive logratio transformation (Equation~\ref {eq:alr}) maps data from an $n$-dimensional compositional space to an $(n-1)$-dimensional Euclidean space. For the A--CN--K data, it maps the data from a ternary diagram ($n=3$) to a bivariate ($n-1=2$) dataspace using Equation~\ref {eq:alr}. In this transformed space, it is safe to calculate the arithmetic mean (black square) and confidence regions (black ellipse). After completion of these calculations, the result can be mapped back to the ternary diagram using the inverse logratio transformation (Equation~\ref {eq:inverse-logratio-transformation}).\\\relax }{figure.14.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}PCA of compositional data}{148}{section.14.3}\protected@file@percent }
\newlabel{sec:compositionalPCA}{{14.3}{148}{PCA of compositional data}{section.14.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {14.1}{\ignorespaces The major element composition (in weight percent) of 16 samples of Namib dune sand.\relax }}{148}{table.14.1}\protected@file@percent }
\newlabel{tab:Major}{{14.1}{148}{The major element composition (in weight percent) of 16 samples of Namib dune sand.\relax }{table.14.1}{}}
\newlabel{eq:Xcomp}{{14.3}{148}{PCA of compositional data}{equation.14.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.5}{\ignorespaces Ternary diagram of the synthetic toy example of Equation\nobreakspace  {}\ref  {eq:Xcomp}. Component $c$ has been multiplied by a factor of 5 to avoid an unsightly overlap between the plot symbols of samples 2 and 3, whose compositions are very similar. \relax }}{148}{figure.14.5}\protected@file@percent }
\newlabel{fig:abc}{{14.5}{148}{Ternary diagram of the synthetic toy example of Equation~\ref {eq:Xcomp}. Component $c$ has been multiplied by a factor of 5 to avoid an unsightly overlap between the plot symbols of samples 2 and 3, whose compositions are very similar.\\\relax }{figure.14.5}{}}
\newlabel{eq:Xa}{{14.4}{149}{PCA of compositional data}{equation.14.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.6}{\ignorespaces PCA biplot of the ternary toy data of Figure\nobreakspace  {}\ref  {fig:abc} after alr-transformation. Note that this result is nearly identical to the toy example of Figure\nobreakspace  {}\ref  {fig:PCA2D3}. The only difference is the labels of the vector loadings, which are logatios instead of the names of the original variables. \relax }}{149}{figure.14.6}\protected@file@percent }
\newlabel{fig:alrPCA}{{14.6}{149}{PCA biplot of the ternary toy data of Figure~\ref {fig:abc} after alr-transformation. Note that this result is nearly identical to the toy example of Figure~\ref {fig:PCA2D3}. The only difference is the labels of the vector loadings, which are logatios instead of the names of the original variables.\\\relax }{figure.14.6}{}}
\newlabel{eq:clr}{{14.5}{149}{PCA of compositional data}{equation.14.3.5}{}}
\newlabel{eq:Xc}{{14.6}{149}{PCA of compositional data}{equation.14.3.6}{}}
\newlabel{eq:PCAcomp}{{14.7}{150}{PCA of compositional data}{equation.14.3.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.7}{\ignorespaces Compositional biplot of the toy data using the clr transformation. The principal components are identical to Figure\nobreakspace  {}\ref  {fig:alrPCA}, but the vector loadings are labelled with the raw variable names, rather than the additive logratios. The biplot tells use that sample\nobreakspace  {}1 is rich in component $a$, whereas samples\nobreakspace  {}2 and 3 are rich in component $b$. The difference between samples\nobreakspace  {}2 and 3 is due to a small difference in component $c$. Compare with the ternary diagram (Figure\nobreakspace  {}\ref  {fig:abc}) to verify these conclusions. \relax }}{150}{figure.14.7}\protected@file@percent }
\newlabel{fig:clrPCA}{{14.7}{150}{Compositional biplot of the toy data using the clr transformation. The principal components are identical to Figure~\ref {fig:alrPCA}, but the vector loadings are labelled with the raw variable names, rather than the additive logratios. The biplot tells use that sample~1 is rich in component $a$, whereas samples~2 and 3 are rich in component $b$. The difference between samples~2 and 3 is due to a small difference in component $c$. Compare with the ternary diagram (Figure~\ref {fig:abc}) to verify these conclusions.\\\relax }{figure.14.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.8}{\ignorespaces Compositional biplot of the major element concentration data of Table\nobreakspace  {}\ref  {tab:Major}, with the samples shown in grey and the vector loadings in black. Samples that plot close together (such as N1 and N4) have similar compositions. The arrows indicate that sample N8 is enriched in MnO relative to sample T8. The arrows for MgO and CaO plot in the same direction, suggesting that these two oxides are correlated. The arrow for K\textsubscript  {2}O points in the opposite direction, suggesting that this oxide anticorrelated with MgO and CaO. The \textbf  {link} between K\textsubscript  {2}O and MgO is perpendicular to the link between TiO\textsubscript  {2} and P\textsubscript  {2}O\textsubscript  {5}, suggesting that the variability of these two \textbf  {subcompositions} is statistically independent. \relax }}{150}{figure.14.8}\protected@file@percent }
\newlabel{fig:majorPCA}{{14.8}{150}{Compositional biplot of the major element concentration data of Table~\ref {tab:Major}, with the samples shown in grey and the vector loadings in black. Samples that plot close together (such as N1 and N4) have similar compositions. The arrows indicate that sample N8 is enriched in MnO relative to sample T8. The arrows for MgO and CaO plot in the same direction, suggesting that these two oxides are correlated. The arrow for K\textsubscript {2}O points in the opposite direction, suggesting that this oxide anticorrelated with MgO and CaO. The \textbf {link} between K\textsubscript {2}O and MgO is perpendicular to the link between TiO\textsubscript {2} and P\textsubscript {2}O\textsubscript {5}, suggesting that the variability of these two \textbf {subcompositions} is statistically independent.\\\relax }{figure.14.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.4}LDA of compositional data}{151}{section.14.4}\protected@file@percent }
\newlabel{sec:compositionalLDA}{{14.4}{151}{LDA of compositional data}{section.14.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.9}{\ignorespaces A dataset of igneous rock compositions from Iceland and the Cascades Mountains on a ternary A--F--M diagram (where A = Na\textsubscript  {2}O+K\textsubscript  {2}O, F = Fe\textsubscript  {2}O\textsubscript  {3}+FeO and M = MgO). The white dots define a `Fenner' trend marking the tholeiitic suite of igneous rocks from Iceland. The black dots define a `Bowen' trend, marking the calc-alkaline suite of rocks from the Cascades.  \relax }}{151}{figure.14.9}\protected@file@percent }
\newlabel{fig:AFM}{{14.9}{151}{A dataset of igneous rock compositions from Iceland and the Cascades Mountains on a ternary A--F--M diagram (where A = Na\textsubscript {2}O+K\textsubscript {2}O, F = Fe\textsubscript {2}O\textsubscript {3}+FeO and M = MgO). The white dots define a `Fenner' trend marking the tholeiitic suite of igneous rocks from Iceland. The black dots define a `Bowen' trend, marking the calc-alkaline suite of rocks from the Cascades. \\\relax }{figure.14.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.10}{\ignorespaces The additive logratio transformation liberates the A--F--M data from the confines of the ternary diagram and maps them to a Euclidean dataspace in which logratios are free to take any value from $-\infty $ to $+\infty $. In this space, the ln(M/A) vs. ln(F/A) values of the tholeiitic and calc-alkali rock suites are clustered into two discrete clouds of roughly equal size that have all the hallmarks of bivariate normal distributions with a shared covariance matrix. Thus the transformed data seems well suited for linear discriminant analysis. \relax }}{151}{figure.14.10}\protected@file@percent }
\newlabel{fig:lrAFM}{{14.10}{151}{The additive logratio transformation liberates the A--F--M data from the confines of the ternary diagram and maps them to a Euclidean dataspace in which logratios are free to take any value from $-\infty $ to $+\infty $. In this space, the ln(M/A) vs. ln(F/A) values of the tholeiitic and calc-alkali rock suites are clustered into two discrete clouds of roughly equal size that have all the hallmarks of bivariate normal distributions with a shared covariance matrix. Thus the transformed data seems well suited for linear discriminant analysis.\\\relax }{figure.14.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.11}{\ignorespaces LDA of the A--F--M data shown in a) logratio space and b) a ternary diagram. \relax }}{152}{figure.14.11}\protected@file@percent }
\newlabel{fig:LDAAFM}{{14.11}{152}{LDA of the A--F--M data shown in a) logratio space and b) a ternary diagram.\\\relax }{figure.14.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {14.5}Logratio processes}{152}{section.14.5}\protected@file@percent }
\newlabel{sec:logratio-processes}{{14.5}{152}{Logratio processes}{section.14.5}{}}
\newlabel{eq:decay}{{14.8}{152}{Logratio processes}{equation.14.5.8}{}}
\newlabel{eq:expdecay}{{14.9}{152}{Logratio processes}{equation.14.5.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.12}{\ignorespaces Exponential decay processes of compositional data become linear trends in logratio space. Initial composition $i$ forms the starting point of two trajectories, marked by dashed and solid lines, respectively. These trends are characterised by different decay parameters. \relax }}{153}{figure.14.12}\protected@file@percent }
\newlabel{fig:cath}{{14.12}{153}{Exponential decay processes of compositional data become linear trends in logratio space. Initial composition $i$ forms the starting point of two trajectories, marked by dashed and solid lines, respectively. These trends are characterised by different decay parameters.\\\relax }{figure.14.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}An introduction to \texttt  {R}}{155}{chapter.15}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:R}{{15}{155}{An introduction to \texttt {R}}{chapter.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}The basics}{155}{section.15.1}\protected@file@percent }
\newlabel{sec:R-basics}{{15.1}{155}{The basics}{section.15.1}{}}
\newlabel{it:geostats}{{13}{161}{The basics}{Item.192}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.2}Plotting data}{161}{section.15.2}\protected@file@percent }
\newlabel{sec:R-plotting}{{15.2}{161}{Plotting data}{section.15.2}{}}
\newlabel{it:anscombe}{{1}{161}{Plotting data}{Item.193}{}}
\newlabel{it:KDE}{{3}{163}{Plotting data}{Item.195}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Summary Statistics}{164}{section.15.3}\protected@file@percent }
\newlabel{sec:R-summary-statistics}{{15.3}{164}{Summary Statistics}{section.15.3}{}}
\newlabel{it:mode}{{2}{165}{Summary Statistics}{Item.199}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Probability}{165}{section.15.4}\protected@file@percent }
\newlabel{sec:R-probability}{{15.4}{165}{Probability}{section.15.4}{}}
\newlabel{fn:mode}{{2}{165}{}{Hfootnote.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.5}The binomial distribution}{166}{section.15.5}\protected@file@percent }
\newlabel{sec:R-binomial}{{15.5}{166}{The binomial distribution}{section.15.5}{}}
\newlabel{it:1sidedbinomR}{{4}{167}{The binomial distribution}{Item.206}{}}
\newlabel{it:2sidedbinomR}{{5}{168}{The binomial distribution}{Item.207}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.6}The Poisson distribution}{169}{section.15.6}\protected@file@percent }
\newlabel{sec:R-poisson}{{15.6}{169}{The Poisson distribution}{section.15.6}{}}
\newlabel{it:1sidedpoisR}{{4}{169}{The Poisson distribution}{Item.213}{}}
\newlabel{it:2sidedpoisR}{{5}{169}{The Poisson distribution}{Item.214}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.7}The normal distribution}{170}{section.15.7}\protected@file@percent }
\newlabel{sec:R-gauss}{{15.7}{170}{The normal distribution}{section.15.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.8}Error propagation}{171}{section.15.8}\protected@file@percent }
\newlabel{sec:R-errorprop}{{15.8}{171}{Error propagation}{section.15.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces Log-likelihood function for the Poisson distribution, evaluated at different values for the parameter $\lambda $, given an observation of $k=4$ successes. The function reaches a maximum value at $\hat  {\lambda }=4$. \relax }}{172}{figure.15.1}\protected@file@percent }
\newlabel{fig:LLpois}{{15.1}{172}{Log-likelihood function for the Poisson distribution, evaluated at different values for the parameter $\lambda $, given an observation of $k=4$ successes. The function reaches a maximum value at $\hat {\lambda }=4$. \relax }{figure.15.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.9}Comparing distributions}{173}{section.15.9}\protected@file@percent }
\newlabel{sec:R-comparingdistributions}{{15.9}{173}{Comparing distributions}{section.15.9}{}}
\newlabel{it:2sided2samplettest}{{3}{173}{Comparing distributions}{Item.226}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.10}Regression}{175}{section.15.10}\protected@file@percent }
\newlabel{sec:R-regression}{{15.10}{175}{Regression}{section.15.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.11}Fractals and chaos}{178}{section.15.11}\protected@file@percent }
\newlabel{sec:R-fractals}{{15.11}{178}{Fractals and chaos}{section.15.11}{}}
\newlabel{it:finland}{{1}{178}{Fractals and chaos}{Item.235}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.12}Unsupervised learning}{179}{section.15.12}\protected@file@percent }
\newlabel{sec:R-unsupervised}{{15.12}{179}{Unsupervised learning}{section.15.12}{}}
\newlabel{it:R-kmeans}{{5}{180}{Unsupervised learning}{Item.242}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.13}Supervised learning}{181}{section.15.13}\protected@file@percent }
\newlabel{sec:R-supervised}{{15.13}{181}{Supervised learning}{section.15.13}{}}
\newlabel{it:LDA}{{1}{181}{Supervised learning}{Item.244}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15.14}Compositional data}{182}{section.15.14}\protected@file@percent }
\newlabel{sec:R-compositional}{{15.14}{182}{Compositional data}{section.15.14}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Exercises}{185}{chapter.16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:exercises}{{16}{185}{Exercises}{chapter.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}The basics}{185}{section.16.1}\protected@file@percent }
\newlabel{sec:ex-basics}{{16.1}{185}{The basics}{section.16.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.2}Plotting data}{185}{section.16.2}\protected@file@percent }
\newlabel{sec:ex-plotting}{{16.2}{185}{Plotting data}{section.16.2}{}}
\newlabel{it:AB}{{2}{185}{Plotting data}{Item.256}{}}
\newlabel{it:randxy}{{3}{185}{Plotting data}{Item.257}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.3}Summary statistics}{186}{section.16.3}\protected@file@percent }
\newlabel{sec:ex-summary-statistics}{{16.3}{186}{Summary statistics}{section.16.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.4}Probability}{186}{section.16.4}\protected@file@percent }
\newlabel{sec:ex-probability}{{16.4}{186}{Probability}{section.16.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.5}The binomial distribution}{186}{section.16.5}\protected@file@percent }
\newlabel{sec:ex-binomial}{{16.5}{186}{The binomial distribution}{section.16.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.6}The Poisson distribution}{187}{section.16.6}\protected@file@percent }
\newlabel{sec:ex-poisson}{{16.6}{187}{The Poisson distribution}{section.16.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.7}The normal distribution}{187}{section.16.7}\protected@file@percent }
\newlabel{sec:ex-gauss}{{16.7}{187}{The normal distribution}{section.16.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.8}Error propagation}{188}{section.16.8}\protected@file@percent }
\newlabel{sec:ex-errorprop}{{16.8}{188}{Error propagation}{section.16.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.9}Comparing distributions}{189}{section.16.9}\protected@file@percent }
\newlabel{sec:ex-comparingdistributions}{{16.9}{189}{Comparing distributions}{section.16.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.10}Regression}{189}{section.16.10}\protected@file@percent }
\newlabel{sec:ex-regression}{{16.10}{189}{Regression}{section.16.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.11}Fractals and chaos}{190}{section.16.11}\protected@file@percent }
\newlabel{sec:ex-fractals}{{16.11}{190}{Fractals and chaos}{section.16.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.12}Unsupervised learning}{190}{section.16.12}\protected@file@percent }
\newlabel{sec:ex-unsupervised}{{16.12}{190}{Unsupervised learning}{section.16.12}{}}
\newlabel{it:ex-withinss}{{3}{190}{Unsupervised learning}{Item.305}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.13}Supervised learning}{191}{section.16.13}\protected@file@percent }
\newlabel{sec:ex-supervised}{{16.13}{191}{Supervised learning}{section.16.13}{}}
\newlabel{it:LDA-training}{{1}{191}{Supervised learning}{Item.307}{}}
\newlabel{it:LDA-test}{{2}{191}{Supervised learning}{Item.308}{}}
\newlabel{it:rpart-training}{{3}{191}{Supervised learning}{Item.309}{}}
\newlabel{it:rpart-test}{{4}{191}{Supervised learning}{Item.310}{}}
\@writefile{toc}{\contentsline {section}{\numberline {16.14}Compositional data}{191}{section.16.14}\protected@file@percent }
\newlabel{sec:ex-compositional}{{16.14}{191}{Compositional data}{section.16.14}{}}
\newlabel{it:mpg1}{{1a}{191}{Compositional data}{Item.312}{}}
\newlabel{it:mpg2}{{1b}{191}{Compositional data}{Item.313}{}}
\newlabel{it:mpg3}{{1c}{191}{Compositional data}{Item.314}{}}
\newlabel{it:mpg4}{{1d}{191}{Compositional data}{Item.315}{}}
\newlabel{it:mpg5}{{1e}{191}{Compositional data}{Item.316}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Solutions}{193}{chapter.17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:solutions}{{17}{193}{Solutions}{chapter.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}The basics}{193}{section.17.1}\protected@file@percent }
\newlabel{sec:sol-basics}{{17.1}{193}{The basics}{section.17.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Plotting data}{194}{section.17.2}\protected@file@percent }
\newlabel{sec:sol-plotting}{{17.2}{194}{Plotting data}{section.17.2}{}}
\newlabel{it:ABsol}{{2}{194}{Plotting data}{Item.325}{}}
\newlabel{it:xyrandsol}{{3}{195}{Plotting data}{Item.326}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.3}Summary statistics}{195}{section.17.3}\protected@file@percent }
\newlabel{sec:sol-summary-statistics}{{17.3}{195}{Summary statistics}{section.17.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.4}Probability}{198}{section.17.4}\protected@file@percent }
\newlabel{sec:sol-probability}{{17.4}{198}{Probability}{section.17.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.5}The binomial distribution}{199}{section.17.5}\protected@file@percent }
\newlabel{sec:sol-binomial}{{17.5}{199}{The binomial distribution}{section.17.5}{}}
\newlabel{it:binomtest}{{3}{200}{The binomial distribution}{Item.338}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.6}The Poisson distribution}{201}{section.17.6}\protected@file@percent }
\newlabel{sec:sol-poisson}{{17.6}{201}{The Poisson distribution}{section.17.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.7}The normal distribution}{202}{section.17.7}\protected@file@percent }
\newlabel{sec:sol-gauss}{{17.7}{202}{The normal distribution}{section.17.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.8}Error propagation}{205}{section.17.8}\protected@file@percent }
\newlabel{sec:sol-errorprop}{{17.8}{205}{Error propagation}{section.17.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.9}Comparing distributions}{206}{section.17.9}\protected@file@percent }
\newlabel{sec:sol-comparingdistributions}{{17.9}{206}{Comparing distributions}{section.17.9}{}}
\newlabel{it:sol-DZ-MDS}{{4}{208}{Comparing distributions}{Item.363}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.10}Regression}{208}{section.17.10}\protected@file@percent }
\newlabel{sec:sol-regression}{{17.10}{208}{Regression}{section.17.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.11}Fractals and chaos}{210}{section.17.11}\protected@file@percent }
\newlabel{sec:sol-fractals}{{17.11}{210}{Fractals and chaos}{section.17.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.12}Unsupervised learning}{212}{section.17.12}\protected@file@percent }
\newlabel{sec:sol-unsupervised}{{17.12}{212}{Unsupervised learning}{section.17.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.1}{\ignorespaces PCA biplot for the \texttt  {iris} data. This diagram indicates there are two groups of flowers that with distinct petal width and length measurements (PC1). Within these two groups, additional variability is caused by the sepal width and length (PC2). The vector loadings of the petal width and length point in the same direction, which means that these two variables are correlated with each other. The vector loading of the sepal width is perpendicular to that of the petal measurements, which means that the sepal and petal dimensions vary independently. \relax }}{212}{figure.17.1}\protected@file@percent }
\newlabel{fig:PCAiris}{{17.1}{212}{PCA biplot for the \texttt {iris} data. This diagram indicates there are two groups of flowers that with distinct petal width and length measurements (PC1). Within these two groups, additional variability is caused by the sepal width and length (PC2). The vector loadings of the petal width and length point in the same direction, which means that these two variables are correlated with each other. The vector loading of the sepal width is perpendicular to that of the petal measurements, which means that the sepal and petal dimensions vary independently.\\\relax }{figure.17.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.2}{\ignorespaces Evaluating the within-cluster sum-of-squares ($ss$) of the k-means algorithm for different numbers of clusters. The $ss$-misfit drops off very quicky before making an `elbow' at $k=3$ clusters. Hence we cannot justify more than 3 clusters. \relax }}{213}{figure.17.2}\protected@file@percent }
\newlabel{fig:elbow}{{17.2}{213}{Evaluating the within-cluster sum-of-squares ($ss$) of the k-means algorithm for different numbers of clusters. The $ss$-misfit drops off very quicky before making an `elbow' at $k=3$ clusters. Hence we cannot justify more than 3 clusters.\\\relax }{figure.17.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.3}{\ignorespaces Hierarchical clustering tree for the detrital zircon data. The tree conveys the same information as the MDS configuration of Figure\nobreakspace  {}\ref  {fig:DZmds}: there are two main clusters; T and Y are the two most similar samples; whilst L and 8 are the two most dissimilar samples.\relax }}{213}{figure.17.3}\protected@file@percent }
\newlabel{fig:DZtree}{{17.3}{213}{Hierarchical clustering tree for the detrital zircon data. The tree conveys the same information as the MDS configuration of Figure~\ref {fig:DZmds}: there are two main clusters; T and Y are the two most similar samples; whilst L and 8 are the two most dissimilar samples.\relax }{figure.17.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.13}Supervised learning}{213}{section.17.13}\protected@file@percent }
\newlabel{sec:sol-supervised}{{17.13}{213}{Supervised learning}{section.17.13}{}}
\newlabel{it:sol-LDA-training}{{1}{213}{Supervised learning}{Item.376}{}}
\newlabel{it:sol-LDA-test}{{2}{214}{Supervised learning}{Item.377}{}}
\@writefile{toc}{\contentsline {section}{\numberline {17.14}Compositional data}{215}{section.17.14}\protected@file@percent }
\newlabel{sec:sol-compositional}{{17.14}{215}{Compositional data}{section.17.14}{}}
\newlabel{it:sol-mpg1}{{1a}{215}{Compositional data}{Item.381}{}}
\newlabel{it:sol-mpg2}{{1b}{215}{Compositional data}{Item.382}{}}
\newlabel{it:sol-mpg3}{{1c}{215}{Compositional data}{Item.383}{}}
\newlabel{it:sol-mpg4}{{1d}{216}{Compositional data}{Item.384}{}}
\newlabel{it:sol-mpg5}{{1e}{216}{Compositional data}{Item.385}{}}
\newlabel{it:sol-ternary-test}{{2}{216}{Compositional data}{Item.386}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.4}{\ignorespaces PCA biplot of the oceanic basalt compositions. The data roughly fall into three clusters, corresponding to OIB, MORB and IAB. OIBs are relatively rich in K\textsubscript  {2}O and poor in CaO; MORBs are rich in MgO and TiO\textsubscript  {2}; and IABs are rich in MnO and poor in TiO\textsubscript  {2}. K\textsubscript  {2}O and CaO are anti-correlated, and so are TiO\textsubscript  {2} and MnO. The variability in MnO/CaO is independent of the variability in TiO\textsubscript  {2}/MnO. The `outliers' on the ternary diagram of exercise\nobreakspace  {}\ref  {it:sol-ternary-test} are OIBs that are particularly poor in CaO.\relax }}{217}{figure.17.4}\protected@file@percent }
\newlabel{fig:testPCA}{{17.4}{217}{PCA biplot of the oceanic basalt compositions. The data roughly fall into three clusters, corresponding to OIB, MORB and IAB. OIBs are relatively rich in K\textsubscript {2}O and poor in CaO; MORBs are rich in MgO and TiO\textsubscript {2}; and IABs are rich in MnO and poor in TiO\textsubscript {2}. K\textsubscript {2}O and CaO are anti-correlated, and so are TiO\textsubscript {2} and MnO. The variability in MnO/CaO is independent of the variability in TiO\textsubscript {2}/MnO. The `outliers' on the ternary diagram of exercise~\ref {it:sol-ternary-test} are OIBs that are particularly poor in CaO.\relax }{figure.17.4}{}}
