\chapter{An introduction to \texttt{R}}
\label{ch:R}

{\tt R} is an increasingly popular programming language that is
available free of charge on any operating system at
\texttt{http://r-project.}\allowbreak\texttt{org}. A number of
different graphical user interfaces (GUIs) are available for
\texttt{R}, the most popular of which are \texttt{RGui},
\texttt{RStudio}, \texttt{RCommander} and \texttt{Tinn-R}.  For this
tutorial, however, the simple command line console suffices.

\section{The basics}
\label{sec:R-basics}

\begin{enumerate}
\item We will start this tutorial from the \texttt{R} \textbf{command
  prompt}, which is the window that begins with a \verb|>|
  symbol. First, do some arithmetic:

\begin{console}
> 1 + 1
[1] 2
\end{console}

\texttt{R} prints the result to the command prompt (\verb|2| in this
case). Here are some other arithmetic operations:

\begin{console}
> sqrt(2)
[1] 1.414214
> exp(log(10))
[1] 10
> 13%%5
[1] 3
\end{console}

\item An arrow operator is used to assign a value to a variable. Note
  that the arrow can point both ways:

\begin{console}
> foo <- 2
> 4 -> bar
> foo <- foo*bar
> foo
[1] 8
\end{console}

\item Create a vector of numbers:

\begin{console}
> myvec <- c(2,4,6,8)
> myvec*2
[1]  4  8 12 16
\end{console}

Query the third value of the vector:

\begin{console}
> myvec[3]
[1] 6
\end{console}

Change the third value of the vector:

\begin{console}
> myvec[3] <- 100
\end{console}

Change the second and the third value of the vector:

\begin{console}
> myvec[c(2,3)] <- c(100,101)
\end{console}

Create a sequence of numbers:

\begin{console}
> seq(from=1,to=10,by=1)
 [1]  1  2  3  4  5  6  7  8  9 10
\end{console}

Equivalently (output omitted for brevity):

\begin{console}
> seq(1,10,1)
> seq(1,10)
> seq(to=10,by=1,from=1)
> seq(to=10)
> 1:10
\end{console}

Create a 10-element vector of twos:

\begin{console}
> rep(2,10)
 [1] 2 2 2 2 2 2 2 2 2 2
\end{console}

\item Create a 2 $\times$ 4 matrix of ones:

\begin{console}
> mymat <- matrix(1,nrow=2,ncol=4)
\end{console}

Change the third value in the first column of \texttt{mymat} to 3:

\begin{console}
> mymat[1,3] <- 3
\end{console}

Change the entire second column of \texttt{mymat} to 2:

\begin{console}
> mymat[,2] <- 2
\end{console}

Remove the first column from \texttt{mymat}:

\begin{console}
> mymat[,-1]
     [,1] [,2] [,3]
[1,]    2    3    1
[2,]    2    1    1
\end{console}

Give names to the rows:

\begin{console}
> rownames(mymat) <- c('first','second')
\end{console}

Use the names:

\begin{console}
> mymat['first',]
[1] 1 2 3 1
\end{console}

The transpose of \texttt{mymat}:

\begin{console}
> t(mymat)
     first second
[1,]    1    1
[2,]    2    2
[3,]    3    1
[4,]    1    1
\end{console}

Element-wise multiplication (\verb|*|) vs. matrix multiplication
(\verb|%*%|):

\begin{console}
> mymat*mymat
       [,1] [,2] [,3] [,4]
first     1    4    9    1
second    1    4    1    1
> p <- mymat %*% t(mymat)
> p
       first second
first     15      9
second     9      7
\end{console}  

The inverse and determinant of a square matrix:

\begin{console}
> invp <- solve(p)
> det(invp %*% p)
[1] 1
\end{console}

\item Lists are used to store more complex data objects:

\begin{console}
> mylist <- list(v=myvec,m=mymat,nine=9)
> mylist$v
[1] 2 4 6 8
\end{console}

\noindent or, equivalently:

\begin{console}
> mylist[[1]]
> mylist[['v']]
\end{console}

Data frames are list-like tables:

\begin{console}
> myframe <- data.frame(period=c('Cz','Mz','Pz','PC'),
+                       SrSr=c(0.708,0.707,0.709,0.708),
+                       fossils=c(TRUE,TRUE,TRUE,FALSE))
> myframe
  period  SrSr fossils
1     Cz 0.708    TRUE
2     Mz 0.707    TRUE
3     Pz 0.709    TRUE
4     PC 0.708   FALSE
\end{console}

You can access the items in \texttt{myframe} either like a list or
like a matrix:

\begin{console}
> myframe$period == myframe[,'period']
[1] TRUE TRUE TRUE TRUE
\end{console}

\item Save data to a text (\texttt{.csv}) file:

\begin{console}
> write.csv(myframe,file='timescale.csv',row.names=FALSE)
\end{console}

Read data from a \texttt{.csv} file:

\begin{console}
> myframe2 <- read.csv(file='timescale.csv',header=TRUE)
\end{console}

Type \texttt{myframe2} at the command prompt to verify that the
contents of this new variable match those of \texttt{myframe}.
  
\item Plot the first against the second row of \texttt{mymat}:

\begin{console}
> plot(x=mymat[1,],y=mymat[2,])
\end{console}

Draw lines between the points shown on the existing plot:

\begin{console}
> lines(mymat[1,],mymat[2,])
\end{console}

Create a new plot with red lines but no points and a 1:1 aspect ratio
for the X- and Y-axis:

\begin{console}
> plot(mymat[1,],mymat[2,],type='l',col='red',asp=1)
\end{console}

Save the currently active plot as a vector-editable \texttt{.pdf}
file:

\begin{console}
> dev.copy2pdf(file="trigonometry.pdf")
\end{console}

\item If you want to learn more about a function, type `\texttt{help}' or
`\texttt{?}':

\begin{console}
> help(c)
> ?plot
\end{console}

\item You can also define your own functions:

\begin{console}
> cube <- function(n){
+     return(n^3)
+ }
\end{console}

Using the newly created function:

\begin{console}
> cube(2)
[1] 8
> result <- cube(3)
\end{console}

\item Create some random (uniform) numbers:

\begin{console}
> rand.num <- runif(100)
> hist(rand.num)
\end{console}

\item Collect the following commands in a file called
  `\texttt{myscript.R}'.  Note that the following text box does not
  contain any `\verb|>|'-symbols because it is not entered at the
  command prompt but in a separate text editor:

\begin{script}
# the 'print' function is needed to show intermediate
# results when running commands from a .R file
print(pi)
\end{script}

You can run this code by going back to the command prompt (hence the
`\verb|>|' in the next box) and typing:

\begin{console}
> source("myscript.R")
[1] 3.141593
\end{console}

Note that everything that follows the `\verb|#|'-symbol was ignored by
\texttt{R}.

\item Conditional statements. Replace the contents of
  \texttt{myscript.R} with:

\begin{script}[firstnumber=3]
toss <- function(){
    if (runif(1)>0.5){
        print("head")
    } else {
        print("tail")
    }
}
\end{script}

Save and run at the command prompt:

\begin{console}
> source('myscript.R')
> toss()
[1] "head"
\end{console}

(you might, of course, get \texttt{"tail"} when you run this)

\item Loops. Add the following function to \texttt{myscript.R}:

\begin{script}[firstnumber=10]
fibonnaci <- function(n=5){ # 5 is the default value
    if (n < 3) { stop('n must be at least 3') }
    # seed the output vector with 0 and 1:
    s <- c(0,1)
    # loop through all numbers from 3 to n:
    for (i in 3:n){
        s[i] <- s[i-1] + s[i-2]
    }
    return(s)
}
\end{script}

Save and run at the command prompt to calculate the first \texttt{n}
numbers in the Fibonnaci series:

\begin{console}
> source('myscript.R')
> fibonnaci()
 [1] 0 1 1 2 3
> fibonnaci(10)
 [1]  0  1  1  2  3  5  8 13 21 34
\end{console}

\item\label{it:geostats} Arguably the greatest power of \texttt{R} is
  the availability of thousands of \textit{packages} that provide
  additional functionality. One of these packages is called
  \texttt{geostats} and was specifically created to accompany these
  notes.  To install this package:

\begin{console}
> install.packages('geostats')
\end{console}

Once installed, the package can be loaded into memory by entering:

\begin{script}
library(geostats)
\end{script}

Let's use \texttt{geostats} to reproduce
Figure~\ref{fig:declusteredquakes}:

\begin{script}[firstnumber=2]
data(declustered,package='geostats')
quakesperyear <- countQuakes(declustered,minmag=5.0,from=1907,to=2006)
barplot(quakesperyear)
\end{script}

Type \texttt{?declustered} and \texttt{?countQuakes} for further
details. To view the source code of the \texttt{countQuakes} function,
just type \texttt{countQuakes} at the command prompt:

\begin{console}
> countQuakes
function(qdat,minmag,from,to){
    bigenough <- (declustered$mag >= minmag)
    youngenough <- (declustered$year >= from)
    oldenough <- (declustered$year <= to)
    goodenough <- (bigenough & youngenough & oldenough)
    table(qdat$year[goodenough])
}
\end{console}

\noindent where \texttt{table} produces a table with the counts of
each value.

\end{enumerate}

\section{Plotting data}
\label{sec:R-plotting}

In the remainder of this text, we will assume that the
\texttt{geostats} package has been loaded into memory:

\begin{console}
library(geostats)
\end{console}

\begin{enumerate}

\item\label{it:anscombe} The Anscombe quartet of
  Table~\ref{tab:anscombe} and Figure~\ref{fig:anscombe} is built into
  \texttt{R}. You can have a look at it by typing \texttt{anscombe} at
  the command prompt. We can then create Figure~\ref{fig:anscombe}:

\begin{script}
par(mfrow=c(1,4))
plot(anscombe$x1,anscombe$y1)
plot(anscombe$x2,anscombe$y2)
plot(anscombe$x3,anscombe$y3)
plot(anscombe$x4,anscombe$y4)
\end{script}

\noindent where \texttt{par(mfrow=c(1,4))} creates a $1\times{4}$ grid
of plot panels. Note that we can also write this more generically:

\begin{script}
np <- 4 # np = 'number of panels'
p <- par(mfrow=c(1,np))
for (i in 1:np){
  plot(anscombe[,i],anscombe[,i+np])
}
\end{script}

Or, adding a few options to make the plot look exactly like
Figure~\ref{fig:anscombe}:

\begin{script}[firstnumber=3]
titles <- c('I','II','III','IV')
for (i in 1:np){
  plot(anscombe[,i],anscombe[,i+np],xlab='x',ylab='y',pch=19,main=titles[i])
}
\end{script}

\item Creating a histogram of clast counts (Figure~\ref{fig:clasts}):

\begin{script}
clasts <- c(10,5,6,20)
names(clasts) <- c('granite','basalt','gneiss','quartzite')
barplot(clasts,col='white')
\end{script}

The \texttt{geostats} package includes a number of datasets, such as
the pH data of Section~\ref{sec:continuous}. Loading this dataset with
\texttt{R}'s \texttt{data($\ldots$)} function and plotting it on a
histogram and rug plot:

\begin{script}
data(pH,package='geostats')
hist(pH)
rug(pH)
\end{script}

Changing the number of bins:

\begin{script}[firstnumber=2]
par(mfrow=c(1,2))
hist(pH,breaks=5)
hist(pH,breaks=10)
\end{script}

Specifying the position of the bins:

\begin{script}[firstnumber=3]
hist(pH,breaks=seq(from=3,to=7,by=0.5))
hist(pH,breaks=seq(from=3.25,to=6.75,by=0.5))
\end{script}

\item\label{it:KDE} A kernel density estimate (KDE) and rug plot of
  the pH data (Section~\ref{sec:continuous} and
  Figure~\ref{fig:pHgaussKDE}):

\begin{script}
dens <- density(pH)
plot(dens)
rug(pH)    
\end{script}

A KDE of the log-transformed clast size data
(Figure~\ref{fig:logKDE}):

\begin{script}
data(clasts,package='geostats')
lc <- log(clasts)
d <- density(lc)
plot(d)
\end{script}

Subjecting the porosity data to a logistic transformation before
plotting as a KDE:

\begin{script}
data(porosity,package='geostats')
lp <- logit(porosity)
d <- density(lp)
plot(d)
\end{script}

\noindent where the \texttt{logit($\ldots$)} function is provided by
the \texttt{geostats} package. To map the density estimate from the
logistic scale ($-\infty,+\infty$) back to the normal porosity scale
($0,1$):

\begin{script}[firstnumber=2]
lp <- logit(porosity,inverse=FALSE)
ld <- density(lp)
d <- logit(ld,inverse=TRUE)
plot(d)
\end{script}

Note that we are using the \texttt{logit($\ldots$)} twice using
different inputs. In programming jargon, the function has been
\textbf{overloaded}. To inspect the \texttt{R}-code of the two
implementations, just type \texttt{logit.default} and
\texttt{logit.density} at the command prompt.

\item The Old Faithful geyser data of section~\ref{sec:multivariate}
  are included with \texttt{R}. Plotting the eruption durations and
  waiting times proceeds in exactly the same way as point~\ref{it:KDE}
  above:

\begin{script}
x <- faithful[,'waiting']
y <- faithful[,'eruptions']
par(mfrow=c(2,1))
plot(density(x),xlab='minutes',main='waiting time')
rug(x)
plot(density(y),xlab='minutes',main='eruption duration')
rug(y)
\end{script}

\noindent where we have \textbf{nested} the \texttt{density} and
\texttt{plot} functions for the sake of brevity. Two-dimensional KDEs
are not part of base \texttt{R}. To access this functionality, we must
first load the important \texttt{MASS} (`Mathematical and Applied
Statistics with S\footnote{\texttt{S} is the name of the programming
  language. \texttt{R} is a free implementation of \texttt{S}, and
  \texttt{S-PLUS} is a commercial alternative.}') package.

\begin{script}[firstnumber=3]
library(MASS)
kde2 <- kde2d(x,y)
contour(kde2)
points(x,y)
\end{script}

\item Calculate the empirical cumulative distribution function of the
  pH data:

\begin{script}
data(pH,package='geostats')
cdf <- ecdf(pH)
plot(cdf)
\end{script}

Adding some optional arguments to produce an output that is more
similar to Figure~\ref{fig:ECDFs}a:

\begin{script}[firstnumber=3]
plot(cdf,verticals=TRUE,pch=NA)
\end{script}

\noindent where \texttt{pch=NA} removes the plot characters, and
\texttt{verticals=TRUE} is self explanatory.

The \texttt{ecdf} function produces another function that can be
evaluated at any value. For example, if we want to evaluate the
fraction of pH values that are less than 4.5:

\begin{console}
> cdf(4.5)
[1] 0.25
\end{console}

\noindent which means that there are 25\% such values.

\end{enumerate}

\section{Summary Statistics}
\label{sec:R-summary-statistics}

\begin{enumerate}

\item Calculating summary statistics in \texttt{R} is straightforward:

\begin{console}
> data(pH,package='geostats')
> mean(pH)
[1] 4.985
> sd(pH) # standard deviation
[1] 0.6698586
> median(pH)
[1] 5.1
> mad(pH) # median absolute deviation
[1] 0.7413
> IQR(pH) # interquartile range
[1] 0.95
\end{console}

\item\label{it:mode} \texttt{R} does not come with a function to
  calculate the mode\footnote{There does exist a \texttt{mode}
    function but it does something different.\label{fn:mode}} so we
  have to write our own.  For categorical data, the mode is the most
  frequent occurring value. Using the declustered earthquake counts of
  Figure~\ref{fig:quakecounts} as an example:

\begin{script}
data(declustered,package='geostats')
quakesperyear <- countQuakes(declustered,minmag=5.0,from=1917,to=2016)
quaketab <- table(quakesperyear)
mod <- which.max(quaketab)
\end{script}

\noindent where \texttt{which.max} returns the index or name of the
maximum value. A more sophisticated implementation is included in a
\texttt{geostats} function called \texttt{Mode}\footnote{Note the
  uppercase `\texttt{M}' in \texttt{Mode}, which aims to avoid the
  conflict with the \texttt{mode} function mentioned in
  footnote~\ref{fn:mode}. \texttt{R} is case sensitive.}.

For continuous variables, in which there are no duplicate values, we
use a KDE to determine the mode:

\begin{script}[firstnumber=4]
data(clasts,package='geostats')
dens <- density(clasts)
mod <- dens$x[which.max(dens$y)]
\end{script}

The skewness is not implemented in \texttt{R} either. But it easy to
write a function for that as well, based on Equation~\ref{eq:skew}:

\begin{script}[firstnumber=7]
skew <- function(x){
  mean((x-mean(x))^3)/(length(x)*sd(x)^3)
}
\end{script}

\item A box plot for the clast size data:

\begin{script}[firstnumber=10]
boxplot(clasts)
\end{script}

\end{enumerate}

\section{Probability}
\label{sec:R-probability}

\begin{enumerate}

\item The factorial operator (!) in Chapter~\ref{ch:probability} is
  implemented as \texttt{factorial(x)}. For example:

\begin{console}
> factorial(1)
[1] 1
> factorial(10)
[1] 3628800
> factorial(100)
[1] 9.332622e+157
> factorial(1000)
[1] Inf
\end{console}

\texttt{factorial(x)} fails to calculate !1000. For large numbers like
this, it is better to use \texttt{lfactorial(x)}, which returns
the natural logarithm of the factorial:

\begin{console}
> lfactorial(1000)
[1] 1792.332
\end{console}

\noindent which means that $1000!=e^{1792.332}$. 

\item Similarly, the combinations $\binom{n}{k}$ of small numbers can
  be calculated with \texttt{nchoosek(n,k)}:

\begin{console}
> choose(n=10,k=2)
[1] 45
> choose(n=10000,k=2000)
[1] Inf
\end{console}

\noindent and for large numbers with \texttt{lchoosek(n,k)}:

\begin{console}
> lchoose(n=10000,k=2000)
[1] 4999.416
\end{console}

\noindent which means that there are $e^{4999.416}$ ways to choose
2,000 items from a collection of 10,000.

\end{enumerate}

\section{The binomial distribution}
\label{sec:R-binomial}

\begin{enumerate}

\item Flip 10 coins and count the number of heads:

\begin{console}
> rbinom(n=1,size=10,prob=0.5)
\end{console}

Repeat 50 times and plot the outcomes as a histogram:

\begin{script}
d <- rbinom(n=50,size=10,prob=0.5)
hist(d)
\end{script}

\item Calculate the probability of observing 4 heads out of 10 throws:

\begin{console}
> dbinom(x=4,size=10,prob=0.5)
[1] 0.2050781
\end{console}

Plot the probability mass function (PMF) of the binomial distribution
(Equation~\ref{eq:binom}) with $n=10$ and $p=0.5$:

\begin{script}
x <- 0:10
pmf <- dbinom(x=0:10,size=10,prob=0.5)
barplot(height=pmf,names.arg=x)
\end{script}
  
\noindent where \texttt{names.arg} specifies the labels of the bar
plot.

\item The probability of observing 4 or fewer heads out of 10 throws:

\begin{console}
> pbinom(q=4,size=10,prob=0.5)
[1] 0.3769531
\end{console}

Plot the cumulative distribution function (CDF) of the binomial
distribution (Equation~\ref{eq:CDF}) with $n=10$ and $p=0.5$:

\begin{script}
cdf <- pbinom(q=0:10,size=10,prob=0.5)
plot(cdf,type='s')
\end{script}

\item\label{it:1sidedbinomR} Calculate the quantiles of the binomial
  distribution. For example, assume that there is a $p=2/3$ chance of
  finding gold in claim, and suppose that there are $n=15$
  claims. Then there is a 95\% chance that the number of successful
  claims is less than

\begin{console}
> qbinom(p=0.95,size=15,prob=2/3)
[1] 13
\end{console}

\noindent where the argument \texttt{p} must not be confused with the
parameter $p$ in Equation~\ref{eq:binom}. The latter parameter is
referred to as \texttt{prob} in \texttt{R}'s binomial functions.

Conversely, if $p=2/3$, then there is a 95\% chance that the number of
successful gold discoveries among 15 claims is \emph{greater} than
  
\begin{console}
> qbinom(p=0.05,size=15,prob=2/3)
[1] 7
\end{console}

Thus the rejection region for the one-sided null hypothesis $H_\circ:
p=2/3$ vs. the alternative hypothesis $H_a: p>2/3$ is $R =
\{0,\ldots,6\}$ (Equation~\ref{eq:1sidedbinomtest15}).

\item\label{it:2sidedbinomR} The boundaries of the rejection region
  for the two-sided null hypothesis $H_\circ: p=2/3$ vs. the
  alternative hypothesis $H_a: p\neq{2/3}$ are given by

\begin{console}
> qbinom(p=c(0.025,0.975),size=30,prob=2/3)
[1] 15 25
\end{console}

Hence $R = \{0,\ldots,14,26,\ldots,30\}$
(Equation~\ref{eq:2sidedbinomtest15}).

\item Based on the 1-sided rejection region calculated under
  point~\ref{it:1sidedbinomR}, a success rate of 6 gold discoveries
  out of 15 claims is incompatible with the null hypothesis $H_\circ:
  p=2/3$ vs.  the one-sided alternative hypothesis $H_a: p<2/3$. This
  is because $6 \in R=\{0,\ldots,6\}$.  Here is another way to obtain
  the same result:

\begin{console}
> binom.test(x=6,n=15,p=2/3,alternative='less',conf.level = 0.95)

	Exact binomial test

data:  6 and 15
number of successes = 6, number of trials = 15, p-value = 0.03083
alternative hypothesis: true probability of success is less than 0.6666667
95 percent confidence interval:
 0.0000000 0.6404348
sample estimates:
probability of success 
                   0.4   
\end{console}

This result shows a p-value of $0.03083<0.05$, leading to the
rejection of $H_\circ$. The 95\% confidence interval for $p$ spans the
range from 0 to 0.6404348, which does not include the hypothesised
value of $p=2/3$.

\item Based on the 2-sided rejection region calculated under
  point~\ref{it:2sidedbinomR}, a success rate of 6 gold discoveries
  out of 15 claims is compatible with the null hypothesis $H_\circ:
  p=2/3$ vs.  the one-sided alternative hypothesis $H_a:
  p\neq{2/3}$. This is because $6 \notin
  R=\{0,\ldots,5,26,\ldots,30\}$. Hence we cannot reject $H_\circ$ in
  this case.  Here is another way to obtain the same result:

\begin{console}
> h <- binom.test(x=6,n=15,p=2/3,alternative='two.sided',conf.level = 0.95)
> h$p.value
[1] 0.05023902
> h$conf.int
[1] 0.1633643 0.6771302
attr(,"conf.level")
[1] 0.95
\end{console}

\noindent where we have stored the output of
\texttt{binom.test($\ldots$)} in a variable \texttt{h}. $H_\circ$
cannot be rejected because the p-value is $0.05024>0.05$, and the
confidence interval ranges from 0.1633643 to 0.6771302, which includes
$p=2/3$.

\end{enumerate}

\section{The Poisson distribution}
\label{sec:R-poisson}

\begin{enumerate}
\item Create a histogram of 100 random values from a Poisson
  distribution with parameter $\lambda=3.5$:

\begin{script}
d <- rpois(n=100,lambda=3.5)
hist(d)
\end{script}

\item Calculate the probability of observing 0 successes if $\lambda=3.5$:

\begin{console}
> dpois(x=0,lambda=3.5)
[1] 0.03019738
\end{console}

Plot the probability mass function (PMF, evaluated up to 15 successes)
of the Poisson distribution (Equation~\ref{eq:poispmf}) for
$\lambda=3.5$.

\begin{script}
x <- 0:15
pmf <- dpois(x=x,lambda=3.5)
barplot(height=pmf,names.arg=x)
\end{script}

\item The probability of observing 9 or fewer successes if $\lambda=3.5$:

\begin{console}
> ppois(q=9, lambda=3.5)
[1] 0.9966851
\end{console}

Plot the CDF of the Poisson distribution with $\lambda=3.5$:

\begin{script}
cdf <- ppois(q=x,lambda=3.5)
plot(cdf,type='s')
\end{script}

\item\label{it:1sidedpoisR} The rejection region ($\alpha=0.05$) for a
  one-sided hypothesis $H_\circ: \lambda=3.5$ vs.  $H_a: \lambda>3.5$
  (Section~\ref{sec:poishyp}):

\begin{console}
> qpois(p=0.95,lambda=3.5)
[1] 7
\end{console}

Hence $R=\{8,\ldots,\infty\}$ (see
Section~\ref{sec:poishyp}.\ref{it:poisl351sided}).

\item\label{it:2sidedpoisR} The two-sided rejection region:

\begin{console}
> qpois(p=c(0.025,0.975),lambda=3.5)
[1] 0 8
\end{console}

Hence $R=\{9,\ldots,\infty\}$.

\item Based on the 1-sided rejection region calculated under
  point~\ref{it:1sidedpoisR}, a success rate of 9 zircons per grid is
  incompatible with the null hypothesis $H_\circ: \lambda=3.5$ vs.
  the one-sided alternative hypothesis $H_a: \lambda>3.5$. This is
  because $9 \in R=\{9,\ldots,\infty\}$.  Here is another way to
  obtain the same result:

\begin{console}
> h <- poisson.test(x=9,r=3.5,alternative='greater',conf.level=0.95)
> h$p.value
[1] 0.009873658
\end{console}

The p-value is $0.009873658<0.05$, leading to a rejection of
$H_\circ$. The 95\% confidence interval is given by:

\begin{console}
> h$conf.int
[1] 4.695228      Inf
\end{console}

$(\lambda = 3.5) \notin[4.695228, \infty)$. Hence, $H_\circ$ is
  rejected.

\item Based on the 2-sided rejection region calculated under
  point~\ref{it:2sidedbinomR}, a success rate of 9 is incompatible
  with the null hypothesis $H_\circ: \lambda=3.5$ vs.  the one-sided
  alternative hypothesis $H_a: \lambda\neq{3.5}$. This is because $9
  \in R=\{9,\ldots,\infty\}$. This again leads to rejection of
  $H_\circ$ in favour of $H_a$. We can obtain the same result with
  \texttt{poisson.test}:

\begin{console}
> poisson.test(x=9,r=3.5,alternative='two.sided',conf.level=0.95)

	Exact Poisson test

data:  9 time base: 1
number of events = 9, time base = 1, p-value = 0.009874
alternative hypothesis: true event rate is not equal to 3.5
95 percent confidence interval:
  4.115373 17.084803
sample estimates:
event rate 
         9 
\end{console}

\end{enumerate}

\section{The normal distribution}
\label{sec:R-gauss}

\begin{enumerate}
  \item Generate 100 random numbers from a normal distribution with
    mean $\mu=50$ and standard deviation $\sigma=5$, and plot as a
    histogram:

\begin{script}
d <- rnorm(n=100,mean=50,sd=5)
hist(d)  
\end{script}

\item Generate 200 random pairs of numbers ($\{x_i,y_i\}$ for
  $1\leq{i}\leq{200}$) from a bivariate normal distribution with mean
  $\{\mu_x=10,\mu_y=20\}$ and covariance matrix

  \[
  \sigma_{x,y} = \left[
    \begin{array}{cc}
      \sigma_x^2 = 2 & \sigma_{x,y} = -3 \\
      \sigma_{x,y} = -3 & \sigma_y^2 = 6
    \end{array}
    \right]
  \]

\begin{script}
library(MASS)
m <- c(10,20)
s <- matrix(data=c(2,-3,-3,6),nrow=2,ncol=2)
xy <- mvrnorm(n=200,mu=m,Sigma=s)
plot(xy)
\end{script}

\item Plot the PDF and CDF of a normal distribution with mean $\mu=50$
  and standard deviation $\sigma=5$:

\begin{script}
par(mfrow=c(1,2))
m <- 50
s <- 5
x <- seq(from=25,to=75,length.out=100)
f <- dnorm(x=x,mean=m,sd=s)
plot(x=x,y=f,type='l',main='PDF')
P <- pnorm(q=x,mean=m,sd=s) 
plot(x=x,y=P,type='l',ylab='P(X<x)',main='CDF')
\end{script}

\end{enumerate}

\section{Error propagation}
\label{sec:R-errorprop}

Propagating analytical uncertainties using the procedures of
Section~\ref{sec:errorpropexamples} is a manual process that does not
require \texttt{R}. However, \texttt{R} does fulfil a useful purpose
for the Fisher Information approach of
Section~\ref{sec:FisherInformation}. In
Section~\ref{sec:FisherInformation}, we manually showed that
$s[\hat{\lambda}] = \hat{\lambda}$. This Section will show how
\texttt{R} can do the same thing numerically.

\begin{enumerate}
\item Recall the log-likelihood function for the Poisson distribution
  (Equation~\ref{eq:poisLL}):

\[
  \mathcal{LL}(\lambda|k) = k \ln[\lambda] - \lambda - \sum\limits_{i=1}^{k}i
\]

Implementing this in \texttt{R}:

\begin{script}
LL <- function(lambda,k){
  k * log(lambda) - lambda - sum(1:k)
}
\end{script}

\item Evaluating \texttt{LL} for different values of \texttt{lambda}
  assuming that \texttt{k=4}.

\begin{script}[firstnumber=4]
N <- 100
lam <- seq(from=0,to=20,length.out=N)
loglik <- rep(0,N)
for (i in 1:N){
  loglik[i] <- LL(lambda=lam[i],k=4)
}
plot(lam,loglik,type='l',xlab=expression(lambda),ylab='LL')
\end{script}

\noindent which produces the following output:

\noindent\begin{minipage}[t][][b]{.3\textwidth}
  \includegraphics[width=\textwidth]{../figures/LLpois.pdf}\\
\end{minipage}
\begin{minipage}[t][][t]{.7\textwidth-27pt}
  \captionof{figure}{Log-likelihood function for the Poisson
    distribution, evaluated at different values for the parameter
    $\lambda$, given an observation of $k=4$ successes. The function
    reaches a maximum value at $\hat{\lambda}=4$.  }
  \label{fig:LLpois}
\end{minipage}

\item We can find the maximum likelihood estimate for $\lambda$ using
  \texttt{R}'s general purpose \texttt{optim} function\footnote{An
    alternative (and easier) function is \texttt{optim} but this does
    not compute the Hessian matrix.}, using \texttt{par=1} as an
  initial guess for $\lambda$:

\begin{script}[firstnumber=11]
o <-  optim(par=1,f=LL,k=4,control=list(fnscale=-1),method='BFGS')
\end{script}

The control parameter \texttt{fnscale=-1} ensures that \texttt{optim}
finds the \textit{maximum} value of \texttt{LL} rather than its
minimum (which is the default).  And \texttt{BFGS} is one of several
minimisation algorithms that is well suited for one-dimensional
optimisation problems. See \texttt{?optim} for further details.  The
maximum likelihood estimate is then obtained by:

\begin{console}
> o$par
[1] 4.000001
\end{console}

\noindent which yields a value of $\hat{\lambda}=4$, ignoring a small
numerical error.

\item To estimate the uncertainty of $\hat{\lambda}$ using the Fisher
  Information approach of Equation~\ref{eq:Fisher} requires just a
  small change to our code:

\begin{script}[firstnumber=11]
o <-  optim(par=1,f=LL,k=4,control=list(fnscale=-1),
            method='BFGS',hessian=TRUE)
\end{script}

Equation~\ref{eq:Fisher} then becomes:

\begin{console}
> -1/o$hessian
         [,1]
[1,] 4.000001
\end{console}

So $s[\hat{\lambda}]^2=4$, which is the same result as we derived by
hand in Equation~\ref{eq:poisvar}.

\end{enumerate}

\section{Comparing distributions}
\label{sec:R-comparingdistributions}

\begin{enumerate}

\item Create a Q-Q plot comparing the Old Faithful eruption durations
  with the eruption waiting times:

\begin{script}
qqplot(faithful[,'eruptions'],faithful[,'waiting'])
\end{script}

Create a Q-Q plot comparing the eruption durations with a normal
distribution (Figure~\ref{fig:qqfaithful1}):

\begin{script}
dat <- faithful[,'eruptions']
qqnorm(dat)
qqline(dat)
\end{script}

\item Perform the one-sided, one-sample t-test of Section~\ref{sec:t}
  (page~\pageref{eq:t}):

\begin{script}
gold1 <- c(19.07,19.09,19.17,19.18,19.31)
h <- t.test(gold1,mu=19.30,alternative='less')
\end{script}

The p-value of this test is

\begin{console}
> h$p.value
[1] 0.01630814
\end{console}

\noindent which is less than 0.05, leading to a rejection of $H_\circ: \mu=19.30$.
Equivalently, the 95\% confidence interval is

\begin{console}
> h$conf.int
[1]     -Inf 19.25435
\end{console}

$\mu=19.30\notin(-\infty,19.25435]$, which again leads to a rejected
null hypothesis.

\item\label{it:2sided2samplettest} Comparing two sets of coins with a
  two-sided, two-sample test:

\begin{script}[firstnumber=2]
gold2 <- c(19.17,19.30,19.31,19.32)
h <- t.test(gold1,gold2)
\end{script}

Inspecting the p-value and two-sided 95\% confidence interval:

\begin{console}
> h$p.value
[1] 0.0839384
> h$conf.int
[1] -0.24136871  0.01936871
\end{console}

We cannot reject $H_\circ: \mu_1=\mu_2$ because $0.0839384>0.05$, and
because $\{-0.24136871\leq{0}\leq{0.01936871}\}$.

\item To carry out the $\chi^2$-test of Section~\ref{sec:chi2}, we
  first calculate the declustered earthquake frequencies.

\begin{script}
data(declustered,package='geostats')
quakesperyear <- countQuakes(declustered,minmag=5.0,from=1917,to=2016)
quaketab <- table(quakesperyear)
\end{script}

Printing the table of earthquake counts per year at the console

\begin{console}
> table(quakesperyear)
quakesperyear
 1  2  3  4  5  6  7  8  9 10 11 12 
 3  8 13 17 13 14 13  5  8  3  1  2 
\end{console}

\noindent shows that there are four bins with fewer than 4 items. In
order for the $\chi^2$-approximation to be valid, we must merge some
of these categories:

\begin{script}[firstnumber=3]
obs <- c( sum(quaketab[1:2]), quaketab[3:9], sum(quaketab[10:12]) )
\end{script}

The corresponding predicted counts are obtained using a Poisson
distribution with parameter given by the mean of the earthquake
counts:

\begin{script}[firstnumber=4]
lam <- mean(quakesperyear)
pred <- c(sum(dpois(x=0:2,lambda=lam)),
          dpois(x=3:9,lambda=lam),
          sum(dpois(x=10:25,lambda=lam))
         )
\end{script}

Then the $\chi^2$-test proceeds as follows:

\begin{script}[firstnumber=9]
h <- chisq.test(x=obs,p=pred/sum(pred))
\end{script}

\noindent where \texttt{pred/sum(pred)} normalises the predicted
probabilities to 1. Querying the p-value:

\begin{console}
> h$p.value
[1] 0.7429772
\end{console}

$0.7429772>0.05$, hence $H_\circ$ is not rejected.

\item Comparing the two collections of gold coins from
  item~\ref{it:2sided2samplettest} above but this time using a
  Wilcoxon test instead of a t-test:

\begin{script}
gold1 <- c(19.07,19.09,19.17,19.18,19.31)
gold2 <- c(19.17,19.30,19.31,19.32)
h <- wilcox.test(x=gold1,y=gold2)
\end{script}

Querying the p-value:

\begin{console}
> h$p.value
[1] 0.174277
\end{console}

We cannot reject $H_\circ$ because $0.174277>0.05$.

\item Load two detrital zircon U--Pb age distributions from the Mu Us
  desert and Yellow River into memory, and compare them with the
  Kolmogorov-Smirnov test:

\begin{script}
data(DZ,package='geostats')
river <- DZ[['Y']]
dune <- DZ[['5']]
h <- ks.test(x=river,y=dune)
\end{script}

Querying the result:

\begin{console}
> h$p.value
[1] 4.500822e-05
\end{console}

$4.500822\times{10}^{-5}\ll{0.05}$ and hence $H_\circ$ is clearly rejected.

\end{enumerate}

\section{Regression}
\label{sec:R-regression}

\begin{enumerate}

\item Load the Rb--Sr data from the \texttt{geostats} package and plot
  the columns \texttt{RbSr} and \texttt{SrSr} as a scatter plot:

\begin{script}
data(rbsr,package='geostats')
plot(x=rbsr[,'RbSr'],y=rbsr[,'SrSr'])
\end{script}

Calculate the correlation coefficient of \texttt{RbSr} and
\texttt{SrSr}:

\begin{script}[firstnumber=3]
cormat <- cor(rbsr[,c('RbSr','SrSr')])
r <- cormat[1,2]
\end{script}

\noindent which yields:

\begin{console}
> r
[1] 0.9847415
> r^2
[1] 0.9697158
\end{console}

\item Fit a line to the data using the method of least squares:

\begin{script}[firstnumber=4]
fit <- lm(SrSr ~ RbSr, data=rbsr)
\end{script}

\noindent which uses \texttt{R}'s \textbf{formula notation}
(\verb|Y ~ X| where \texttt{X} is the independent variable and
\texttt{Y} is the dependent variable).

Query the slope and intercept:

\begin{console}
> fit$coef
(Intercept)        RbSr 
 0.69742660  0.01391808
\end{console}

So the best fit line is given by
[\textsuperscript{87}Sr/\textsuperscript{86}Sr] = 0.696 + 0.014
[\textsuperscript{87}Rb/\textsuperscript{86}Sr] (rounded to two
significant digits). Add the best fit line to the existing
scatter plot:

\begin{script}[firstnumber=5]
x <- range(rbsr[,'RbSr'])
y <- fit$coef[1] + fit$coef[2]*x
lines(x,y)
\end{script}

Or, equivalently:

\begin{script}[firstnumber=6]
y <- predict(fit, newdata=data.frame(RbSr=x))
lines(x,y)
\end{script}

Or, shorter:

\begin{script}[firstnumber=5]
abline(fit)
\end{script}

\item Test the statistical significance of the correlation coefficient
  using Equation~\ref{eq:tr}:

\begin{script}[firstnumber=6]
n <- nrow(rbsr)
tstat <- r*sqrt(n-2)/sqrt(1-r^2)
p <- pt(q=tstat,df=n-2)
\end{script}

\noindent which gives:

\begin{console}
> p
[1] 0.9999956
\end{console}

Hence the p-value for $H_\circ: \beta_0=0$ vs. $H_a: \beta_0\neq{0}$
is:

\begin{console}
> 2*(1-p)
[1] 8.779983e-06
\end{console}

Thus $H_\circ$ is rejected and the linear trend is real. An easier way
to obtain the same result is to apply the \texttt{summary($\ldots$)}
function to the output of the \texttt{lm($\ldots$)}:

\begin{console}
> summary(fit)

Call:
lm(formula = SrSr ~ RbSr, data = rbsr)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.007887 -0.004425 -0.002511  0.006451  0.009178 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.697427   0.006273  111.18 3.57e-11 ***
RbSr        0.013918   0.001004   13.86 8.78e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.007092 on 6 degrees of freedom
Multiple R-squared:  0.9697,	Adjusted R-squared:  0.9647 
F-statistic: 192.1 on 1 and 6 DF,  p-value: 8.78e-06
\end{console}

\noindent in which we can recognise both the p-value and $r^2$, as
well as lots of other information about the fit.

\item Construct a 95\% confidence envelope for the Rb--Sr data:

\begin{script}[firstnumber=9]
x <- seq(from=min(rbsr[,'RbSr']),to=max(rbsr[,'RbSr']),length.out=20)
pred <- predict(fit,newdata=data.frame(RbSr=x),
                interval="confidence",level=0.95)
matlines(x,pred,lty=1,col='black')
\end{script}

\noindent where \texttt{matlines} simultaneously plots multiple lines.
Adding a prediction interval to the existing plot:

\begin{script}[firstnumber=13]
pred <- predict(fit,newdata=data.frame(RbSr=x),
                interval="prediction",level=0.95)
matlines(x,pred,lty=2,col='black')
\end{script}

\item The \texttt{rbsr} file contains five columns, specifying the
  ratios as well as their uncertainties and error correlations:

\begin{console}
> rbsr
  RbSr errRbSr  SrSr errSrSr   rho
1 2.90  0.0944 0.745 0.00702 0.586
2 7.14  0.0970 0.803 0.00625 0.470
3 9.10  0.1040 0.823 0.00740 0.476
4 3.41  0.1040 0.737 0.00697 0.468
5 1.91  0.0967 0.720 0.00676 0.486
6 7.15  0.1110 0.793 0.00749 0.507
7 5.92  0.0948 0.789 0.00632 0.561
8 8.28  0.1070 0.807 0.00678 0.460
\end{console}

Plotting these data as error ellipses and implementing the
weighted least squares regression algorithm of
Section~\ref{sec:weightedregression} is not trivial in base
\texttt{R}. Fortunately, the \texttt{geostats} package comes to the
rescue:

\begin{script}[firstnumber=2]
y <- york(x=rbsr)
\end{script}

The name of the weighted least squares regression refers to
geophysicists Derek York, who developed an early version of the
algorithm\footnote{York, D., 1968. Least squares fitting of a straight
  line with correlated errors. \emph{Earth and Planetary Science
    Letters}, 5, pp.320-324.}.

\end{enumerate}

\section{Fractals and chaos}
\label{sec:R-fractals}

\begin{enumerate}
  
\item\label{it:finland} Calculate and plot the size-frequency
  distribution of Finnish lakes (Figure~\ref{fig:Finlandpowerlaw}):

\begin{script}
data(Finland,package='geostats')
sf <- sizefrequency(Finland$area)
plot(frequency~size,data=sf,log='xy')
fit <- lm(log(frequency)~log(size),data=sf)
lines(x=sf$size,y=exp(predict(fit)))
\end{script}

\noindent where the \texttt{sizefrequency} function is provided by
\texttt{geostats}.\\

\item Create a Gutenberg-Richter plot for the recent earthquake data
  (Figure~\ref{fig:gutenberg}):

\begin{script}
data(earthquakes,package='geostats')
gutenberg(earthquakes$mag)
\end{script}

\noindent where \texttt{gutenberg} is a \texttt{geostats} function
that is similar to the Finnish lake code of step~\ref{it:finland}. You
can check out the implementation details by typing
`\texttt{gutenberg}' at the command prompt.

\item Create a \textit{Koch snowflake}, i.e. a triangle of three Koch
  curves (Figures~\ref{fig:koch1}-\ref{fig:koch6}):

\begin{script}
k <- koch(n=5)
\end{script}

Compute the fractal dimension of the curve:

\begin{script}[firstnumber=2]
fit <- fractaldim(k)
\end{script}

\noindent which yield a slope of -1.2 and, hence, a fractal dimension
of 1.2.

\end{enumerate}

\section{Unsupervised learning}
\label{sec:R-unsupervised}

\begin{enumerate}

\item The \texttt{geostats} package includes a function called
  \texttt{PCA2D} that can be used to reproduce
  Figures~\ref{fig:PCA2D1}--\ref{fig:PCA2D3}:

\begin{script}
X <- rbind(c(-1,7),c(3,2),c(4,3))
colnames(X) <- c('a','b')
PCA2D(X)
\end{script}

\noindent where \texttt{rbind} binds three rows together into one
matrix.

\item Verifying the equivalence of PCA and classical MDS:

\begin{script}[firstnumber=3]
d <- dist(X)           # create a Euclidean distance matrix
conf <- cmdscale(d)    # classical MDS
plot(conf,type='n')    # create an empty plot
text(conf,labels=1:3)  # add text labels to the empty plot
\end{script}

This script produces the same output as the first panel of
\texttt{PCA2D}.

\item \texttt{R} contains not one but two built-in PCA functions:
  \texttt{prcomp} and \texttt{princomp}. Both produce essentially the
  same output but use different algebraic
  algorithms\footnote{\texttt{prcomp} uses singular value
    decomposition, whereas \texttt{princomp} uses an eigen
    docomposition.} to achieve the matrix decomposition of
  Equation~\ref{eq:PCA}. Applying \texttt{prcomp} to the US arrests
  data to produce the biplot shown in Figure~\ref{fig:USArrests}:

\begin{script}
pc <- prcomp(USArrests, scale=TRUE)
biplot(pc)
\end{script}

\item Reproducing the MDS analysis of European road distances of
  Figure~\ref{fig:eurodist}:

\begin{script}
conf <- cmdscale(eurodist)
plot(conf,type='n',asp=1)
text(conf,labels=labels(eurodist))
\end{script}

Repeating the same exercise using nonmetric MDS:

\begin{script}
library(MASS)
mds <- isoMDS(eurodist)
conf <- mds$points
plot(conf,type='n',asp=1)
text(conf,labels=labels(eurodist))
\end{script}

Flip the y-axis to make the MDS configuration look more like the map
of Europe:

\begin{script}[firstnumber=4]
ylim <- rev(range(conf[,2]))        # reverse the minimum and maximum values
plot(conf,type='n',asp=1,ylim=ylim) # change the y-axis limits
\end{script}

Assess the goodness of fit on a Shepard plot:

\begin{script}[firstnumber=4]
sh <- Shepard(d=eurodist,x=conf)  
stress <- signif(mds$stress,2)
plot(sh,main=paste0('stress=',stress))
\end{script}

\noindent where \texttt{signif(x,2)} rounds \texttt{x} to 2
significant digits.

\item\label{it:R-kmeans} Visualise the iris data in a ${4}\times{4}$
  grid of scatter plots (Figure~\ref{fig:Iris}):

\begin{script}
measurements <- iris[,-5]
species <- iris[,5]
plot(measurements,pch=as.numeric(species))
\end{script}

\noindent where \texttt{as.numeric} converts the species to numbers,
which are subsequently used as colours. Classify the data into three
groups using the k-means algorithm:

\begin{script}[firstnumber=4]
fit <- kmeans(measurements,centers=3)
\end{script}

Compare the classes to the known species of the flowers:

\begin{console}
> table(fit$cluster,species)
    setosa versicolor virginica
  1      0          2        36
  2      0         48        14
  3     50          0         0
\end{console}

\item Hierarchical clustering of the iris data:

\begin{script}
tree <- hclust(dist(measurements))
plot(tree)
\end{script}

\end{enumerate}

\section{Supervised learning}
\label{sec:R-supervised}

\begin{enumerate}
\item\label{it:LDA} Discriminant analysis is implemented in the
  \texttt{MASS} package:

\begin{script}
library(MASS)
ld <- lda(Species ~ ., data=iris)
\end{script}

Predict the species of a new flower with a sepal length of 6.0~cm, a
sepal width of 3.0~cm, a petal length of 5.0~cm and a petal width of
1.5~cm:

\begin{script}[firstnumber=3]
newflower <- data.frame(Sepal.Length=6.0,Sepal.Width=3.0,
                        Petal.Length=5.0,Petal.Width=1.5)
pred <- predict(ld,newdata=newflower)
\end{script}

Query the posterior likelihoods of the LDA classification:

\begin{console}
> pred$posterior
        setosa versicolor virginica
1 3.207192e-27  0.8098087 0.1901913
\end{console}

\noindent which means that there is an 81\% chance that the new flower
is \textit{versicolor} and a 19\% chance that it is
\textit{virginica}.

Hence the predicted species of the new flower is:

\begin{console}
> pred$class
[1] versicolor
\end{console}

\item To apply quadratic discriminant analysis, we simply replace
  \texttt{lda} by \texttt{qda} in the previous code:

\begin{script}[firstnumber=2]
qd <- qda(Species ~ ., data=iris)
\end{script}

\noindent which produces the following outcome:

\begin{console}
> predict(qd,newdata=newflower)$posterior
         setosa versicolor virginica
1 2.147589e-103  0.7583223 0.2416777
\end{console}

Thus, according to QDA, there is a 76\% probability that the new
flower is \textit{versicolor}, and a 24\% chance that it is
\textit{virginica}.

\item Decision trees are implemented in the \texttt{rpart} package:

\begin{script}
library(rpart)
tree <- rpart(Species ~ ., data=iris, method="class")
plot(tree)
text(tree)
\end{script}

To add the misclassification rates to the tree:

\begin{script}[firstnumber=4]
text(tree, use.n=TRUE)
\end{script}

Using the \texttt{newflower} data frame that we created in
step~\ref{it:LDA}, the class probabilities are:

\begin{console}
> predict(object=tree,newdata=newflower)
  setosa versicolor  virginica
1      0  0.9074074 0.09259259
\end{console}

\noindent which leads to the following classification:

\begin{console}
> predict(object=tree,newdata=newflower,type='class')
         1 
versicolor 
\end{console}

\end{enumerate}
  
\section{Compositional data}
\label{sec:R-compositional}

\begin{enumerate}

\item Load the A--CN--K data of Section~\ref{sec:logratios} into
  memory and transform them from the ternary simplex to bivariate
  logratio space using the alr transformation:

\begin{script}
data(ACNK,package='geostats')
uv <- alr(ACNK)
plot(uv)
\end{script}

\noindent where the \texttt{alr} function is provided by the
\texttt{geostats} package (type \texttt{alr} at the command prompt to
see the code). Computing the mean and covariance matrix of the
logratio data:

\begin{script}[firstnumber=3]
mu <- colMeans(uv)
covmat <- cov(uv)
\end{script}

Add the mean to the logratio plot as a black square and the confidence
ellipse as a polygon:

\begin{script}[firstnumber=5]
points(mu,pch=22,bg='black')
ell <- ellipse(mu,covmat)
polygon(ell)
\end{script}

\item Plot the A--CN--K data on a ternary diagram:

\begin{script}
ternary(ACNK,labels=c(expression('Al'[2]*'O'[3]),
                      expression('CaO+Na'[2]*'O'),
                      expression('K'[2]*'O')))
\end{script}

\noindent where the \texttt{expression} function allows \texttt{R} to
use subscripts and special characters in text labels. Mapping the
logratio mean and error ellipse back to the ternary diagram:

\begin{script}[firstnumber=4]
ternary(alr(mu,inverse=TRUE),add=TRUE,type='p',pch=22,bg='black')
ternary(alr(ell,inverse=TRUE),add=TRUE,type='l')
\end{script}

\item Apply PCA to the major element compositions of
  Table~\ref{tab:Major}:

\begin{script}[firstnumber=3]
data(major,package='geostats')
comp <- clr(major)
pc <- prcomp(comp)
biplot(pc)
\end{script}

\item Apply LDA to the AFM data of Figure~\ref{fig:AFM}:

\begin{script}
library(MASS)    
data(AFM,package='geostats')
ld <- lda(x=comp,grouping=AFM$affinity)
\end{script}

\noindent where the \texttt{lda} function does not use formula
notation (Section~\ref{sec:R-supervised}) but an alternative format
(see \texttt{?lda} for further details).  Classify a new rock with
1~wt\% FeO, 8~wt\% Na\textsubscript{2}O+K\textsubscript{2}O, and
0.1~wt\% MgO:

\begin{script}[firstnumber=4]
newrock <- data.frame(F=1,A=8,M=0.1)
newcomp <- alr(newrock)
pr <- predict(object=ld,newdata=newcomp)
\end{script}

This produces:

\begin{console}
> pr$posterior
            ca          th
[1,] 0.9931941 0.006805909
\end{console}

\noindent which suggests that the new sample is a calc-alkaline
basalt.

\end{enumerate}
