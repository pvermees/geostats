========
Plotting
========

In this session, I will introduce some ways to plot statistical data
because, as the cliche goes, a picture is worth more than 1000 words,
and nowhere is this more true than in statistics. So before we explore
the more quantitative aspects of data analysis, it is useful to first
visualise the data.

---

Consider, for example, the following four bivariate datasets, which
are known as Anscombe's quartet, named after the statistician who
invented it. Each of the four datasets in the quartet contains two
variables X and Y. We have 10 measurements for each of these, which we
can attempt to summarise using so-called summary statistics.

Although summary statistics are the subject of the next session, I am
pretty sure that you are already familiar with the first of these
values, which is the mean. The mean is simply the sum of all the
values divided by the number of values. Well, the mean of all the X's
is nine. It's nine for the first data set. It's nine for the second,
the third as well as the fourth data set. So the means are identical
for X, but also for Y. The mean of the Y's is 7.5 for the first,
second, third and fourth dataset.

In the next session, we will see that the variance is a summary
statistic that can be used to quantify the spread or 'dispersion' of
the data. Well, the variance of X is 11 for all four data sets, and
the variance of the Y is 4.125. Again, the four data sets look
identical for this particular summary statistic.

The correlation coefficient is a parameter that we will define in the
session on linear regression, which is covered by Chapter 10 of the
notes.  The correlation coefficient of the X's and the Y's is 0.816
for all four datasets. And if we fit a line through the data with the
methods that will also be introduced in the session on linear
regression, then we get an intercept of 3 and a slope of 0.5. So
again, the four datasets look identical.

In conclusion, the quantitative summary statistics of these four
datasets all appear to be identical.

---

However, when we visualise them as scatter plots, we see that in fact
they are very different. Some of the datasets are clustered, others
are more spread out. There are certain patterns here. There are
outliers. They're all very different and this was not immediately
apparent from the summary statistic.

The take home message is that before we attempt to do any quantitative
analysis of your data, it is very important that we first have a look
at the data; that we visualise them.

Bivariate scatter plots are just one way to visualise analytical
data. There are many other graphical devices, each of which is
appropriate for a particular type of data. In the next set of slides I
will introduce a number of these different data types and highlight
the associated plots that you can use to explore and interpret the
data before proceeding to the quantitative aspect, which will be
discussed in later sessions.

--

In the remainder of this lecture, I will work with a dataset of twenty
imaginary river catchments have been analysed for six different types
of data: the lithology of the underlying bedrock and its stratigraphic
age (which is either Cenozoic, Mesozoic, Palaeozoic or Precambrian);
the number of natural springs in the catchments; the pH of the river
water; its Ca/Mg ratio; and the percentage of the catchment area that
is covered by vegetation.

In statistical textbooks you will find many different ways to classify
data like this. However, on the highest level, it is useful to make a
distinction between discrete and continuous data. Discrete data can be
visualised on bar charts or histograms and can be subdivided into
three other classes.

1. Categorical data take on a limited number of values, assigning each
'object' to a particular unordered class or category. Geological
examples of categorical data include animal species in a bone bed; the
modal composition of a thin section; and the lithologies in the
catchments dataset.

2. Ordinal data are a special type of categorical data, in which the
classes are ordered but the distances between them are either
irregular or unknown. Geological examples of ordinal quantities
include Moh's hardness scale; metamorphic grade; and the geologic
timescale, which is used to form the second column of the catchments
dataset.

3. Count data are a special type of ordinal data in which the
categories are evenly spaced into integer intervals. Geological
examples of count data include the number of gold chips found in a
panning session; the annual number of earthquakes that exceed a
certain magnitude; and the number of dry wells in a wildcat drilling
survey; and the number of natural springs in a river catchment.

Categorical, ordinal and count data are three classes of discrete
data, which can be assigned integer values. However, not all
geological or geophysical measurements take on discrete values. Many
are free to take on decimal values. We can also subdivide these
continuous data into further classes, such as:

4. Cartesian quantities are continuous variables that can have any
decimal value, including positive and negative ones.  Geoscientific
examples of this data type include the magnitude of earthquakes; the
spontaneous electrical potential between geological strata; or the pH
of aqueous solutions.  Although all the pH values in the catchments
dataset are positive, negative pH values are possible and do occur in
the real world although rarely or never in the natural environment.

5. Jeffreys quantities can only have positive values. Examples of this
include mass, volume, density, speed, etc. The Ca/Mg ratio of the
river water in our catchments also fits this definition. It is obvious
that negative Ca/Mg ratios do not exist.

6. Proportions are quantities that are constrained to a finite
interval from 0 to 1 (or from 0 to 100%). Examples of this include
chemical concentrations; volume fractions; and porosities. The final
column of the catchments dataset shows the percentage of each
catchment that is covered by vegetation.

---

Discrete datasets naturally fall into categories and the natural way
to plot them is as a bar chart. In the case of the lithology data, the
order of the categories along the horizontal axis is completely
arbitrary and can be changed without loss of information.  As the name
suggests, this is not so for ordinal data, in which the categories are
ranked. The four bins of the geologic timescale span vastly different
amounts of time, with the Cenozoic being just 65 million years long,
whereas the Precambrian is 4 billion years long. In contrast, the bins
of the count data are all equally sized.

Although continuous data do not naturally fall into distinct
categories, then can be coerced into a histogram by binning. This
reveals that the pH data follow a bell-shaped distribution, in
contrast with the Ca/Mg ratio data, which is characterised by a lot of
low values and comparatively few high values. Finally, the histogram
of the vegetation data shows two clusters of low and high values, with
few values in between.

---

Although the previous slide has shown that it is possible to divide
continuous data into discrete bins, doing so poses two practical
problems.

The first issues is that we need to decide how many bins to use and
how wide these bins should be. The number of bins strongly affects the
appearance of the histogram, as is illustrated on the top half of this
slide. The histogram on the left uses a bin width of one pH unit,
whereas the histogram on the right uses a bin width of half a pH
unit. The two histograms look considerably different, and it's not
immediately clear which choice of bin width is best.

The second problem is that we need to choose where to place the
bins. In the bottom half of the slide are shown two histograms, whose
bins have the same width of half a pH unit, but which have been offset
relative to each other by a quarter of a pH unit. This arbitrary
decision once again strongly affects the appearance of the histogram.

---

To solve the bin placement problem, let us fist develop a variant of
the ordinary histogram that is constructed as follows. First, we rank
the values from low to high along a line. Second, we stack a
rectangular box on top of these measurements. And third, we add all
these boxes together to produce one connected line. The figure on the
right hand side applies this procedure to the 20 pH measurements,
removing the need to choose the bin locations.  Normalising the area
under this curve produces a so called kernel density estimate, the
mathematical formulation of which is shown at the top.

This is a formula that includes a function capital K, which is the
kernel. This describes the shape of, in our case, the rectangular
boxes that are placed on top of each of the measurements. n represents
the number of measurements which, in our case, is 20 because we have
20 pH measurements. h is the band width, which describes the width of
these rectangles. Instead of the rectangular kernel, e could also use
triangles to construct the kernel density curve or any other symmetric
function.

---

The most popular choice of kernel for density estimation is probably
the Gaussian kernel whose function is shown in the top right
here. This produces a continuous curve, which does more justice to the
continuous pH data than the discrete steps of the histogram or the
rectangular kernel. Although kernel density estimation solves the bin
placement problem, it is not entirely free of design decisions either.

---

The bandwidth of a kernel density estimate fulfils a similar role as
the bin width of a histogram. Changes in the bandwidth affect the
smoothness of the kernel density curve. This is illustrated on the
left hand side of this slide, which shows a kernel density estimate
for the pH data that uses a band with a 0.1. On the right hand side is
the same dataset shown as a KDE with a band width of 1, which is 10
times bigger than 0.1. The left hand distribution is undersmoothed,
which means that it is too bumpy. In contrast, the curve on the right
hand side is possibly oversmoothed, and you might miss some details in
the distribution.

The selection of the band with is a similar problem to the selection
of a bin width for histograms. There are some rules of thumb that can
be used to optimise that selection, but I do not have the time to
discuss this in more detail.

---

Let's move on to the second continuous dataset of 20 measurements of
Ca/Mg ratio measurements. Now, Ca/Mg ratios are strictly positive
values. Yet the left tail of the kernel density estimate shown here
extends into negative data space, implying that there is a finite
chance of observing negative Ca/Mg ratios. This is clearly
nonsense. In geophysics, positive quantities are sometimes called
Jeffries quantities, named after the British geophysicist Sir Harold
Jeffries. As mentioned before, other examples of Jeffrey's quantities
are mass volume, density, speed, etc. These parameters exists within
an infinite half space between zero and plus infinity. They all
exhibit the problem where, if you have a lot of values close to zero,
that the kernel density estimate crosses over into physically
impossible negative values.

---

Fortunately, the negative value problem can easily be solved, using a
transformation that maps the Ca/Mg ratios from strictly positive
values to the infinite space of all the numbers, including both
negative and positive values. So shown here are on the left is the
original kernel density estimate. Taking the logarithm of the values
and constructing a new kernel density estimate produces a much more
symmetric curve. Then we can take the exponents of these values and
map those results back to linear space. This produces a third kernel
density estimate that does not cross over into negative data space.

This logarithmic transformation is one example of a simple
transformation of the data solving some problems associated with
constrained data.  We will see this solution in different forms,
appearing again later in the module.

---

Jeffrey's quantities are just one example of constrained
measurements. As another example, consider the 20 vegetation coverage
measurements of the catchments data. As discussed before, vegetation
cover is a proportional quantity that takes on values between 0 and
100%. Again, the Gaussian kernel density estimate of the data plots
into physically impossible values of negative vegetation coverages, or
values that exceed 100%.

---

Using a similar approach as before, we can solve the problem of
impossible values using a data transformation that maps the data from
the constrainted space of values between 0 and 1 to the entire line of
numbers, from minus infinity to plus infinity. For proportional data
like vegetation cover, this is no achieved with a simple log
transformation, but with a so-called logistic transformation. So if
'x' is our vegetation cover, then 'u' is the logistically transformed
value, which is the logarithm of x, divided by 1-x. After constructing
the kernel density estimate of the logit of the vegetation values, we
can then map those results back to our constrainted space from 0 to 1,
using an inverse log-ratio transformation, where we take the exponents
of the logits divided by the exponents of the logits plus 1.

This solves our problem. The KDE no longer crosses over into
impossible negative values or vegetation coverage values that are
greater than one. We will see later on that this logistic
transformation is a special case of a general class of logratio
transformations that will be useful for the analysis of compositional
data, which include chemical data, mineralogical data and so
forth. But that won't be discussed until Chapter 14 of the notes.

---

Kernel density estimates can easily be generalised from 1 to 2
dimensions. For example, this slide shows a dataset of eruption
timings from the old Faithful Geyser in Yellowstone National Park in
the United States. The dataset records 272 observations shown as grey
circles of two variables. We've got the duration of each eruption on
the Y axis on the waiting times on the X axis, both expressed in
minutes. On the top and at the right, there are two marginal
distributions, which represent projections of the data onto the X and
Y axis. They show us the kernel density estimate of the waiting times
and of the durations of the eruptions, respectively. The kernel
density estimate here has been constructed in a very similar way as
the one dimensional KDE. But instead of plotting a one dimensional
bell curve on top of each measurement we've stacked a two dimensional
bell curve.

We can generalise the same idea to greater than two dimensions. But
then it becomes very difficult to visualise the result on a two
dimensional sheet of paper or a two dimensional computer screen. In
this case, there are two options. If you want to visualise higher
dimensional data, we could either plot the data as a series of one or
two dimensional marginal plots like these guys, or we can extract the
most important patterns or trends in the data by projection onto a
lower dimensional plane. Then we can show these predict projected data
as a two dimensional graphic. The second strategy is known as
ordination. We will discuss it later on in the session that is
dedicated to so called unsupervised machine learning.

Both histograms and kernel density estimates require the selection of
a smoothing parameter. For the histogram, this is the bin width,
whereas for the kernel density estimate it is the bandwidth.  This
selection of a smoothing parameter adds a certain degree of
arbitrariness to the creation of these density estimates.

---

An empirical cumulative distribution function, or ECDF, is an
alternative data visualisation device that avoids this problem and
does not require smoothing. A cumulative distribution function is a
step function that shows, on the Y axis, the fraction of the
measurements that are less than or equal to the values shown in the X
axis. For example, here we've got 40% of the pH values being less than
4.6. Empirical cumulative distribution functions do not require
binning or selecting a bandwidth because they do not require
smoothing. They do not spill over into physically impossible values,
so there are no negative Ca/Mg values. Similarly, there are no
negative values for vegetation coverage, and neither are there any
vegetation coverages exceeding 100%. Therefore, the construction of a
cumulative distribution function is completely hands off.

The visual interpretation of a cumulative distribution function is
different from that of a histogram or kernel density estimate. Whereas
different clusters of values stand out as peaks in the histogram or
KDE, they are marked by steep segments of the cumulative
distribution. For example, the geyser data have two modes, with one
cluster of eruptions that last 2 minutes, and a second cluster of
eruptions that last 4.5 minutes. These two clusters correspond to
peaks in the kernel density estimate. The same values of 2 and 4.5
minutes are marked by the steepest parts of the cumulative
distribution function. Some people find this these cumulative
distribution functions slightly more difficult to read than histograms
or kernel density estimates. And this is perhaps why histograms and
KDEs are found more frequently in the literature. Nevertheless,
cumulative distributions are extremely useful. In fact, we will see
see that they can even be used for statistical tests, to to define
some important summary statistics. But those applications will be
discussed in the next session of this module.

==================
Summary statistics
==================

After a purely qualitative inspection of the data, we can now move on
to a more quantitative description. In this session, I will introduce
a number of summary statistics to describe larger data sets, using
just a few numerical values that capture the location, the dispersion
and the shape of a probability distribution. Before proceeding with
this topic, it is useful to bear in mind that these summary statistics
have limitations.

---

The Anscombe Quartet, which was discussed in the previous session,
showed that very different looking data sets can have identical
summary statistics, including the same mean and the same variance. But
with this caveat in mind, summary statistics are an essential
component of data analysis, provided that they are preceded by a
visual inspection of the data.

---

In this session, I will introduce three types of summary statistics. A
first class of summary statistics are measures of location, which
represent the average of the data, and which quantify whether that
average is shifted towards lower values or towards high values.

A second group of summary statistics represent measures of
dispersion. These quantify the spread of the data, capturing whether
the date are concentrated near a single value, or whether they are
spread out over a wider range of values.

And finally, a third group of summary statistics quantify the shape of
the distribution; for example, whether it is leaning towards the left
or leaning towards right; whether it is skewed towards high
values or skewed towards low values.

---

There are many different ways to define the average value of a
multi-value dataset.  In this session. I will introduce three of these
ways, but later sessions will introduce a few more. The arithmetic
mean is arguably the best known and most widely used way to average
data. If we have n values x_1, x_2 and so forth to x_n, then the
arithmetic mean, which is often denoted by x-bar, is simply the sum of
the x-values divided by the number of values.

But this is just one of many ways to average data. Another,
non-parametric, approach is the median. This value is obtained by
ranking the observations according to size from the smallest value to
the largest value and selecting the middle one. The median is the
half-way point on an empirical cumulative distribution function, the
step function that I introduced in the previous session. Put in more
formal terms, the median is identical to the 50 percentile of the
distribution.

Finally, the mode is simply the most frequently occurring value. For a
continuous variable, it is the highest point on the kernel density
estimate or a histogram, or the steepest point on a cumulative
distribution function.

---

Applying these three concept to the pH data of the previous session,
the arithmetic mean is calculated, as I said, by taking to sum of
these 20 values; dividing this sum by 20 and yields a value of five.

The median is obtained by ranking the pH measurements from the most
acidic value of 3.8 to the most basic value of 6.2. If there are an
even number of values in the dataset, which is indeed the case here,
then we take the arithmetic mean of those values. In this case, the
two values are 5.0 and 5.2, so our median is 5.1.

Finally, the mode for this continuous variable is obtained by
constructing a kernel density estimate, the heighest point of which
corresponds to a pH value of 5.4. So that is the modal value that we
can use as a summary statistic.

---

Here the three different summary statistics are shown together
graphically for the pH dataset on a kernel density estimate and an
empirical cumulative distribution function. The median is again the 50
percentile of the data set. So it is the intersection between the
dash-dot line onto the empirical cumulative step function. This
intersection is marked by the dashed line, which indicates that 50% of
the pH measurements are less than 5.1, and 50% are higher than 5.1.

The arithmetic mean, which is shown as a solid white line here, and
the mode, which is this dotted line are both in reasonably close
vicinity to the median. So in this case, it doesn't really matter
which summary statistic you use. All measures of location give you
pretty much the same value.

---

However, the situation is very different for our dataset of Ca/Mg
ratios which, unlike the pH data, does not follow a symmetric but an
asymmetric distribution. Here, the mean, which is shown as a solid
line, is 3.2. The mode, which is shown by a dotted line, is only 0.45
and the median falls somewhere in between, at a value of 2.1. There is
a factor of seven difference between the smallest measure of location
and the largest measure of location. The mean in particular is
strongly affected by the long tail of large outliers towards the right
of the diagram.

Only 30% of the data are larger than the mean of the distribution, and
only one of the 20 measurements, which is only 5% of the data, is
smaller than the mode. So in this case, it is fair to say that the
choice of measure of location is very important. The median is the
probably most sensible value in this case. In contrast, the arithmetic
mean is not representative of the data.

---

Finally, this slide shows rug plots, kernel density estimates and
empirical cumulative distribution functions for the vegetation data on
the left, and the geyser eruption duration data on the right. Both of
these distributions are bimodal, meaning that they have two peaks in
the kernel density estimates and too steep segments in the cumulative
distribution function.

The highest of these peaks are marked by dotted lines, which give us
the modes but ignore the other peaks. The mean and the median fall in
between the two data clusters and are not representative of the
data.

So in conclusion, considering all four dasets, we can say that the
mean is only a meaningful measure of location for unimodal and
symmetric distributions. The arithmetic mean is more strongly affected
by outliers than the median. Therefore, the median is a more robust
estimator of location for asymmetric data sets than the mean. However,
multimodal data sets such as these two here can never be adequately
summarised with a single location parameter. And that is why it is so
important to visualise the data before you go ahead and use the
summary statistics.

---

OK, let us now move on to the second class of summary statistics,
which are measures of dispersion. It is rare for all the values in a
dataset to be exactly the same. In most cases, the values are spread
out over a finite range of values. The amount of spread can be defined
in a number of ways, the most common of which are the standard
deviation, the median absolute deviation and the interquartile
range.

The standard deviation is closely related to the arithmetic mean
expert, so given n values x_1, x_2 to x_n, the standard deviation is
obtained by taking the sum of the squared differences between each
value and the arithmetic mean. The differences are squared because,
intuitively, we want the dispersion to be a positive number.  We then
divide this sum by n-1 and take the square root. I will discuss the
factor n-1 in a later session, but at the moment you just take that as
a given.

If we omit the square root, then then this corresponds to the
variance.  So the variance is the square of the standard deviation.
Both of these are used as measured of dispersion, but the standard
deviation is more useful as a summary statistic.

The median absolute deviation uses, as the name suggests, not the
arithmetic mean but the median. It is the median of the absolute
values, which is what these vertical lines mean, of the differences
between each value and the median of the entire data set. Again, like
the standard deviation, this is a positive number.

And then, finally, the interquartile range is defined using the
empirical cumulative distribution function. Recall that the median is
defined as the value that corresponds to the intersection between a
horizontal line that goes through 0.5. Similarly, the 75 percentile is
obtained by taking the intersection of a horizontal line that goes
through 0.75 on the empirical cumulative distribution, and we can also
do the same for the 25 percentile. These three percentiles are called
the lower, middle and upper quartiles. The interquartile range is
simply the difference between the 75 and 25 percentiles.  Again, it is
a positive number.

---

Let us apply these concepts to our pH data. The standard deviation is
given by this formula. So first we list the 20 measurements in our
original unsorted order. We take the difference between each value and
the arithmetic mean, which had calculated in the previous step. That
gives us 20 numbers that can either be positive or negative. We square
these values to turn them all into positive numbers. We sum these
squared differences, giving us a value of 8.42.

Then we divide this sum by n-1, which is 19. After taking the square
root, we get a standard deviation of 0.67. The interquartile range is
calculated by first sorting our values. So these values are exactly as
the same as those, but they are ranked from the smallest (3.8) to the
highest (6.2). The 25 percentile (or lower quartile) is the arithmetic
mean of the fifth and the sixth value, which is 4.55. The 75
percentile (or upper quartile) is the arithmetic mean of the 15th and
the 16th value, which is 5.55. The interquartile range is then simply
5.55-4.55 = 1.0.

To calculate the median absolute deviation, we need to take the
difference between each of the values and the median, which we already
calculated before. That gives us values that there are either positive
or negative. We then take the absolute value of these values, turning
them all into positive numbers. We sort these values from small to
large and take their median, which is 0.5. So our median absolute
deviation is 0.5.

---

And finally, a third group of summary statistics that I would like to
discuss today involves measures of shape for probability
distributions.  Like the measures of location and dispersion that I
talked about previously, also the shape of a distribution can be
quantified in a number of different ways. I won't go into much detail
about this, but just introduce you to one measure of shape, which is
the skewness.

The definition of skewness looks very similar to the definition of the
variance. Like the variance, also the skewness is a sum of differences
between the values and their arithmetic mean. But whereas the variance
raises these differences to the second power, the skewness raises them
to the third power. The variance and skewness are both known as
moments of the distribution.

The variance is a second moment, whereas the skewness is the third
moment. You can also define higher order moments such as, for example,
the kurtosis, which is proportional to the fourth power of these
differences. I won't discuss the kurtosis and much detail, but in
principle you can describe the entire distribution by simply listing
all the moments. However this is a more advanced subject.

Here, at the bottom, are three examples, three datasets that have
different skewness. The skewness of the pH data is close to zero
because this is a symmetric distribution. The Ca/Mg data lean heavily
towards the left and have a long tail towards high values. This data
set is positively skewed. And then here on the right is a dataset that
I downloaded from the web. It gives us the Covid-19 mortality rate in
the United Kingdom during March of 2020. This distribution is
negatively skewed, where highest number of deaths per 100,000 people
was among the older population. But there is a long tail towards
younger people as well.

---

The most important summary statistics can be jointly visualised in a
compact way on a so-called box-and-whisker plot. As the name suggests,
a Box and whisker plot consists of a box, which is drawn from the
first to the third quartile, so from the 25 percentile to the 75
percentile of a dataset. The median is marked by a line in the middle
of that box, and then two lines or whiskers extend from this box
outwards towards the minimum value and the maximum value of the
dataset, ignoring outliers, where outlines are defined as all those
points that fall more than 1.5 times the interquartile range below the
first quartile, or more than 1.5 times the interquartile range above
the third quarter.

In this particular example, there is no lower outlier, but we do have
one outlier at the high end of our dataset. The median is offset
towards the left hand side of the box, indicating the positive
skewness of this particular dataset. And we can plot these Box and
whisker plots either horizontally, as on this slide, or alternatively,
you also can put them vertically. Box plots can be used to visualise a
single sample or multiple samples. In the latter case, box plots allow
easy inspection, in just one glance, of the variability in location,
spread and shape of large numbers of samples. We will actually do this
in one of our exercises.

===========
Probability
===========

Probability is a fundamental concept in mathematical statistics, that
pops up in various contexts, as a numerical description of how likely
it is for an event to occur, or how likely it is for a proposition to
be true.

---

In the context of a sampling experiment in which all the outcomes are
equally likely, the probability of an outcome A can be defined as the
ratio of the number of ways in which A can occur, divided by the total
number of possible outcomes.

---

For example, the probability of tossing an unbiased coin and observing
head is the ratio of the number of outcomes of head which, in the case
of a single coin, is one divided by the total number of outcomes,
which is either head or a tail. So that probability is 1 divided by 2
or one half.

---

Next, let's calculate the probability of tossing the same unbiased
coin three times and observing 2 x head and 1 x tail. This probability
is a total number of cases in which we've got one T and two H's, which
happens in three scenarios, divided by the total number of outcomes,
which is eight. So the probability of two heads and one tail is
3/8ths.

---

Similarly, the probability of throwing two dice and obtaining one 2
and a one 6 is the total number of ways to get a 2 and a 6, which is
either first a 2 and then a 6 or first a 6 and then a 2, divided by
the 6 x 6 = 36 possible outcomes of the dice throwing experiment. So
we get 2 divided by 36, which equals 1 divided by 18, and the
probability of this outcome is 1/18th.

---

The multiplicative rule of probability dictates that the probability
of two combined INDEPENDENT experiments is given by the product of
their respective probabilities. Independent means that the outcome of
the first experiment does not affect the outcome of the second.  We
will discuss situations where this is not the case at the end of this
session.

If we carry out two coin tossing experiments, then the probability of
obtaining a head in the first experiment AND a head in the second
experiment is simply one half times one half, which is one
quarter. Similarly, when we carry out a coin tossing experiment AND a
die throwing experiment, then the probability of obtaining two heads
and one tail for the first experiments, and throwing a two and a six
in the second experiment is the product of 3/8ths and 1/18th, which is
3/144ths, or about 2.1%.

The additive rule of probability dictates that the probability of
observing either of two mutually exclusive outcomes is given by the
sum of their respective probabilities. So if you toss three coins,
then the probability of obtaining two heads and one tail OR of
obtaining three heads is the sum of 3/8ths and 1/8th, which is 4/8ths
or 50%.

---

However, if the two outcomes that you are comparing are not mutually
exclusive, then the additive rule needs to be modified to account for
the overlap between the two outcomes.

For example, if we combine the probability of success in a coin
tossing experiments where success is defined as two heads and a tail,
with the probability of success in a dice throwing experiment where
success is defined as obtaining a two and a six, then the probability
of success in the first OR the second experiments equals the sum of
the probability of success in the first experiment, which includes all
possible outcomes for the second experiment, PLUS the probability of
success in the second experiment, which includes all outcomes of the
first experiments MINUS the probability of success in both
experiments, which would otherwise be double-counted. And that gives
us a probability of 0.41.

---

Let us now move on to an important subject in statistics that will
make your brain hurt. This subject is called combinatorics. A
permutations is a first concept in combinatorics that refers to an
ordered arrangement of objects. These objects can be selected either
by sampling with replacement, or by sampling without
replacement. Let's illustrate the difference between these two
concepts. Consider one urn with 20 balls that are numbered from 1 to
20. Suppose that we draw 10 balls from this urn. Then there are two
ways for doing so.

---

A first method is to sample the balls with replacement. So we draw a
first ball from the urn. There are 20 possible ways for doing so. And
we write down its number. Suppose that this is 5. We then put the ball
back into the urn. We thoroughly mix the balls and draw a second
number. Again, there are 20 ways for doing so. Suppose that the second
outcome is two. The total number of possible ways in which we can
select or draw two balls from the urn with replacement is 20 times 20,
which is 400.

We could then collect a 3rd ball, a 4th ball, and so forth until we
have collected 10 in total. Then the total number of possible outcomes
for this experiment is 20 times 20 times 20 etc. So we've got 20 to
the tenth power. This is a very large number of outcomes indeed
because we replaced the balls after writing down the numbers.

---

There is a possibility that we've got duplicate values in our
collection of 10 numbers. In this example, here we've got two
appearances of the number 5 and two appearances of the number 19.

---

This duplication of values does not occur under an alternative
experimental design, in which the sampling is not done with but
without replacement. So consider the same urn with 20 balls as before,
and draw the first number like before. There are 20 possible ways to
do so.

Suppose that the number on the first ball is 15. This time we do not
put ball number 15 back into the urn. With the first ball removed, we
now draw a second number. This time there are not 20, but only 19
possible ways for doing so. Suppose that the second number is 8. The
total number of ways in which we can collect two balls out of 20 is
not 20 times 20, but 20 times 19. We can repeat this experiment for
the third or fourth balls from the urn, which is progressively
dwindling in size, and the total number of ways to collect 10 balls
out of 20 from the original urn is not 20 to the tenth power, but 20
times 19 times 18, etc. to 11.

This can be written as 20!/10!, where ! means n times n-1 times n-2,
etc., and is denoted by the exclamation mark. 20!/10! is a smaller
number that 20^10.

---

OK, let's now apply these two formulas for sampling with and without
replacements to a classical problem in statistics. Suppose that we
have a classroom with k students, then what is the probability that at
least two of these students celebrate their birthdays on the same day?

---

To calculate this probability, it is useful to first compute the
probability that none of the birthdays overlap and that everybody's
got a unique birthday. To that end, we can see that there are 365^k
possible birthdays, including overlapping birthdays. So this is
sampling with replacement.

And of these 365^k possible combination of birthdays, 365!/(365-k)!
birthdays do not overlap which is the formula for sampling without
replacement.

---

Then the probability that no birthdays overlap is simply the ratio of
this number of outcomes divided by this number of outcomes and that
produces 365!/[(365 - k)!365^k]. The probability that at least one
pair of students have overlapping birthday is then the complement of
this probability, so one minus this number.

---

When we plot this probability against the size of the class, then we
can see that initially the probability of overlapping birthdays is
small. If there are only two students in our class, the probability
that they celebrate their birthdays on the same day is 1/365. But this
probability increases rapidly so that by the time you reach a
classroom size of 23, there is a greater than 50% chance that at least
two students in that class will celebrate their birthdays on the same
day.

---

Having considered coins, dice and lottery balls, let us now
(literally) deal with a fourth archetypal source of statistical
experiments, namely playing cards.

The formula for sampling with replacement tells us that there are
52!/49! or 132,600 unique possible ways to select three cards from a
deck. Suppose that we have drawn an ace of spades, a 6 of diamonds and
a king of hearts.

Then there are 3! = 6 ways to order these cards.

Suppose that we don't care in which order the cards appear. How many
different three card hands are possible?

---

Well, it is easy to see that the number of ordered samples equals the
product of the number of unordered samples times the number of ways to
order those samples.

Rearranging this equation for the number of unordered samples gives us
the ratio of the possible number of ordered samples to the number of
ways to order those samples.

Both of these numbers can be calculated using the formula for sampling
without replacement. As we have seen before, the number of ways to
select k objects from a collection of n is n!/(n-k)!, and the number
of ways to shuffle those objects around is k!.

The ratio of those numbers is also known as the binomial coefficient,
which is pronounced as "n-choose-k".

---

Applying this formula to the card dealing example, the number of ways
to draw three cards from a deck of 52 is 52-choose-3, or 22,100.

---

So far, we have assumed that all coin tosses or throws of a dice were
done independently, so that the outcome of one experiment did not
affect that of the other. However, this is not always the case in
geology. Sometimes one event depends on another. We can capture this
phenomenon with this definition.

P of A vertical bar B stands for the conditional probability of A
given B. To illustrate this concept, suppose that A is ammonites and B
is Bajocian, which is a particular stage of the Jurassic in which
Ammonites are found. Then P of A bar B stands for the probability of
finding an Ammonite in a Bajocian deposit.

---

The multiplication law dictates that the probability of outcomes A and
B both occurring simultaneously equals the probability of A given B
times a probability of B or, equivalently, the probability of B given
A times the probability of A, which equals the probability of B and A
both occurring.

So for example, suppose that 70% of a field area is covered by
Bajocian ocean deposits, and that 20% of the Bajocian deposits contain
ammonites. Then the probability of finding a Bajocian
Ammonite is 0.7 times 0.2, which is 14%.

---

The law of total probability prescribes that, if we have any mutually
exclusive outcomes B_1, B_2, etc., the the total probability of A
is the probability of A given B_1 times the probability of B_1,
plus the probability of A given B_2 times the probability of B_2,
etc. for all B_i.

---

So, if we have a river catchment that contains 70% Bajocian deposits
and 30% Bathonian deposits, where Bajocian is denoted as B_1 and
Bathonian as B_2, and we have a 20% chance of observing ammonites and
the Bajocian and a much greater 50% chance of observing ammonites in a
Bathonian deposit, then we can use the law of total probability to
calculate the probability of observing ammonites in our catchment.

It is simply the sum of the products of the probability of ammonites
in the Bajocian times the relative area covered by Bajocian deposits,
plus the probability of finding ammonites in the Bathonian times the
relative area covered by Bathonian deposits, which is 0.2 times 0.7 +
0.5 times 0.3.  In other words, there is a 29% chance of finding
ammonites in our field area.

---

The formula for the multiplication law can easily be rearranged to
form a famous formula in statistics called "Bayes' Rule", which allows
us to calculate the probability of B given A from the probability of A
given B and the total probabilities of A and B.

---

If there are multiple possible outcomes for B, then we can rephrase
Bayes' Rule to allow us to calculate the probability of any of these
subgroups of B given A from the probabilities of A given the different
subgroups of B, using the law of total probability. So we simply plug the
law of total probability into Bayes' rule and we get this generalised
form of Bayes rule for multiple outcomes of B.

---

These equations all look a little bit abstract, perhaps, but hopefully
they will be a little clearer if we apply Bayes rule to a specific
problem. Consider the same catchment as before, with 70% Bajocian
deposits and 30% Bathonian deposits. Suppose that there is a river in
this catchment, and that we have found an ammonite fossil in the
riverbed. Is this fossil likely to be Bajocian or Bathonian?

Well, to calculate the probability that it is Bajocian, we take the
ratio of the probability of finding an ammonite in the Bajocian times
the fraction of the field area covered by Bajocian deposits, divided
by the sume of the products of the probability of observing an
ammonite in the Bajocian times the fraction of the area covered by
Bajocian, plus the probability of observing an ammonite in the
Bathonian times the fraction of the field area covered by Bathonian
deposits. Plugging in the numbers yields a 48% chance that our
ammonites is of Bajocian age and a 52% chance that it is of Bathonian
age.

================
