=======
Welcome
=======

Welcome to GEOL0061, Statistics for Geoscientists. My name is Pieter
Vermeesch. I am a geochronologist at UCL, and geochronology is just
one of many fields within the Earth sciences that require a solid
understanding of statistics. In this module, you will learn essential
tools and techniques to analyse not only geochronological data, but
also data in structural geology, palaeobiology, sedimentology,
geophysics and environmental studies.

In this module, I will try to strike a balance between providing you
with practical recipes that you can use for your research later in
your career at UCL and beyond, whilst at the same time also explaining
the theory behind these recipes. This is important because without
some theoretical understanding, it is not always possible to
appreciate the limitations of statistical methods and avoid mistakes
when you apply them in a new context.

To help you make better use of statistics in the future, I will also
teach you a statistical programming language called R. Those of you
who have taken my Isotope Geology module will already be familiar with
this language.  But don't worry if you did not take Isotope Geology
because we will start again from scratch.

To help you climb the steep statistical learning curve, I have written
a detailed set of lecture notes for you, which are available on
Moodle. I have printed out a hard copy of these notes for each of you,
which will provide to you free of charge. I have tried to make these
notes as accessible as possible by using a very graphical approach to
explaining the various statistical concepts. So the notes include more
than 180 figures, which were all created in R.

We will cover the material in these notes over the course of 20
sessions, which will run on Tuesdays from 11 to 1, and on Fridays from
4 to 6. All 20 sessions will follow essentially the same
format. First, I will provide some theoretical background. Second, you
will answer some short quiz questions on Moodle. Your answers to these
quizzes will not count towards your final mark. Third, you will follow
one of the tutorials of Chapter 17 in the notes to familiarse yourself
with the R functions that you need to tackle the exercises that will
be solved in the fourth part of each session.

On the Moodle page you will also find some additional
exercises. Although no new material is introduced in these exercises,
completing them will deepen your understanding of the course material.

The R practicals will be done in a programming environment called
RStudio. Both R and RStudio are free and work in any operating system,
including Windows, Mac OS and Linux. They have been installed on the
cluster computers. Alternatively, or additionally, you can also use
your own computer. Installation instructions are provided on the
Moodle page.

The assessment of this module consists of two parts. 30% of the marks
will be awarded to two pieces of R-based coursework that will be set
during weeks 5, and 10 of our 10-week timetable. One third of these
coursework marks will be reserved for originality, so I would urge you
to complete these pieces of coursework independently.

The remaining 70% of the marks will be reserved for the final exam,
which will take place in the third term. This will be three hour,
in-person, pen-on-paper, open-book exam. It consist of 13 questions,
which must all be answered. You may bring your lecture notes to the
exam, including any annotations that you may have made in them.

Please use the Moodle forums or drop me an email at any time during
this term if you have any questions, I will try to answer your
question as soon as possible.  If you find any mistakes in the notes,
then please let me know and I will update them immediately. As the
cliche goes, there is no such thing as a bad question, and I am
grateful for any feedback. I hope that you will enjoy the module and
look forward to seeing you in class.

========
Plotting
========

In this session, I will introduce some ways to plot statistical data
because, as the cliche goes, a picture is worth more than 1000 words,
and nowhere is this more true than in statistics. So before we explore
the more quantitative aspects of data analysis, it is useful to first
visualise the data.

---

Consider, for example, the following four bivariate datasets, which
are known as Anscombe's quartet, named after the statistician who
invented it. Each of the four datasets in the quartet contains two
variables X and Y. We have 10 measurements for each of these, which we
can attempt to summarise using so-called summary statistics.

Although summary statistics are the subject of the next session, I am
pretty sure that you are already familiar with the first of these
values, which is the mean. The mean is simply the sum of all the
values divided by the number of values. Well, the mean of all the X's
is nine. It's nine for the first data set. It's nine for the second,
the third as well as the fourth data set. So the means are identical
for X, but also for Y. The mean of the Y's is 7.5 for the first,
second, third and fourth dataset.

In the next session, we will see that the variance is a summary
statistic that can be used to quantify the spread or 'dispersion' of
the data. Well, the variance of X is 11 for all four data sets, and
the variance of the Y is 4.125. Again, the four data sets look
identical for this particular summary statistic.

The correlation coefficient is a parameter that we will define in the
session on linear regression, which is covered by Chapter 10 of the
notes.  The correlation coefficient of the X's and the Y's is 0.816
for all four datasets. And if we fit a line through the data with the
methods that will also be introduced in the session on linear
regression, then we get an intercept of 3 and a slope of 0.5. So
again, the four datasets look identical.

In conclusion, the quantitative summary statistics of these four
datasets all appear to be identical.

---

However, when we visualise them as scatter plots, we see that in fact
they are very different. Some of the datasets are clustered, others
are more spread out. There are certain patterns here. There are
outliers. They're all very different and this was not immediately
apparent from the summary statistic.

The take home message is that before we attempt to do any quantitative
analysis of your data, it is very important that we first have a look
at the data; that we visualise them.

Bivariate scatter plots are just one way to visualise analytical
data. There are many other graphical devices, each of which is
appropriate for a particular type of data. In the next set of slides I
will introduce a number of these different data types and highlight
the associated plots that you can use to explore and interpret the
data before proceeding to the quantitative aspect, which will be
discussed in later sessions.

--

In the remainder of this lecture, I will work with a dataset of twenty
imaginary river catchments have been analysed for six different types
of data: the lithology of the underlying bedrock and its stratigraphic
age (which is either Cenozoic, Mesozoic, Palaeozoic or Precambrian);
the number of natural springs in the catchments; the pH of the river
water; its Ca/Mg ratio; and the percentage of the catchment area that
is covered by vegetation.

In statistical textbooks you will find many different ways to classify
data like this. However, on the highest level, it is useful to make a
distinction between discrete and continuous data. Discrete data can be
visualised on bar charts or histograms and can be subdivided into
three other classes.

1. Categorical data take on a limited number of values, assigning each
'object' to a particular unordered class or category. Geological
examples of categorical data include animal species in a bone bed; the
modal composition of a thin section; and the lithologies in the
catchments dataset.

2. Ordinal data are a special type of categorical data, in which the
classes are ordered but the distances between them are either
irregular or unknown. Geological examples of ordinal quantities
include Moh's hardness scale; metamorphic grade; and the geologic
timescale, which is used to form the second column of the catchments
dataset.

3. Count data are a special type of ordinal data in which the
categories are evenly spaced into integer intervals. Geological
examples of count data include the number of gold chips found in a
panning session; the annual number of earthquakes that exceed a
certain magnitude; and the number of dry wells in a wildcat drilling
survey; and the number of natural springs in a river catchment.

Categorical, ordinal and count data are three classes of discrete
data, which can be assigned integer values. However, not all
geological or geophysical measurements take on discrete values. Many
are free to take on decimal values. We can also subdivide these
continuous data into further classes, such as:

4. Cartesian quantities are continuous variables that can have any
decimal value, including positive and negative ones.  Geoscientific
examples of this data type include the magnitude of earthquakes; the
spontaneous electrical potential between geological strata; or the pH
of aqueous solutions.  Although all the pH values in the catchments
dataset are positive, negative pH values are possible and do occur in
the real world although rarely or never in the natural environment.

5. Jeffreys quantities can only have positive values. Examples of this
include mass, volume, density, speed, etc. The Ca/Mg ratio of the
river water in our catchments also fits this definition. It is obvious
that negative Ca/Mg ratios do not exist.

6. Proportions are quantities that are constrained to a finite
interval from 0 to 1 (or from 0 to 100%). Examples of this include
chemical concentrations; volume fractions; and porosities. The final
column of the catchments dataset shows the percentage of each
catchment that is covered by vegetation.

---

Discrete datasets naturally fall into categories and the natural way
to plot them is as a bar chart. In the case of the lithology data, the
order of the categories along the horizontal axis is completely
arbitrary and can be changed without loss of information.  As the name
suggests, this is not so for ordinal data, in which the categories are
ranked. The four bins of the geologic timescale span vastly different
amounts of time, with the Cenozoic being just 65 million years long,
whereas the Precambrian is 4 billion years long. In contrast, the bins
of the count data are all equally sized.

Although continuous data do not naturally fall into distinct
categories, then can be coerced into a histogram by binning. This
reveals that the pH data follow a bell-shaped distribution, in
contrast with the Ca/Mg ratio data, which is characterised by a lot of
low values and comparatively few high values. Finally, the histogram
of the vegetation data shows two clusters of low and high values, with
few values in between.

---

Although the previous slide has shown that it is possible to divide
continuous data into discrete bins, doing so poses two practical
problems.

The first issues is that we need to decide how many bins to use and
how wide these bins should be. The number of bins strongly affects the
appearance of the histogram, as is illustrated on the top half of this
slide. The histogram on the left uses a bin width of one pH unit,
whereas the histogram on the right uses a bin width of half a pH
unit. The two histograms look considerably different, and it's not
immediately clear which choice of bin width is best.

The second problem is that we need to choose where to place the
bins. In the bottom half of the slide are shown two histograms, whose
bins have the same width of half a pH unit, but which have been offset
relative to each other by a quarter of a pH unit. This arbitrary
decision once again strongly affects the appearance of the histogram.

---

To solve the bin placement problem, let us fist develop a variant of
the ordinary histogram that is constructed as follows. First, we rank
the values from low to high along a line. Second, we stack a
rectangular box on top of these measurements. And third, we add all
these boxes together to produce one connected line. The figure on the
right hand side applies this procedure to the 20 pH measurements,
removing the need to choose the bin locations.  Normalising the area
under this curve produces a so called kernel density estimate, the
mathematical formulation of which is shown at the top.

This is a formula that includes a function capital K, which is the
kernel. This describes the shape of, in our case, the rectangular
boxes that are placed on top of each of the measurements. n represents
the number of measurements which, in our case, is 20 because we have
20 pH measurements. h is the band width, which describes the width of
these rectangles. Instead of the rectangular kernel, e could also use
triangles to construct the kernel density curve or any other symmetric
function.

---

The most popular choice of kernel for density estimation is probably
the Gaussian kernel whose function is shown in the top right
here. This produces a continuous curve, which does more justice to the
continuous pH data than the discrete steps of the histogram or the
rectangular kernel. Although kernel density estimation solves the bin
placement problem, it is not entirely free of design decisions either.

---

The bandwidth of a kernel density estimate fulfils a similar role as
the bin width of a histogram. Changes in the bandwidth affect the
smoothness of the kernel density curve. This is illustrated on the
left hand side of this slide, which shows a kernel density estimate
for the pH data that uses a band with a 0.1. On the right hand side is
the same dataset shown as a KDE with a band width of 1, which is 10
times bigger than 0.1. The left hand distribution is undersmoothed,
which means that it is too bumpy. In contrast, the curve on the right
hand side is possibly oversmoothed, and you might miss some details in
the distribution.

The selection of the band with is a similar problem to the selection
of a bin width for histograms. There are some rules of thumb that can
be used to optimise that selection, but I do not have the time to
discuss this in more detail.

---

Let's move on to the second continuous dataset of 20 measurements of
Ca/Mg ratio measurements. Now, Ca/Mg ratios are strictly positive
values. Yet the left tail of the kernel density estimate shown here
extends into negative data space, implying that there is a finite
chance of observing negative Ca/Mg ratios. This is clearly
nonsense. In geophysics, positive quantities are sometimes called
Jeffries quantities, named after the British geophysicist Sir Harold
Jeffries. As mentioned before, other examples of Jeffrey's quantities
are mass volume, density, speed, etc. These parameters exists within
an infinite half space between zero and plus infinity. They all
exhibit the problem where, if you have a lot of values close to zero,
that the kernel density estimate crosses over into physically
impossible negative values.

---

Fortunately, the negative value problem can easily be solved, using a
transformation that maps the Ca/Mg ratios from strictly positive
values to the infinite space of all the numbers, including both
negative and positive values. So shown here are on the left is the
original kernel density estimate. Taking the logarithm of the values
and constructing a new kernel density estimate produces a much more
symmetric curve. Then we can take the exponents of these values and
map those results back to linear space. This produces a third kernel
density estimate that does not cross over into negative data space.

This logarithmic transformation is one example of a simple
transformation of the data solving some problems associated with
constrained data.  We will see this solution in different forms,
appearing again later in the module.

---

Jeffrey's quantities are just one example of constrained
measurements. As another example, consider the 20 vegetation coverage
measurements of the catchments data. As discussed before, vegetation
cover is a proportional quantity that takes on values between 0 and
100%. Again, the Gaussian kernel density estimate of the data plots
into physically impossible values of negative vegetation coverages, or
values that exceed 100%.

---

Using a similar approach as before, we can solve the problem of
impossible values using a data transformation that maps the data from
the constrainted space of values between 0 and 1 to the entire line of
numbers, from minus infinity to plus infinity. For proportional data
like vegetation cover, this is no achieved with a simple log
transformation, but with a so-called logistic transformation. So if
'x' is our vegetation cover, then 'u' is the logistically transformed
value, which is the logarithm of x, divided by 1-x. After constructing
the kernel density estimate of the logit of the vegetation values, we
can then map those results back to our constrainted space from 0 to 1,
using an inverse log-ratio transformation, where we take the exponents
of the logits divided by the exponents of the logits plus 1.

This solves our problem. The KDE no longer crosses over into
impossible negative values or vegetation coverage values that are
greater than one. We will see later on that this logistic
transformation is a special case of a general class of logratio
transformations that will be useful for the analysis of compositional
data, which include chemical data, mineralogical data and so
forth. But that won't be discussed until Chapter 14 of the notes.

---

Kernel density estimates can easily be generalised from 1 to 2
dimensions. For example, this slide shows a dataset of eruption
timings from the old Faithful Geyser in Yellowstone National Park in
the United States. The dataset records 272 observations shown as grey
circles of two variables. We've got the duration of each eruption on
the Y axis on the waiting times on the X axis, both expressed in
minutes. On the top and at the right, there are two marginal
distributions, which represent projections of the data onto the X and
Y axis. They show us the kernel density estimate of the waiting times
and of the durations of the eruptions, respectively. The kernel
density estimate here has been constructed in a very similar way as
the one dimensional KDE. But instead of plotting a one dimensional
bell curve on top of each measurement we've stacked a two dimensional
bell curve.

We can generalise the same idea to greater than two dimensions. But
then it becomes very difficult to visualise the result on a two
dimensional sheet of paper or a two dimensional computer screen. In
this case, there are two options. If you want to visualise higher
dimensional data, we could either plot the data as a series of one or
two dimensional marginal plots like these guys, or we can extract the
most important patterns or trends in the data by projection onto a
lower dimensional plane. Then we can show these predict projected data
as a two dimensional graphic. The second strategy is known as
ordination. We will discuss it later on in the session that is
dedicated to so called unsupervised machine learning.

Both histograms and kernel density estimates require the selection of
a smoothing parameter. For the histogram, this is the bin width,
whereas for the kernel density estimate it is the bandwidth.  This
selection of a smoothing parameter adds a certain degree of
arbitrariness to the creation of these density estimates.

---

An empirical cumulative distribution function, or ECDF, is an
alternative data visualisation device that avoids this problem and
does not require smoothing. A cumulative distribution function is a
step function that shows, on the Y axis, the fraction of the
measurements that are less than or equal to the values shown in the X
axis. For example, here we've got 40% of the pH values being less than
4.6. Empirical cumulative distribution functions do not require
binning or selecting a bandwidth because they do not require
smoothing. They do not spill over into physically impossible values,
so there are no negative Ca/Mg values. Similarly, there are no
negative values for vegetation coverage, and neither are there any
vegetation coverages exceeding 100%. Therefore, the construction of a
cumulative distribution function is completely hands off.

The visual interpretation of a cumulative distribution function is
different from that of a histogram or kernel density estimate. Whereas
different clusters of values stand out as peaks in the histogram or
KDE, they are marked by steep segments of the cumulative
distribution. For example, the geyser data have two modes, with one
cluster of eruptions that last 2 minutes, and a second cluster of
eruptions that last 4.5 minutes. These two clusters correspond to
peaks in the kernel density estimate. The same values of 2 and 4.5
minutes are marked by the steepest parts of the cumulative
distribution function. Some people find this these cumulative
distribution functions slightly more difficult to read than histograms
or kernel density estimates. And this is perhaps why histograms and
KDEs are found more frequently in the literature. Nevertheless,
cumulative distributions are extremely useful. In fact, we will see
see that they can even be used for statistical tests, to to define
some important summary statistics. But those applications will be
discussed in the next session of this module.

==================
Summary statistics
==================

After a purely qualitative inspection of the data, we can now move on
to a more quantitative description. In this session, I will introduce
a number of summary statistics to describe larger data sets, using
just a few numerical values that capture the location, the dispersion
and the shape of a probability distribution. Before proceeding with
this topic, it is useful to bear in mind that these summary statistics
have limitations.

---

The Anscombe Quartet, which was discussed in the previous session,
showed that very different looking data sets can have identical
summary statistics, including the same mean and the same variance. But
with this caveat in mind, summary statistics are an essential
component of data analysis, provided that they are preceded by a
visual inspection of the data.

---

In this session, I will introduce three types of summary statistics. A
first class of summary statistics are measures of location, which
represent the average of the data, and which quantify whether that
average is shifted towards lower values or towards high values.

A second group of summary statistics represent measures of
dispersion. These quantify the spread of the data, capturing whether
the date are concentrated near a single value, or whether they are
spread out over a wider range of values.

And finally, a third group of summary statistics quantify the shape of
the distribution; for example, whether it is leaning towards the left
or leaning towards right; whether it is skewed towards high
values or skewed towards low values.

---

There are many different ways to define the average value of a
multi-value dataset.  In this session. I will introduce three of these
ways, but later sessions will introduce a few more. The arithmetic
mean is arguably the best known and most widely used way to average
data. If we have n values x_1, x_2 and so forth to x_n, then the
arithmetic mean, which is often denoted by x-bar, is simply the sum of
the x-values divided by the number of values.

But this is just one of many ways to average data. Another,
non-parametric, approach is the median. This value is obtained by
ranking the observations according to size from the smallest value to
the largest value and selecting the middle one. The median is the
half-way point on an empirical cumulative distribution function, the
step function that I introduced in the previous session. Put in more
formal terms, the median is identical to the 50 percentile of the
distribution.

Finally, the mode is simply the most frequently occurring value. For a
continuous variable, it is the highest point on the kernel density
estimate or a histogram, or the steepest point on a cumulative
distribution function.

---

Applying these three concept to the pH data of the previous session,
the arithmetic mean is calculated, as I said, by taking to sum of
these 20 values; dividing this sum by 20 and yields a value of five.

The median is obtained by ranking the pH measurements from the most
acidic value of 3.8 to the most basic value of 6.2. If there are an
even number of values in the dataset, which is indeed the case here,
then we take the arithmetic mean of those values. In this case, the
two values are 5.0 and 5.2, so our median is 5.1.

Finally, the mode for this continuous variable is obtained by
constructing a kernel density estimate, the heighest point of which
corresponds to a pH value of 5.4. So that is the modal value that we
can use as a summary statistic.

---

Here the three different summary statistics are shown together
graphically for the pH dataset on a kernel density estimate and an
empirical cumulative distribution function. The median is again the 50
percentile of the data set. So it is the intersection between the
dash-dot line onto the empirical cumulative step function. This
intersection is marked by the dashed line, which indicates that 50% of
the pH measurements are less than 5.1, and 50% are higher than 5.1.

The arithmetic mean, which is shown as a solid white line here, and
the mode, which is this dotted line are both in reasonably close
vicinity to the median. So in this case, it doesn't really matter
which summary statistic you use. All measures of location give you
pretty much the same value.

---

However, the situation is very different for our dataset of Ca/Mg
ratios which, unlike the pH data, does not follow a symmetric but an
asymmetric distribution. Here, the mean, which is shown as a solid
line, is 3.2. The mode, which is shown by a dotted line, is only 0.45
and the median falls somewhere in between, at a value of 2.1. There is
a factor of seven difference between the smallest measure of location
and the largest measure of location. The mean in particular is
strongly affected by the long tail of large outliers towards the right
of the diagram.

Only 30% of the data are larger than the mean of the distribution, and
only one of the 20 measurements, which is only 5% of the data, is
smaller than the mode. So in this case, it is fair to say that the
choice of measure of location is very important. The median is the
probably most sensible value in this case. In contrast, the arithmetic
mean is not representative of the data.

---

Finally, this slide shows rug plots, kernel density estimates and
empirical cumulative distribution functions for the vegetation data on
the left, and the geyser eruption duration data on the right. Both of
these distributions are bimodal, meaning that they have two peaks in
the kernel density estimates and too steep segments in the cumulative
distribution function.

The highest of these peaks are marked by dotted lines, which give us
the modes but ignore the other peaks. The mean and the median fall in
between the two data clusters and are not representative of the
data.

So in conclusion, considering all four dasets, we can say that the
mean is only a meaningful measure of location for unimodal and
symmetric distributions. The arithmetic mean is more strongly affected
by outliers than the median. Therefore, the median is a more robust
estimator of location for asymmetric data sets than the mean. However,
multimodal data sets such as these two here can never be adequately
summarised with a single location parameter. And that is why it is so
important to visualise the data before you go ahead and use the
summary statistics.

---

OK, let us now move on to the second class of summary statistics,
which are measures of dispersion. It is rare for all the values in a
dataset to be exactly the same. In most cases, the values are spread
out over a finite range of values. The amount of spread can be defined
in a number of ways, the most common of which are the standard
deviation, the median absolute deviation and the interquartile
range.

The standard deviation is closely related to the arithmetic mean
expert, so given n values x_1, x_2 to x_n, the standard deviation is
obtained by taking the sum of the squared differences between each
value and the arithmetic mean. The differences are squared because,
intuitively, we want the dispersion to be a positive number.  We then
divide this sum by n-1 and take the square root. I will discuss the
factor n-1 in a later session, but at the moment you just take that as
a given.

If we omit the square root, then then this corresponds to the
variance.  So the variance is the square of the standard deviation.
Both of these are used as measured of dispersion, but the standard
deviation is more useful as a summary statistic.

The median absolute deviation uses, as the name suggests, not the
arithmetic mean but the median. It is the median of the absolute
values, which is what these vertical lines mean, of the differences
between each value and the median of the entire data set. Again, like
the standard deviation, this is a positive number.

And then, finally, the interquartile range is defined using the
empirical cumulative distribution function. Recall that the median is
defined as the value that corresponds to the intersection between a
horizontal line that goes through 0.5. Similarly, the 75 percentile is
obtained by taking the intersection of a horizontal line that goes
through 0.75 on the empirical cumulative distribution, and we can also
do the same for the 25 percentile. These three percentiles are called
the lower, middle and upper quartiles. The interquartile range is
simply the difference between the 75 and 25 percentiles.  Again, it is
a positive number.

---

Let us apply these concepts to our pH data. The standard deviation is
given by this formula. So first we list the 20 measurements in our
original unsorted order. We take the difference between each value and
the arithmetic mean, which had calculated in the previous step. That
gives us 20 numbers that can either be positive or negative. We square
these values to turn them all into positive numbers. We sum these
squared differences, giving us a value of 8.42.

Then we divide this sum by n-1, which is 19. After taking the square
root, we get a standard deviation of 0.67. The interquartile range is
calculated by first sorting our values. So these values are exactly as
the same as those, but they are ranked from the smallest (3.8) to the
highest (6.2). The 25 percentile (or lower quartile) is the arithmetic
mean of the fifth and the sixth value, which is 4.55. The 75
percentile (or upper quartile) is the arithmetic mean of the 15th and
the 16th value, which is 5.55. The interquartile range is then simply
5.55-4.55 = 1.0.

To calculate the median absolute deviation, we need to take the
difference between each of the values and the median, which we already
calculated before. That gives us values that there are either positive
or negative. We then take the absolute value of these values, turning
them all into positive numbers. We sort these values from small to
large and take their median, which is 0.5. So our median absolute
deviation is 0.5.

---

And finally, a third group of summary statistics that I would like to
discuss today involves measures of shape for probability
distributions.  Like the measures of location and dispersion that I
talked about previously, also the shape of a distribution can be
quantified in a number of different ways. I won't go into much detail
about this, but just introduce you to one measure of shape, which is
the skewness.

The definition of skewness looks very similar to the definition of the
variance. Like the variance, also the skewness is a sum of differences
between the values and their arithmetic mean. But whereas the variance
raises these differences to the second power, the skewness raises them
to the third power. The variance and skewness are both known as
moments of the distribution.

The variance is a second moment, whereas the skewness is the third
moment. You can also define higher order moments such as, for example,
the kurtosis, which is proportional to the fourth power of these
differences. I won't discuss the kurtosis and much detail, but in
principle you can describe the entire distribution by simply listing
all the moments. However this is a more advanced subject.

Here, at the bottom, are three examples, three datasets that have
different skewness. The skewness of the pH data is close to zero
because this is a symmetric distribution. The Ca/Mg data lean heavily
towards the left and have a long tail towards high values. This data
set is positively skewed. And then here on the right is a dataset that
I downloaded from the web. It gives us the Covid-19 mortality rate in
the United Kingdom during March of 2020. This distribution is
negatively skewed, where highest number of deaths per 100,000 people
was among the older population. But there is a long tail towards
younger people as well.

---

The most important summary statistics can be jointly visualised in a
compact way on a so-called box-and-whisker plot. As the name suggests,
a Box and whisker plot consists of a box, which is drawn from the
first to the third quartile, so from the 25 percentile to the 75
percentile of a dataset. The median is marked by a line in the middle
of that box, and then two lines or whiskers extend from this box
outwards towards the minimum value and the maximum value of the
dataset, ignoring outliers, where outlines are defined as all those
points that fall more than 1.5 times the interquartile range below the
first quartile, or more than 1.5 times the interquartile range above
the third quarter.

In this particular example, there is no lower outlier, but we do have
one outlier at the high end of our dataset. The median is offset
towards the left hand side of the box, indicating the positive
skewness of this particular dataset. And we can plot these Box and
whisker plots either horizontally, as on this slide, or alternatively,
you also can put them vertically. Box plots can be used to visualise a
single sample or multiple samples. In the latter case, box plots allow
easy inspection, in just one glance, of the variability in location,
spread and shape of large numbers of samples. We will actually do this
in one of our exercises.

===========
Probability
===========

Probability is a fundamental concept in mathematical statistics, that
pops up in various contexts, as a numerical description of how likely
it is for an event to occur, or how likely it is for a proposition to
be true.

---

In the context of a sampling experiment in which all the outcomes are
equally likely, the probability of an outcome A can be defined as the
ratio of the number of ways in which A can occur, divided by the total
number of possible outcomes.

---

For example, the probability of tossing an unbiased coin and observing
head is the ratio of the number of outcomes of head which, in the case
of a single coin, is one divided by the total number of outcomes,
which is either head or a tail. So that probability is 1 divided by 2
or one half.

---

Next, let's calculate the probability of tossing the same unbiased
coin three times and observing 2 x head and 1 x tail. This probability
is a total number of cases in which we've got one T and two H's, which
happens in three scenarios, divided by the total number of outcomes,
which is eight. So the probability of two heads and one tail is
3/8ths.

---

Similarly, the probability of throwing two dice and obtaining one 2
and a one 6 is the total number of ways to get a 2 and a 6, which is
either first a 2 and then a 6 or first a 6 and then a 2, divided by
the 6 x 6 = 36 possible outcomes of the dice throwing experiment. So
we get 2 divided by 36, which equals 1 divided by 18, and the
probability of this outcome is 1/18th.

---

The multiplicative rule of probability dictates that the probability
of two combined INDEPENDENT experiments is given by the product of
their respective probabilities. Independent means that the outcome of
the first experiment does not affect the outcome of the second.  We
will discuss situations where this is not the case at the end of this
session.

If we carry out two coin tossing experiments, then the probability of
obtaining a head in the first experiment AND a head in the second
experiment is simply one half times one half, which is one
quarter. Similarly, when we carry out a coin tossing experiment AND a
die throwing experiment, then the probability of obtaining two heads
and one tail for the first experiments, and throwing a two and a six
in the second experiment is the product of 3/8ths and 1/18th, which is
3/144ths, or about 2.1%.

The additive rule of probability dictates that the probability of
observing either of two mutually exclusive outcomes is given by the
sum of their respective probabilities. So if you toss three coins,
then the probability of obtaining two heads and one tail OR of
obtaining three heads is the sum of 3/8ths and 1/8th, which is 4/8ths
or 50%.

---

However, if the two outcomes that you are comparing are not mutually
exclusive, then the additive rule needs to be modified to account for
the overlap between the two outcomes.

For example, if we combine the probability of success in a coin
tossing experiments where success is defined as two heads and a tail,
with the probability of success in a dice throwing experiment where
success is defined as obtaining a two and a six, then the probability
of success in the first OR the second experiments equals the sum of
the probability of success in the first experiment, which includes all
possible outcomes for the second experiment, PLUS the probability of
success in the second experiment, which includes all outcomes of the
first experiments MINUS the probability of success in both
experiments, which would otherwise be double-counted. And that gives
us a probability of 0.41.

---

Let us now move on to an important subject in statistics that will
make your brain hurt. This subject is called combinatorics. A
permutations is a first concept in combinatorics that refers to an
ordered arrangement of objects. These objects can be selected either
by sampling with replacement, or by sampling without
replacement. Let's illustrate the difference between these two
concepts. Consider one urn with 20 balls that are numbered from 1 to
20. Suppose that we draw 10 balls from this urn. Then there are two
ways for doing so.

---

A first method is to sample the balls with replacement. So we draw a
first ball from the urn. There are 20 possible ways for doing so. And
we write down its number. Suppose that this is 5. We then put the ball
back into the urn. We thoroughly mix the balls and draw a second
number. Again, there are 20 ways for doing so. Suppose that the second
outcome is two. The total number of possible ways in which we can
select or draw two balls from the urn with replacement is 20 times 20,
which is 400.

We could then collect a 3rd ball, a 4th ball, and so forth until we
have collected 10 in total. Then the total number of possible outcomes
for this experiment is 20 times 20 times 20 etc. So we've got 20 to
the tenth power. This is a very large number of outcomes indeed
because we replaced the balls after writing down the numbers.

---

There is a possibility that we've got duplicate values in our
collection of 10 numbers. In this example, here we've got two
appearances of the number 5 and two appearances of the number 19.

---

This duplication of values does not occur under an alternative
experimental design, in which the sampling is not done with but
without replacement. So consider the same urn with 20 balls as before,
and draw the first number like before. There are 20 possible ways to
do so.

Suppose that the number on the first ball is 15. This time we do not
put ball number 15 back into the urn. With the first ball removed, we
now draw a second number. This time there are not 20, but only 19
possible ways for doing so. Suppose that the second number is 8. The
total number of ways in which we can collect two balls out of 20 is
not 20 times 20, but 20 times 19. We can repeat this experiment for
the third or fourth balls from the urn, which is progressively
dwindling in size, and the total number of ways to collect 10 balls
out of 20 from the original urn is not 20 to the tenth power, but 20
times 19 times 18, etc. to 11.

This can be written as 20!/10!, where ! means n times n-1 times n-2,
etc., and is denoted by the exclamation mark. 20!/10! is a smaller
number that 20^10.

---

OK, let's now apply these two formulas for sampling with and without
replacements to a classical problem in statistics. Suppose that we
have a classroom with k students, then what is the probability that at
least two of these students celebrate their birthdays on the same day?

---

To calculate this probability, it is useful to first compute the
probability that none of the birthdays overlap and that everybody's
got a unique birthday. To that end, we can see that there are 365^k
possible birthdays, including overlapping birthdays. So this is
sampling with replacement.

And of these 365^k possible combination of birthdays, 365!/(365-k)!
birthdays do not overlap which is the formula for sampling without
replacement.

---

Then the probability that no birthdays overlap is simply the ratio of
this number of outcomes divided by this number of outcomes and that
produces 365!/[(365 - k)!365^k]. The probability that at least one
pair of students have overlapping birthday is then the complement of
this probability, so one minus this number.

---

When we plot this probability against the size of the class, then we
can see that initially the probability of overlapping birthdays is
small. If there are only two students in our class, the probability
that they celebrate their birthdays on the same day is 1/365. But this
probability increases rapidly so that by the time you reach a
classroom size of 23, there is a greater than 50% chance that at least
two students in that class will celebrate their birthdays on the same
day.

---

Having considered coins, dice and lottery balls, let us now
(literally) deal with a fourth archetypal source of statistical
experiments, namely playing cards.

The formula for sampling with replacement tells us that there are
52!/49! or 132,600 unique possible ways to select three cards from a
deck. Suppose that we have drawn an ace of spades, a 6 of diamonds and
a king of hearts.

Then there are 3! = 6 ways to order these cards.

Suppose that we don't care in which order the cards appear. How many
different three card hands are possible?

---

Well, it is easy to see that the number of ordered samples equals the
product of the number of unordered samples times the number of ways to
order those samples.

Rearranging this equation for the number of unordered samples gives us
the ratio of the possible number of ordered samples to the number of
ways to order those samples.

Both of these numbers can be calculated using the formula for sampling
without replacement. As we have seen before, the number of ways to
select k objects from a collection of n is n!/(n-k)!, and the number
of ways to shuffle those objects around is k!.

The ratio of those numbers is also known as the binomial coefficient,
which is pronounced as "n-choose-k".

---

Applying this formula to the card dealing example, the number of ways
to draw three cards from a deck of 52 is 52-choose-3, or 22,100.

---

So far, we have assumed that all coin tosses or throws of a dice were
done independently, so that the outcome of one experiment did not
affect that of the other. However, this is not always the case in
geology. Sometimes one event depends on another. We can capture this
phenomenon with this definition.

P of A vertical bar B stands for the conditional probability of A
given B. To illustrate this concept, suppose that A is ammonites and B
is Bajocian, which is a particular stage of the Jurassic in which
Ammonites are found. Then P of A bar B stands for the probability of
finding an Ammonite in a Bajocian deposit.

---

The multiplication law dictates that the probability of outcomes A and
B both occurring simultaneously equals the probability of A given B
times a probability of B or, equivalently, the probability of B given
A times the probability of A, which equals the probability of B and A
both occurring.

So for example, suppose that 70% of a field area is covered by
Bajocian ocean deposits, and that 20% of the Bajocian deposits contain
ammonites. Then the probability of finding a Bajocian
Ammonite is 0.7 times 0.2, which is 14%.

---

The law of total probability prescribes that, if we have any mutually
exclusive outcomes B_1, B_2, etc., the the total probability of A
is the probability of A given B_1 times the probability of B_1,
plus the probability of A given B_2 times the probability of B_2,
etc. for all B_i.

---

So, if we have a river catchment that contains 70% Bajocian deposits
and 30% Bathonian deposits, where Bajocian is denoted as B_1 and
Bathonian as B_2, and we have a 20% chance of observing ammonites and
the Bajocian and a much greater 50% chance of observing ammonites in a
Bathonian deposit, then we can use the law of total probability to
calculate the probability of observing ammonites in our catchment.

It is simply the sum of the products of the probability of ammonites
in the Bajocian times the relative area covered by Bajocian deposits,
plus the probability of finding ammonites in the Bathonian times the
relative area covered by Bathonian deposits, which is 0.2 times 0.7 +
0.5 times 0.3.  In other words, there is a 29% chance of finding
ammonites in our field area.

---

The formula for the multiplication law can easily be rearranged to
form a famous formula in statistics called "Bayes' Rule", which allows
us to calculate the probability of B given A from the probability of A
given B and the total probabilities of A and B.

---

If there are multiple possible outcomes for B, then we can rephrase
Bayes' Rule to allow us to calculate the probability of any of these
subgroups of B given A from the probabilities of A given the different
subgroups of B, using the law of total probability. So we simply plug the
law of total probability into Bayes' rule and we get this generalised
form of Bayes rule for multiple outcomes of B.

---

These equations all look a little bit abstract, perhaps, but hopefully
they will be a little clearer if we apply Bayes rule to a specific
problem. Consider the same catchment as before, with 70% Bajocian
deposits and 30% Bathonian deposits. Suppose that there is a river in
this catchment, and that we have found an ammonite fossil in the
riverbed. Is this fossil likely to be Bajocian or Bathonian?

Well, to calculate the probability that it is Bajocian, we take the
ratio of the probability of finding an ammonite in the Bajocian times
the fraction of the field area covered by Bajocian deposits, divided
by the sume of the products of the probability of observing an
ammonite in the Bajocian times the fraction of the area covered by
Bajocian, plus the probability of observing an ammonite in the
Bathonian times the fraction of the field area covered by Bathonian
deposits. Plugging in the numbers yields a 48% chance that our
ammonites is of Bajocian age and a 52% chance that it is of Bathonian
age.

============
Binomial (1)
============

In this session, we will discuss various properties and inferences
made for the binomial distribution, which is the distribution that
describes so-called Bernoulli variables.

---

A Bernoulli variable is a quantity that takes on only two values,
either zero or one. Examples of such variables are coin tosses, which
could either result in head or tail; or a die, which can land on a six
or on a different value. And in the geological context, oil wells
could either yield petroleum or be dry.

---

As a more detailed example, let's consider five gold diggers during
the 1849 California gold rush, who have each purchased a claim in the
Sierra Nevada foothills. Suppose that geological evidence suggests
that on average two thirds of the claims in the area should contain
gold and the remaining third do not. So the number of successful gold
claims is a Bernoulli variable.

---

The presence of gold in any given claim could be denoted by one, and
the absence of gold could be denoted by zero. The probability that
none of the five prospectors find gold then equals the probability
that the first prospector did not find gold, which is one third times
the probability that the second prospector did not find gold, which is
also one third time. Another third for the third prospector and for
the fourth and the fifth prospector, so that the probability is one
third to the fifth power, or 0.41 percent.

The chance that exactly one of the prospectors finds gold equals the
probability that the first prospector strikes gold and the other four
do not, plus the probability that the second prospective finds gold
and the other four don't, plus the probability that the third, the
fourth or the fifth. The probability that the first prospector finds
gold and the remaining four don't equals two thirds times one third to
the fourth power, which is 0.82 percent.

Similarly, the probability that the second prospective finds gold and
the first, third, fourth and fifth do not equals one third for the
first prospector times, two thirds for the second prospector times,
one third cubed for prospectors three, four and five, which again is
0.82 percent and again for prospector three, four and five. The
probabilities of one of these striking gold and the other four to not
strike gold equals 0.82 percent.

So the probability that exactly one of the four prospectors find gold
is the sum of these probabilities, which is five-choose-one. The
binomial coefficient times two thirds, which is the probability of any
one of the prospectors finding gold finds one third to the fourth
power, which is the product of the probabilities of the other four not
finding gold, which is five times 0.82 percent. And that gives us a
total probability of 4.1 percent for one of the prospectors to find
gold and the other four not to find gold.

---

Using exactly the same logic, we can also calculate the probability
that exactly two of the five prospectors find gold. This is then five
choose two times two thirds squared times. One third cubed is 16
percent.

The probability that three of the five gold diggers are lucky is five
to three times two thirds cubed times. One third squared is 33
percent. The probability of four out of five gold diggers to find gold
is five chews, four times two thirds to the fourth times one third is
again 33 percent.

And the probability that all five prospectors find gold. If the true
occurrence of gold in the area is two thirds is two thirds to the
fifth power, it's 13 percent.

---

The generic function for the binomial distribution is shown on the
left here, the probability of key successes amongst and trials, if the
probability of success is p is given by the binomial coefficient
n-choose-k times the probability of success raised to the power
times. The complement of that probability one minus p to the minus
case power.

And we can visualise this these probabilities on the bar chart or a
probability mass function where on the horizontal axis we've got the
number of successful claims amongst the total of and equals five in
our case claims. And on the y axis, these probabilities are
labelled.

These probabilities are again for zero successes, zero point zero for
one percent for one successful claim. Amongst five, it is 4.1
percent. Then we've got 16 percent, 33 percent, 33 percent and 13
percent. The sum of all these probabilities, of course, has to be
one.

---

Equivalently, the results can also be visualised as a cumulative
distribution function. Or CDF, this function represents the running
some of the probability mass function. The horizontal axis is again
labelled with a number of claims that produce gold. The vertical axis
shows the cumulative probability of these respective outcomes. For
example, the probability that two or fewer prospectors find gold is 21
percent. That is the sum of the probability of zero successful claims,
so zero point zero for one, plus the probability of one successful
claim for one percent, plus the probability of two successful claims,
which was 60 percent to some of those values, is 21 percent. And that
is the number that we can read off the y axis of the CDF.

---

So far, we have assumed that the probability of success is lowercase p
here is known in the real world. However, this is rarely the case. In
fact, p is usually the parameter whose value we want to determine
based on some data. So consider the general case of K successes
amongst and trials. Given this probability of p, if we rephrase this
equation, this probability mass function not ask given and p, but as p
given n and k, then the formula range remains exactly the same. But
we now call this not a probability mass function, but a likelihood
function.

So if we know N and K, then we can estimate P by maximising this
likelihood function. The notes show how you could do this by taking
derivative, setting those derivatives to zero and rearranging. I won't
go through this calculation in this video, but I will just cut to the
chase and give you the solution. Given k successes out of n trials,
our best estimates for the probability of success is simply K divided
by N.

This is a pretty trivial result, but you can derive it using this
so-called method of maximum likelihood. If we have to suppose that we
have two successes amongst the five gold clients, then our best
estimate for the probability of finding gold and any given claim is
two fifths or 40 percent. So in this case, the method of maximum
likelihood is pretty trivial. But this is not the case in all
probability distribution and this approach of maximising the
likelihood really underpins much of mathematical statistics.

---

So let us continue with this example of two successful outcomes
amongst five trials. This again, leads to a maximum likelihood
estimate for the parameter p of two fifths of 40 percent. Now, recall
that earlier I said that the geological estimate for the occurrence of
gold in the area was two thirds, which is sixty seven percent. That is
quite a bit higher than the observed 40 percent.

So we might ask ourselves the question if this discrepancy between the
predicted and the observed number of successes can be attributed to
bad luck, or if it means that the geological estimates were wrong and
that the true probability of finding success might actually be less
than the hypothesised probability of two thirds.

To answer this question, we perform a so-called hypothesis test, which
follows a sequence of six or seven steps. The first of these steps is
the formulation of two formalised hypotheses. A null hypothesis, which
is in our case is P equals two thirds of the alternative hypothesis,
which is that P is less than two thirds.

---

The second step in the hypothesis testing procedure is the calculation
of a test statistic. In our case, this will simply be the number of
successes. So for us, that is an outcome of two successes amongst our
five tries. The third step is to calculate the null distribution of
this test statistic under the null hypothesis in which p the
probability of success is two thirds.

This null distribution can be tabulated either as the probability mass
function or as the cumulative distribution function. So these numbers
represent the values in this histogram or in this step function. Then
the probability of observing less than or equal to the observed number
of successes, in our case, about 21 percent is also known as the P
value.

The fourth step in the hypothesis testing procedure is the selection
of a significance level alpha. I will discuss the implications of
changing alpha later in the session, but suffice it to say here that
we will always use five percent in this module. Then we go back to our
table of probability, mass and cumulative distribution functions, and
we select all the outcomes for which the cumulative distribution
function is less than the significance level alpha.

There are two values in this table or in this row of the table that
are less than 0.05. They correspond to outcomes of zero successes and
of one success. These two outcomes are then grouped in a rejection
region. The second step is checking whether our observed outcome of
two successes falls in this rejection region or not. Our outcome is to
the rejection region does not include the value to, and therefore we
are unable to reject the null hypothesis. This does not mean that we
have accepted the null hypothesis. It just means that we don't have
enough evidence to reject the null hypothesis. I'll say a few more
words about that subtle distinction later in the session.

Now, alternatively, and equivalently, we can also have a look at the P
value, which we saw earlier is about twenty one percent. So 0.2 to one
is greater than the significance level alpha. And therefore, again, we
are unable to reject the null hypothesis.

---

The same procedure can also be visualised graphically on a probability
mass function on the left and a cumulative distribution function on
the right. For a binomial distribution with a parameter P equals two
thirds, so that's a no hypotheses and an equals five trials. The
observed outcome of k=2 successful claims is shown as a vertical
dashed line on both the probability mass function and the cumulative
distribution function and the rejection region of our null hypothesis
is shown as these two white bars are the probability mass
function. Because the horizontal dashed line falls outside this
rejection region, we are unable to reject the null hypothesis.

Equivalently, we can see that the significance level alpha
five percent is shown as a horizontal dotted line, but this horizontal
dotted line plots beneath the P value, which is shown as a horizontal
dashed line. The P value is twenty one percent. It is the horizontal
line that intersects the cumulative distribution function at the same
point where our observed outcome of two successes has intersected that
cumulative distribution function. Again, we are unable to reject the
null hypothesis.

So the outcome of the hypothesised probability of success of two
thirds remains plausible within in the light of this observed two out
of five successes.

---

The hypothesis test that we have just carried out is called a one
sided hypothesis test. Because the null hypothesis and the alternative
hypothesis are asymmetric, we are only considering as an alternative
outcome. Those probabilities of success that are less than those of
the null hypothesis. Alternatively, we can also formulate a two sided
hypothesis test in which the null and the alternative hypotheses are
symmetric. So here are null hypothesis remains p equals two
thirds.

But now we consider not only alternative outcomes that are less than
the null hypothesis, but also values for P that could potentially be
greater than two thirds. The procedure for a two sided hypothesis test
is only slightly different from that of a one sided hypothesis
test.

---

Again, we need to calculate a test statistic, which will be the same
as before. It's a number of successful, claims Kate. The third step is
also the same. We need to calculate the null distribution of this test
statistic under the null hypothesis, and this can be tabulated as a
probability mass function and as a cumulative distribution function
like before. But now it is also useful to add not only the lower tail
of the cumulative distribution of the note distribution, but also the
upper tail.

Because we are doing a two sided hypothesis test, we also want to
evaluate the probability of an outcome exceeding the observed outcome
of two successful claims under the null hypothesis. For example, the
probability of observing zero or more successes if the true
probability of success is two thirds is 100 percent, the probability
of having one or more successes is ninety nine point six percent and
the probability of having five or more successes, which is the same as
the probability of having five successes.

---

If the true probability is two thirds, or 13 percent, the significance
level is kept the same at five percent. But we now evaluate this
significance twice at alpha, divided by two. So about two and a half
percent to accommodate both tails of the binomial distribution, both
the low tail and the high tail. We then mark all the outcomes that are
incompatible with the null hypothesis. So all the values for the
cumulative distribution function and for the upper tail of the
distribution that are less than alpha divide it by two that are less
than two and a half percent. There is only one value for which this is
the case, and that is the outcome of zero successes. So there are no
outcomes in the upper tail that are less than two and a half
percent.

So our rejection region only contains one outcome zero successes or
rejection region has a smaller than it was for the one sided
hypothesis test. Again, the observed outcome of two successful claims
does not fall in this rejection region, and we are therefore unable to
reject the null hypothesis. This means that we cannot rule out the
possibility that the true value of the parameter P is indeed two
thirds and that, in other words, the geologists assessment of the
likelihood of finding gold was in fact correct.

---

Displaying the two sided hypothesis test graphically shows again on
the left the probability mass function and on the right, the
cumulative distribution function that was previously used for our one
sided test. The vertical dashed line is again the observed outcome of
a two successive successes for both the probability mass function and
the cumulative distribution function.

The rejection region is smaller than before. Only the bar for zero is
coloured white hair. So again, the observed outcome does not fall in
this rejection region, and we are therefore unable to reject the null
hypothesis on the accumulative distribution function.

We have not one but two horizontal dotted lines marking the two and a
half and ninety seven and a half percent levels of the cumulative
distribution function. These two horizontal lines intersect the
accumulative distribution at values of a one and five successes,
respectively. So the rejection region consists of all the outcomes
that are less than one or greater than five. Of course, values of
greater than five are impossible.

The observed outcome is shown by a vertical dashed line, which plots
in between these two boundaries of the rejection region. So again, we
are unable to reject the null hypothesis. Equivalently, the cumulative
probability of the observed outcome is shown as a horizontal dashed
line on the cumulative distribution. This value, which is 21 percent,
is greater than 2.5 percent and less than ninety seven point five
percent, and this falls within the acceptable range.

So again, we are unable to reject the null hypothesis. In this case,
the one sided and two sided hypothesis tests produce exactly the same
result. However, this certainly is not always the case, as we will see
in the next session of this module.

============
Binomial (2)
============

In this second session on the binomial distribution, I will formally
define the concept of statistical power, and I will show that the
power of statistical tests to reject false null hypotheses increases
with increasing sample size.

---

Let us continue with the same gold digging example of the previous
session considering, again our outcome of two successful claims out of
five total claims again gives rise to a maximum likelihood estimates
for the parameter p of two fifths or 40 percent. And even though this
is less than the hypothesised sixty seven percent, the difference is
not statistically significant. But what happens if we do not have five
but 15 gold prospectors purchasing a claim in the same area?

Suppose that six of these prospectors have struck gold, then the
maximum likelihood estimate for our parameter p is 6/15, which is
again 40 percent same value as before. Then, the one sided hypothesis
tests of contrasting p equals two thirds with p as less than two
thirds procedes us before, but it leads to a different table of
probabilities instead of values ranging from zero to five, this table
now tabulates values from 0 to 15.

Again, both the probability mass function and the cumulative
distribution function, the rejection region groups all the outcomes
for which the cumulative distribution function is less than zero point
zero five that groups all the samples are all the outcomes from zero
to six. Our observed outcome of six successful claims falls and this
rejection region, and therefore we can reject the null hypothesis
equivalently.

The p-value is the probability of observing six or less successes
under the null hypothesis. This probability is only three percent. The
P value is less than the alpha significance level cut-off. Again, we
reject the null hypothesis.

---

Visualising the results of this one sided tests graphically again on
the left is the probability mass function of a binomial distribution
with a parameter p=2/3 and n=15 experiments.

On the right is again the cumulative density function, which is the
running sum of that bar chart. The vertical dashed lines on both of
these panels are the observed outcome of six successful claims amongst
15 claims the rejection region as shown in white.

This time, our observed outcome plots inside the rejection region, so
we reject a null hypothesis on the right hand side. We see the p value
shown as a horizontal dashed line. Again, this is the intersection of
our observed outcome with the cumulative distribution function.

This p value is less than the alpha cut-off than the five percent mark
shown by this horizontal dotted line, and the boundary of the cut-off
of the rejection region is shown as the vertical dotted line. Our
observed outcome plots towards the left of that, so inside our
rejection region again leading to the rejection of our null
hypothesis.

---

Moving on to the two sided hypothesis test, which compares the null
hypothesis that P equals two thirds to the symmetric alternative
hypothesis that P does not equal two thirds, our table of
probabilities again includes not only the cumulative distribution
function, but also the probabilities of outcomes exceeding the
observed value. The rejection region groups all the values of the CDF
that are less than two and a half percent. So these are outcomes zero
through five, plus the outcomes for which the CDF exceeds ninety seven
point five percent. So that's outcomes 14 and 15.

This tells us that had we observed 14 successful claims out of 15
total claims, then that would have been incompatible with this null
hypothesis of P equalling exactly two thirds. So outcomes zero through
five plus outcomes 14 and 15 together form the rejection region, and
our observed six out of 15 successful claims does not belong to this
rejection region. We are therefore unable to reject the two sided null
hypothesis.

---

Again, summarising the hypothesis testing graphically, we've got the
same probability mass function as before and the same cumulative
distribution function. However, now we are evaluating a two sided
hypothesis test, so our rejection region includes both a lower tail
and an upper. Our observed outcome of six successful claims does not
plots inside these white bars, and therefore we are unable to reject
the null hypothesis.

In the cumulative distribution function, we've got two horizontal
dotted lines marking the two point five percentile and the ninety
seven point five percent lines, our one sided P-value is shown again
by the horizontal dashed line. This value is higher than the two and a
half percent cut-off value, and therefore we are unable again to
reject the null hypothesis. Also, the rejection region shown on the
cumulative distribution function, the boundary, the upper boundary is
13.

The lower boundary for our rejection region is six, which equals our
observed value. We are therefore again unable to reject the null
hypothesis. So unlike the smaller experiment with five total claims in
which the one sided hypothesis test and the two sided hypothesis test
both resulted in the same outcome, namely the failure to reject the
null hypothesis.

When we increase the sample size from five to 15, then actually the
two hypotheses test leads to different results. The one sided
hypothesis test led to the rejection of the null hypothesis, and the
two sided hypothesis test did not allow us to reject the null
hypothesis.

---

Now what happens when we increase the sample size, even more so
instead of 15 claims? We now evaluate 30 claims and we've got 12
prospectors that strike gold, then the maximum likelihood estimate for
the parameter B is 12. Divided by 30 is again 40 percent, showing the
outcome of the one sided hypothesis test for P equals two thirds on
the left and of the two sided hypothesis test on the right shows that
in both cases, the observed outcome of 12 successes out of three
trials plots inside the rejection region of the one sided hypothesis
test and of the two sided hypothesis test. So in both cases, we reject
the null hypothesis.

In the cumulative distribution functions we've got again, the P values
shown as horizontal dashed lines. In both cases, they plot below the
alpha=5% mark have shown as a horizontal dotted line, but also below
the alpha divided by two cut-off shown on the lower right corner of
our diagram. The rejection region for the one sided hypothesis test is
everything to the left of the dotted line. Our outcome plots in that
rejection region.

For the two sided hypothesis test, the rejection region is everything
that plots towards the left of the first dotted line and towards the
right of the second dotted line. Our outcome plots in this rejection
region again leading to the rejection of the null hypothesis.

So we can see that as we move from small to large sample sizes, both
tests are unable to reject the null hypothesis for very small data
sets of just five claims. If we increase the number of perspectives
from five to 15, then we can reject the one sided hypothesis test. But
we fail to reject the null hypothesis in the two sided case. And if we
increase the sample size even more to 30 claims whilst still keeping
the percentage of success the same, we can firmly reject both the one
sided and the two sided null hypothesis. In statistical terms, the
increase in sample size has increased the power of the test to reject
the null hypothesis.

--

There are four possible outcomes for a hypothesis test, which can be
organised in a two by two contingency table. In this table, there are
two ways to make a correct decision. If the null hypothesis is false
and we reject it, then we have made the correct decision. If the null
hypothesis is true and we do not reject it, again we've made the
correct decision.

But there are also two ways to make an incorrect decision. If we
reject a true null hypothesis, then we have committed a so-called
type-I error. For the gold prospecting example, this is equivalent to
rejecting the expert opinion of the geologist, whose assessments
indicated a 2/3 chance of finding gold when that geologist is in fact correct.

A type-II error occurs when the null hypothesis is false, but we fail
to reject it. In the geological example, this means that we trust the
geological assessment despite it being wrong.

---

To appreciate the difference between the type-I and type-II errors, it
is useful to compare statistical hypothesis testing with a legal
analogue. The jury in a court of law faces a situation that is similar
in some ways to that of statistical hypothesis testing. They are faced
with a person who has either committed a crime or not, and they must
decide whether to convict this person or to acquit them.

In this case, our null hypothesis is that the accused is innocent. The
jury then needs to decide whether there is enough evidence to reject
this hypothesis in favour of the alternative hypothesis, which is that
the accused is guilty.

We can cast this process again in a two by two table. We reached a
correct decision if the accused is guilty and ends up being convicted,
or if the accused is innocent and we acquit them. But there are also
two ways to make an incorrect decision.

A type-I error is committed when an innocent person is put in prison
and the type-II error is committed when we let a guilty person get
away with a crime because we do not have enough evidence.

---

The probability of committing a type-I error depends on a single
parameter, the confidence level alpha, which is nearly always taken to
be five percent in a geological context. So using this value, there is
a five percent chance of committing a type one error. Even if the null
hypothesis is correct, then we would still expect to reject this
correct null hypothesis once every 20 times.

This may be acceptable in geological studies, but probably not in the
legal system. The principle that guilt must be proven beyond any
reasonable doubt is similar to choosing a very small significance
level of alpha, much less than 0.05. However, it is never possible to
enforce alpha equals zero. So it is inevitable that some innocent
people are sentenced every once in a while.

The probability of committing a type two error, which is referred to
as beta, depends not on one, but on two factors. First, it depends on
the degree to which the null hypothesis is false. The more wrong our
null hypothesis is, the easier it is to reject it, and the smaller the
probability of committing a type two error.

The second factor, which we've already briefly touched upon, is sample
size. With increasing sample size, it becomes easier to reject wrong,
null hypotheses, and again, beta decreases.

Now, this probability of a type two error is closely related to the
formal mathematical definition of statistical power. Data simply
equals one minus power, and conversely, power equals one minus
theta. So with increasing degree of falseness of the null hypothesis,
the power of the statistical test to reject the hypothesis increases
and with increasing sample size again, the power to reject the null
hypothesis increases. In the next two slides, I will now further
explore these two points in the context of our geological gold
prospecting example.

---

Let us first explore the effect of the degree to which the null
hypothesis is false. Recall that our null hypothesis was that the
probability of finding gold in the area was two thirds, so sixty seven
percent. Under this null hypothesis, we can predict the probability
distribution of successful claims and that is shown here as a
probability mass function and a cumulative distribution function as a
grey bar chart and great step function for each of these three sets of
panels.

Suppose that in reality, the actual occurrence of gold in the field
area is not 67 percent, but a different value, and let's explore that
for three different scenarios the first in which there is 40 percent
gold in the area, the second in which there is 20 percent gold, and
the third scenario in which the field area does not contain any gold
at all.

Under these three different alternative hypotheses, we can calculate
the probability mass function and the cumulative distribution function
again. Now these alternative distributions will be shifted towards the
left and offset relative to the null distribution and the degree of
separation between the alternative distribution and the null
distribution varies according to the alternative hypothesis.

It is the smallest when the the alternative hypothesis is relatively
close to the null hypothesis and then with increasing disagreements
between the null and the alternative hypothesis, these two probability
distributions get separated until the point where our alternative
hypothesis stipulates that there is no gold in the area. There is a
clear separation between the alternative hypothesis and the null
distribution.

The rejection region for the null hypothesis that there is 67 percent
gold in the area is defined as or groups all the outcomes for which
the cumulative probability is less than our alpha cut-off, so less
than five percent, which is shown here as a horizontal dotted line. So
this rejection region is everything that falls to the left of this
vertical dotted line, which is the intersection between our cumulative
distribution function and our five percent cut-off. Therefore the
rejection region includes the value zero and the value one.

We coloured this rejection region as whites on the probability mass
function, and we observe that the sum of these two bars do not amount
to five percent, but in cover a larger proportion of the alternative
distribution. So when 34 percent of the outcomes of the alternative
distribution, we would reject the null hypothesis. This probability of
34 percent, the area covered by these white bars is an estimate for
the power of our statistical test one minus the power. So sixty six
percent is the probability of incurring a type two error. Therefore,
the probability of failing to reject this false null hypothesis is 66
percent.

Let us now consider a second alternative hypothesis. Instead of sixty
seven percent occurrence of gold in the area under the null hypothesis
or 40 percent occurrence of gold on the the first alternative
hypothesis, let's now consider the situation in which there is only 20
percent gold in the area, so a much smaller amount. This alternative
hypothesis is more different from the null hypothesis than the
previous alternative hypothesis that we considered and consequently
the alternative distribution, both in its probability mass function
and in its cumulative distribution function are more different of
separated by a greater distance from the cumulative distribution of
the null hypothesis than the previous alternative hypothesis did.

The rejection region remains the same, so it includes the values zero
and wanted the same because we still have the same null
distribution. But now the rejection region, or the proportion of the
alternative hypothesis or alternative distribution that is covered by
the rejection region increases from 34 percent to a whopping 74
percent. So the power of our hypothesis test to reject this false null
hypothesis has doubled to 74 percent, and therefore the probability of
committing a tatu error has dropped from 66 percent to only 26
percent.

Finally, let us now consider an end member scenario in which the
prospecting area does not contain any gold at all. So P is zero, then
the probability of finding gold is obviously zero. Under this trivial
scenario, the entire alternative distribution falls in the rejection
region of our null hypothesis. Therefore, the power of the test is 100
percent, and the probability of committing a type two error is
zero.

---

The second factor that affects the power of a statistical test and the
probability of incurring a type two error is sample size. We had
actually already explored this effect when I was comparing the
outcomes of a one sided and a two sided binomial hypothesis test for
different numbers of claims from five to 15 and 30.

Recall that when I had only five claims, neither the one sided nor the
two sided hypothesis tests were rejected. When the number of claims
was increased to fifteen, we were able to reject the one sided
hypothesis test, but we were not able to reject the two sided
hypothesis test. This tells us that's a one sided hypothesis test is
more powerful than a two sided hypothesis test.

And then finally, when we increased sample size to 30, when we
evaluated 30 claims, then, both the one sided and the two sided test
were rejected. This procedure is illustrated graphically here as
probability mass functions and as cumulative distribution
functions. The null distributions for the null hypothesis of equalling
two thirds are again shown in grey and the alternative distributions,
which are all the same in this case, the alternative distribution
being that the true occurrence of gold in the field area is 40
percent. These are also shown as black and white probability mass
functions and as a white cumulative distribution function.

With increasing sample size from five to 15 to 30, the null
distribution and the alternative distribution both become more
detailed and narrower so that the degree of overlap between these two
distributions gets smaller and smaller and the separation between the
no and the alternative distribution becomes clearer, making it easier
to reject the null hypothesis.

The rejection region for the small sample of five claims again was
zero and one. The area covered under the probability mass function or
not by this horizontal dashed line on the cumulative distribution
function was 34 percent. When we increase the sample size from five to
15, then our rejection region, as I showed earlier, includes all the
outcomes from zero to six successes.

The area of the alternative probability mass function that covered
these values covers an area of not 34 percent, but of 61 percent of
the probability mass function, which is shown as a horizontal dashed
line on the cumulative distribution function. So the power of the test
to reject the false null hypothesis increases to 61 percent or
alternatively, the probability of incurring a type two error has
decreased to 39 percent.

Finally, further increasing our data set from 15 to 30 claims our
rejection region now includes all the values from zero to and
including 15. These are all the outcomes of which the cumulative
probability under the null hypothesis is less than five percent. The
this rejection region covers an area of 90 percent of our alternative
distribution and alternative distribution, which again assumes that
the actual occurrence of gold in the area is not 67 percent, but 40
percent is the same alternative distribution that was used over here
and over there, but now simply by having more samples. Our power to
reject the null hypothesis has increased to 90 percent or
equivalently. The probability of incurring a type two error has
reduced to only 10 percent.

============
Binomial (3)
============

In this third and final session on the binomial distribution, I will
highlight some potential pitfalls of statistical hypothesis testing
before introducing the important concept of confidence intervals,
which can be used to estimate population parameters from samples of
data.

---

I will now slightly digress into the realm of the philosophy of
science, but I only do so because it is very important to understand
the limitations of using statistical tests to validate scientific
hypotheses. The scientific method consists of three simple
steps.

First, we formulate a hypothesis rooted in some theory. Second, we
design an experiment to test the outcomes predicted by the
hypothesis. Third, we carry out the experiments and check to see if it
matches the predicted predictions from the hypothesis.

It is not difficult to see the great appeal that formalised
statistical hypothesis tests, such as the binomial test that we saw in
the previous session's hold for scientists, such as Earth
scientists.

However, we have to be very careful before using such tests and
applying them to scientific problems. You often read in the newspapers
that scientists proved this, or it's been scientifically proven that,
you know, in reality, scientists can rarely or never prove anything.
Mathematicians prove theorems. Scientists generally can only disprove
theories.

However this is still a very powerful pathway to the truth.  It has
put a man on the Moon. This method helped us beat the COVID 19
crisis. And it has taught us an incredible amount of information about
the planet on which we live. New knowledge is gained when the results
of an experiment do not match the expectations.

---

For example, suppose considered a hypothesis that the Earth's lower
mantle is made of olivine. This is a hypothesis that leads to certain
predictions about the material properties of this mineral. So we can
test the hypothesis by simulating the pressures and the temperatures
in the lower mantle and subjecting olivine to those conditions. When
you do these experiments, you will find that olivine is not stable at
the pressures that are found in the lower mantle. Hence, we can reject
that hypothesis from the experiment. We still do not know what the
lower mantle is made of, but at least we know that it is not
olivine.

Let us compare this outcome with that of a second hypothesis, which is
that the Earth's lower mantle is made of perovskites. Again, we could
test the hypothesis using an experiment in which we simulate lower
mantle pressures and apply it to perovskites. And the results of that
experiment would be that perovskite is indeed stable at lower mantle
pressures. Now what have you learnt from this experiment?  Actually,
not as much as we've learnt from the olivine experiment.

We certainly did not prove that the Earth's lower mantle consists of
perovskites. There are lots of other minerals that are stable at lower
mantle pressures. The only thing that we can say is that the null
hypothesis has survived our tests to live another day.

Now, the scientific method is strikingly similar to the way in which a
statistical hypothesis test is carried out. A null hypothesis, like a
scientific hypothesis cannot be proven. It can only be
disproven. Rejection of a null hypothesis is the best outcome because
it is the only outcome that teaches us something new. So it may seem
very natural to use this statistical approach to test statistical
hypotheses. However, doing so is not without dangers. 

---

To explain these dangers, let us go back to the power analysis that we
carried out in the previous session.  The power of a hypothesis test,
for example, a binomial hypothesis test, to reject a null hypothesis
increases with sample size. This slide shows the two-sided p-values
for four alternative hypotheses that differ from the null hypothesis
by different amounts.

If our hypothesis is that the probability of finding gold is p=2/3, so
sixty seven percent, and if the actual occurrence of gold is 50
percent, then a small sample allows us to reject the null hypothesis
by dropping the p value below the 0.05 cut-off.

If the null hypothesis is more similar to the alternative hypothesis,
for example, if the true occurrence of gold is sixty percent, then it
is more difficult to reject the null hypothesis. The p values go down
more slowly and we therefore need a larger samples of more than 100
claims to reject that false null hypothesis.

But even when the actual occurrence of gold is very similar to the
null value of 67 percent, for example, if it is 62.5 or 65%, then we
can still eventually reject the alternative hypothesis, provided that
enough claims have been analysed.

So no matter how small the violations of the null hypothesis are, there
always exist a sample size that is a large enough to detect
it. Statistical tests are an effective way to evaluate mathematical
hypotheses. They are less useful, however, to test scientific
hypotheses.

There is a profound difference between a mathematical and a scientific
hypothesis. Whereas a mathematical hypothesis is either right or
wrong. Scientific hypotheses are always somewhat wrong. For example,
it would be unreasonable to expect the geologists assessments to
determine the probability of finding gold to be exactly two thirds
down to the one hundredth significant digit. Estimating the correct
proportion of gold in the area to better than 10 percent would already
be a remarkable achievement.

Given enough data, there are always come some points where the
geological prediction is disproved. Given the large enough datasets,
even a tiny one percent deviation from the predicted value would yield
an unacceptably small p value.

---

This pitfall of statistical hypothesis testing is nicely captured by
these two quotes from two prominent statisticians. "All hypotheses are
wrong in some decimal place" and "All models are wrong, but some are
useful".

Let me give you another example suppose that we have analysed the
Mineralogical composition of two samples of sand that were collected
just 10 centimetres apart on the same beach. Our null hypothesis is
that the composition of the two samples is the same. Not plausible,
though this hypothesis may seem it will always be possible to reject
it, given a large enough sample. Perhaps we need to classify a million
grains from each sample, but at some point a statistically significant
difference will be found. Given the large enough sample, even the
tiniest hydraulic sorting effect becomes detectable.

So in conclusion, formula formalise hypothesis tests are of limited
use in science. The question that is relevant to scientists is not so
much whether a hypothesis is wrong, but rather how wrong it is. Now
this question can be answered using confidence intervals, which will
be discussed next.

---

In the context of our gold prospecting example, there is little use in
testing whether the parameter p is exactly two thirds. In reality, P
is unknown. It's far more useful to estimate B and to quantify the
statistical uncertainty associated with it. Earlier, we saw that given
to successful claims out of five trials, the maximum likelihood
estimates for our parameter p is two divided by five or 40
percent. However, this does not rule out other values, so let's
explore all possible values for P that are compatible with the
observed two successes successes out of the five claims.

First, would it be possible for peace to be zero? Well, if the
parameter is zero, then that would mean that there is no gold in the
field area. In that case, it would be impossible to have two
successes, so definitely equals zero can be ruled out. How about
equalling 0.1 or 10 percent?

Under this scenario, the probability of observing at least two
successful claims out of five would be the sum of all the beans and
the probability mass function from two to five. So that is the high
end, the high end tail of the binomial distribution. This probability
is 8.1 percent, which exceeds the cut-off value for a two sided
hypothesis test two and a half percent, which means that we can retain
the possibility of P being 0.1. This hypothesised value for our
parameter is compatible with the observation.

Moving on to P equalling 40 percent or two fifths, well, this is the
maximum likelihood estimate for P. So it's definitely possible that
this is the true value. Moving on to T equalling two thirds, which is
a value that is greater than our maximum likelihood estimates. We've
already done this hypothesis test, so we evaluated the lower end of
the binomial distribution, integrating the values from zero to
two. And we had already seen that the P value for that test is twenty
one percent, which is greater than the alpha divided by two cut-off,
which means that we have to retain this parameter value as a possible
true value given the two successes.

In contrast, the hypothesis that P equals 90 percent is incompatible
with our outcome because the sum of the probabilities from zero to
two.

Under this hypothesis is only zero point eighty six percent, which is
less than alpha divided by two. So this parameter value can be ruled
out. Similarly, the hypothesis that there is 100 percent gold in the
area is incompatible, obviously with our observations, because if this
were true, then we should always observe five successes out of five
claims. The fact that we did not observe this means that this
hypothesis and this parameter value again can be ruled out.

The sets of all the values for P that are compatible with the observed
outcome of two successful claims forms a confidence interval.

---

Using an iterative process, we can show that the lower and upper
limits of this interval are given by 0.053 and 0.85. These bounds mark
the lowest and highest possible value for P that are compatible with
our outcome of two successful claims at a 95 percent confidence
level.

We can visualise these outcomes as probability mass functions on the
left and as cumulative distribution functions on the right. If the
true probability of finding gold in the area is five point three
percent, then we would expect the majority of the outcomes to be less
than the observed two successful claims. But there is a two and a half
percent chance of observing two successful claims or more.

If the true probability of success is five point three percent in
black is shown, the upper range of the the outcomes corresponding to
the upper range of the parameter p. If the occurrence of gold is 85
percent in the field area, then most of the outcomes should be higher
than two successful claims. But there still is a two and a half
percent chance of observing two or fewer successful claims under this
scenario.

On the cumulative distribution functions, the dotted lines mark the
two and a half percent and ninety seven and a half percent lines, and
these intersects the lower range for the parameter p cumulative
distribution corresponding to that at the ninety seven point five
percent mark and the higher arranged. The upper limit for our
confidence intervals is intersected at the two and a half percent
level.

---

Repeating this entire procedure for a different outcome, suppose that
four out of five claims were successful, so that shown as this
vertical dashed line. Then the 95 percent confidence interval for the
parameter P is different from before. It is now encompassing all the
values between zero points to a force or twenty eight point four
percent gold in the area and ninety nine point five percent gold in
the area.

If the true proportion of gold is twenty eight point four percent,
then there is a two and a half percent chance of observing four or
more successes amongst five trials. If there is ninety nine point five
percent gold in the area, then most of the time we will have five
successful claims. But about two and a half percent of the time, we
will have only four or even fewer successful gold discoveries. Under
this scenario, and this is why these two values are the boundaries of
the 95 percent confidence interval for P given for successes out of
five claims.

---

What happens if we increase the sample size from five to 30 claims and
the number of successful claims from two to 12, then the maximum
likelihood estimate for P remains two fifths, which equals twelve
thirtieth or 40 percent, just like the previous the first example. But
our 95 percent confidence interval gets a lot narrower to values from
zero point twenty three to zero point fifty nine. So the procedure is
identical as before. We've got the binomial distributions under the
low end scenario shown in grey and the binomial distribution under the
high p scenario shown in white and black. And there is again a two and
a half percent likelihood that the that we get more than twelve
discoveries under the low end scenario, or that we get less fewer than
twelve discoveries on the the high end scenario.

---

To further explore this trend of narrowing confidence intervals with
increasing sample size, let us evaluate the 95 percent confidence
intervals for maximum likelihood estimates shown as dotted lines here
of two thirds on the left and of one fifth on the right for a range of
different sample sizes from end equalling three to end equalling
300. These confidence intervals are asymmetric. They plot completely
within the allowable range for P of zero to one, and they become
progressively narrower with increasing sample size. This reflects a
steady improvement of the precision of our estimates of peak with
increasing sample size. In other words, large datasets are rewarded
with better precision compared to small datasets.

=======
Poisson
=======

In this session, I will introduce a different probability
distribution, the Poisson distribution, which is related to the
binomial distribution but describes the frequency of occurrence of
rare and independent events.

---

This bar chart represents a time series of the number of earthquakes
that have happened in the United States of magnitude five or greater
between 1917 on 2016, with aftershocks removed. So this is a
declustered earthquake catalogue. The number of earthquakes per year
varies greatly between years. Some years have a lot of strong
earthquakes whereas other years have few of them. The number of
earthquakes in each of these bins forms a new data sets of 100
numbers, one for each year, which can be visualised as a new
histogram. This dataset of the number of earthquakes per year has a
mean of 5.43 a standard deviation of 2.5 on the variance (the square
of the standard deviation) of 6.24. Now, this mean and variance for
the dataset of earthquake frequencies are pretty similar.

---

A second data set that we will use for today's session is a dataset of
5000 grains of sand. All these circles that have been mounted in an
uncovered thin section and imaged with a scanning electron microscope
or SEM. This instrument has identified the locations of zircon
crystals that are suitable for geochronology. These are marked as
white squares. We have subdivided the slide into a number of gratitude
and counted the number of zircons within each of these squares
subdivisions of our slide. This set of counts represents a new data
set.

---

When we tally the number of the circles per graticule, then calculate
the arithmetic mean of this data set, we get a value of 3.5. The
standard deviation is 1.85 and the square of that (the variance) is
3.4, which again is similar to the arithmetic mean.  This similarity
of the variance to the mean turns out to be a characteristic property
of the Poisson distribution.

---

The Poisson distribution describes the frequency of rare events in
time or space. It predicts the likelihood of observing the number of
successes k given the long term average of successes lambda. Its
probability mass function contains just one parameter, lambda, as
opposed to the binomial distribution, which had two parameters: p,
which is the probability of success and n, which was the sample size.
These panels show the probability mass function and the cumulative
distribution function for Poisson distributions with different
parameters lambda, so different arithmetic means, which are shown as
vertical dash lines. These lambda values vary from 1, 2 and 5 to
10. We can see that the Poisson distribution is positively skewed,
but becomes more symmetric with increasing lambda.

---

The Poisson distribution is closely related to the binomial
distribution. Recall, again, that the binomial distribution depends on
two parameters, n and p. We can show that the binomial distribution
converges to the Poisson distribution with increasing n and decreasing
p. In the limit of n equaling infinity and p equaling zero, the
binomial distribution simplifies to a Poisson distribution with a
lambda parameter equal to n x p.

This table illustrates this phenomenon by evaluating the probability
of observing two or fewer successes under a binomial distribution with
n p = 5. So if n=10 and p=0.5, then the probability of observing two
or fewer successes is 5.47%. If we increase n by multiplying it with 2
and we divide p by 2, then n x p still equals 5. But now the
probability of observing two or fewer successes increases to 9.13%. If
we further increase n to 50 and decrease p to 0.1, then the
probability of observing 2 or fewer successes under a binomial
distribution is 11.2% and we can continue doing so until n=10,000 and
p is reduced to 0.5. That number times 10,000 equals five. The
binomial probability converges to a value of 12.46% which is exactly
the result that you would obtain from a Poisson distribution where
lambda equals 5.

---

The Poisson distribution expresses the probability of a given number
of events occurring in a fixed interval of time or space if these
events occurred with a constant mean rate and are independent of the
time that has elapsed since the last event. Examples of possible
variables include the number of people killed by lightning per year;
the number of mutations in the DNA per generation; the number of
radioactive this integrations that happen per second, per year or per
millions of years; the number of mass extinctions that occurred 100
million years as a result of large asteroid impacts. All these
phenomena, if we tally then we calculate the mean on the variance of
their occurrence, is we will find that these two values will be very
similar. However, the number off earthquakes per unit time, including
aftershocks so without the clustering does not follow a Poisson
distribution and neither does a number of floods per year. Because
both of these natural phenomena are clustered in time. So if we can
tally them on, we calculate the arithmetic mean and the variance of
their occurences, we find that those values will no equal each other.

---

The probability mass function of the Poisson distribution depends, as
I said, on a single parameter lambda, and so far I have assumed that I
know the value of this parameter, and I could then evaluate the
probability of any given outcome k given this parameter. Now, in
reality, we generally do not know what lambda is. So just as we did
for the binomial distribution, we can estimate the parameter lambda
given a certain number of successes k by simply rephrasing this
probability mass function as a function of lambda instead of a
function of k.

This is the likelihood function and we can estimate Lambda by
maximising this function given k. The derivation of this is again
contained in the notes. I will not go into the details of this, but
suffice it to say that if we have k successes than this value k equals
our best estimate for lambda, which is written as lambda hat, our
maximum likelihood estimate for the parameter of our Poisson
distribution.

---

Hypothesis testing for Poisson variables proceeds in exactly the same
way as it did for binomial variables. Let us illustrate this using a
zircon example. Suppose that the long term average number of zircons
per graticule in a thin section is hypothesised to be 3.5. And further
suppose that we count nine zircons in a particular graticule. Is this
outcome of nine zircons compatible with our hypothesised parameter
value?

Well, to test this, we formalised the hypothesis. So our hypothesis is
that lambda=3.5. Our maximum likelihood estimate for the parameter
would have been 9, which is greater than the hypothesised version
3.5. So the natural alternative hypothesis to test would be the
asymmetric hypothesis that lambda is greater than 3.5, and we carry
out a one sided hypothesis test.

In contrast with the examples that I considered for the binomial
distribution, this one sided tests will actually test the upper tail
of the Poisson distribution. Our test statistic is again the number of
successes which is nine and our null distribution now is, of course,
no binomial distribution but a Poisson distribution of which I
tabulate here the probability mass function from 0 to 10, so including
our outcome of nine. I also tabulate the probability of observing an
outcome of greater than or equal to any of these numbers of successes
under the null hypothesis that lambda=3.5.

---

The significance level is, as always, 5%; and the rejection region
groups all those values for the upper tail of the Poisson distribution
that are less than 0.05. This includes the values of 8, 9, 10 and, in
fact, any value greater than that this, as the table could go up to
infinity. In principle, we could find more than 10 zircons. But I've
cut off this table because these probabilities are already getting
very small.

Our observed outcome of nine successes falls in this rejection region
and therefore our null hypothesis is rejected. Equivalently, the
probability of observing nine or more successes under the null
hypothesis is the p value. This p value is 0.1, so 1%. This is less
than our significance level of 5%, again leading to the rejection of
the null hypothesis.

---

Visualising the same results graphically, we have the probability mass
function of the null distribution on the left and its cumulative
distribution function on the right. The observed outcome of nine
zircons is shown as vertical dashed lines on both of these panels.

The rejection region is this area coloured in white, which
covers 5% of the area under the probability mass function. Our
observed outcome plots firmly inside this rejection region, leading to
the rejection of our null hypothesis. On the cumulative distribution
function, the 95 percentile of our null distribution gives us a
lower limit to the rejection region of 7. Our observed outcome
plots towards the right of this. So again we reject the null hypothesis.

The p-value of the one sided hypothesis test is one minus the position
of this horizontal dashed line, which is the intersection of our
observation with a cumulative distribution function. This is less than
0.5. The dash line plots above 0.95 percentile, again leading to the
rejection of our null hypothesis.

---

The hypothesis test that we just did was referring to the zircon
counting example of the beginning of the session. The average number
of observations per graticule, so the average number of zircons per
bin in this example was 3.5. And therefore, according to our maximum
likelihood estimation, the best estimate for our possible parameter
lander is 3.5 as well.

According to our hypothesis test a value of nines zircons is
incompatible with a parameter value of lambda equaling 3.5. However,
if you pay close attention to the slide, you will see that the value
of nine zircons does appear in our data sets. Now, does this mean that
our data did not follow a Poisson distribution? Well, the answer is
no. The apparent contradiction between the point counting data and the
hypothesis test is simply the result of multiple testing.

---

To understand this problem, we need to go back to the multiplicity of
rule of our session on probability theory. The probability of
incurring a type-1 error is alpha, so 5%. Therefore, the probability
of not committing a type-1 error is one minus alpha or 95%. But this
is only true for one test. If we perform two tests than the
probability of twice avoiding a type one error is one minus alpha
squared, which is 90.25%. And the probability of committing at least
one type one error is one minus that probability, which is 9.75% or
nearly twice our probability of incurring a type-1 error for one
experiment.

If we increase the number of tests to three, then the probability of
incurring at least one type one error further increases to 14%. And if
we have 48 experiments, the probability of making at least one type-1
error has increased to 91.5%. This is the situation that we are in
because there are 48 graticules in this thin section. So we are more
likely than not to erroneously reject a correct null hypothesis. We
could remediate this problem by adjusting our significance
criteria. This is called a Bonferroni correction.

If we simply divide alpha by the number of experiments, then we reduce
our probability of incurring a type-1 error. However, a side effect or
a consequence of this Bonferroni correction is that we also reduce the
power of our hypothesis test to reject false null hypotheses.

---

The construction of confidence intervals for the Poisson parameter
lambda proceeds in pretty much the same way as it did for the binomial
parameter p. To illustrate this, let us move from the zircons to the
earthquake example. Let's construct a 95% confidence interval for
lambda, given the observation that five magnitude five or greater
earthquakes occurred in the United States in 2016. Let's try to
estimate from this single observation what the long term number of
earthquakes of that magnitude would be.

The lower limit of a 95% confidence region for the number of
earthquakes per year is marked by the value of lambda that is more
than 2.5% likely to produce an observation of five or more earthquakes
per year. This turns out to be a lambda equaling 1.62, as shown as a
grey step function in the cumulative distribution panel, and as a grey
bar plot in the probability mass function panel. The upper limit of
the confidence interval is marked by the value of lambda that is more
than 97.5% likely to produce an observation of five or fewer
earthquakes per year. This value is 11.7, and is shown as a white
cumulative distribution function and probability mass function. Hence,
the 95% confidence interval ranges from 1.62 to 11.7. This includes
the long term average of all the 100 preceding years which was shown
at the beginning of this session, which is 5.43.

---

Finally, repeating this exercise for all the observations in our
declustered American earthquake data set yields 100 confidence
intervals for the long term average number of earthquakes per
year. The horizontal dotted line marks the average of all the years,
which again, is the value of 5.43. The dots are our maximum
likelihood estimates for the individual observations, and the grey
bars are 95% confidence intervals.

The vast majority of these intervals shown in great overlap with our
long term average. Three years (1941, 1954 and 1984), however, stand
out because their 95% confidence intervals do not overlap with the
long term average. Does this mean that the earthquake statistics did
not fit a possible distribution during those years? Well, again, the
answer to this question is no for the same reason, as in our zircon
example.

When a large number of confidence intervals is drawn, it is inevitable
that some of these do not include the true parameter value. In fact,
it would be suspicious if all the error bars overlapped with the long
term average. With a significant level of 5% there should roughly be a
5% chance of committing a type-1 error. Therefore, we would expect 5%
of the samples to be rejected and 5% of the error bars to exclude the
true parameter value. The observed number of rejected samples, which
is three out of 100 is perfectly in line with those expectations.

=======================
The Normal distribution
=======================

After introducing the binomial and Poisson distributions, the next
session will discuss the Gaussian distribution, which is also known as
the normal distribution.

---

In the previous two sessions, we saw that the probability mass
function of the binomial distribution and of the Poisson distribution
can be described as a function of two parameters for the binomial, and
of one parameter for the Poisson distribution. However, the binomial
and Poisson are just two examples of countless possible
distributions. Some other examples of distributions that are relevant
to the Earth sciences includes the negative binomial distribution,
which models the number of successes or failures in a sequence of
Bernoulli trials before a specified number of failures or
successes. For example, it describes a number of dry holes X there are
drilled before R petroleum discoveries are made given a probability of
discovery P that is described by this probability mass function.

The multinomial distribution is an extension of the binomial
distribution, where more than two outcomes are possible. For example,
it describes the point counts of multiple minerals in a thin
section. If p1, p2 etc to pm are the relative proportions of m
minerals, where the sum of these probabilities of these proportions is
1 and if k1, k2, etc to km are the respective counts of these minerals
in a thin section where the sum of these counts is N, then the
probability of any outcome given these parameter values, is the
product of all these values here. This formula simplifies to the
binomial distribution if m=2.

The binomial and Poisson distributions are univariate distributions
that aim to describe one dimensional data sets. However, the
multinomial distribution is an example of a multivariate probability
distribution, which describes multidimensional data sets.

Next, let's move on to the uniform distribution, which is the simplest
example of a continuous distribution, as opposed to the discrete
probability distributions that were introduced before. For any number
x between the minimum a and the maximum b, the probability is given by
this simple function here. X here does not have to be an integer, but
is free to take on any decimal value. Therefore, this probability is
not referred to as a probability mass function or PMF, but as a
probability density function or PDF. Whereas probability mass
functions are represented by the letter P, probability density
functions are generally referred to as lower case F. This is because
the probability of observing any particular value X is actually
zero. To calculate the probability that x falls in a certain range,
for example, between c and d, we can calculate this probability p as
an integral of all the densities between these two boundaries.

We will not discuss this uniform distribution or these other
distributions in more detail. Instead, I would like to focus our
attention on one other distribution the Gaussian distribution, which
is so common that it's also known as the normal distribution, implying
that all the other distributions, including these guys, are abnormal.

---

To understand what is so normal about the Gaussian distribution. Let
us revisit the Old Faithful data set that we discussed in our session
on statistical plotting. So here is a kernel density estimate and a
rug plot of the marginal distribution of 272 Old Faithful eruption
durations, in minutes. This distribution has two modes at about two
minutes and about 4.5 minutes.

---

Now let us select two events at random from the original distribution
of guyser eruptions and add their durations together. Repeat this to
create a new dataset of 500 values, which has shown here as a rug plot
on the new kernel density estimate. This kernel density estimate has
not two but three modes, including one at four minutes, which groups
all the pairs of events that came from the two minute mode of our
original distribution; one mode at nine minutes, which groups the sums
of pairs of eruptions that came from the 4.5 minute mode of our
original distribution; and then there's a big a third mode in the
middle of values around 6.5 minutes minutes, which consists of the
sums of pairs that combine one events from the two minute mode and one
event from the 4.5 minute mode.

---

Next, we repeat this exercise using not two but three randomly chosen
events from the geyser eruption data, and again we add the duration of
these three events together and repeat that 500 times to create a
third data set shown as a rug plot and a kernel density estimate
again. This KKDE has four visible modes, including peaks at six
minutes, which groups three events of two minute duration; one mode at
8.5 minutes, which consists of two events from the two minute modes,
plus one event at the 4.5 minute mode one event; one mode at 11
minutes, which is one event at two minutes, plus two events at 4.5
minutes; and one mode at 13.5 minutes, which consists of 3, 4.5
minutes events.

---

And finally, here is a distribution of 500 sums off 10 randomly
selected eruption durations. This produces a fourth dataset whose
kernel density estimate has a single mode with a symmetric shape that
tails off towards lower and higher values. This is the characteristic
bell shape of a Gaussian distribution.

---

The bell shape can be mathematically described by this probability
density function, which has two parameters: mu, which is the mean and
sigma, which is the standard deviation. It can be mathematically
proven that the sum of randomly selected values converges to a
Gaussian distribution provided that the number of these values in the
sum is large enough. This convergence is guaranteed regardless of the
distribution of the original data. This important mathematical law is
called the Central Limit Theorem.

---

The Gaussian distribution is known as the normal distribution because
it naturally arises from additive processes, which are very common in
nature. It is actually easy to create a normally distributed dataset
in the laboratory environment. There even exists a machine that
generally generates normally distributed numbers. This machine is
called Gaulton's bean machine. It's a mechanical device that simulates
additive processes, which consists of a triangular arrangement of pegs
that are located above a linear array of containers.

When a bead enters the machine from the top, the bead bounces off the
pegs of the machine on its way down to this linear array of
containers, the probability of bouncing to the left equals a
probability of bouncing to the right. After a given number of bounces,
the bead lands in one of the containers, forming a bell shaped
distribution with increasing number of layers in the machine. This
distribution converges to a Gaussian distribution as the number of
peges increases.

Additive processes like this are very common in physics. For example,
when a drop of ink is put in a volume of water, the ink molecules
spread by bouncing off the water molecules. This brownian motion
creates a Gaussian distribution in which most ink molecules remain
near the original location at the mean with wide tails and other
directions.

---

The binomial, Poisson, negative binomial, multinomial, uniform and
univariate normal distributions are just a small selection from an
infinite number of possible probability distributions. These
particular distributions were given specific names because they
commonly occur in nature. However, the majority of probability
distributions do not fall into a specific Parametric category.

For example, the bivariate distribution of Old Faithful eruption gaps
and durations is not really captured by any of these distributions. In
fact, it's easy to invent your own distribution. So here is an example
of four distributions that I invented in two dimensional data
space.

Let us now collect 100 random points from these four distributions and
plot them as four scatter plots. Then these scatter plots of sampling
distributions resemble the true populations at the top of the slide.

---

Next, we can calculate the sum of all the Xs and all the Ys for each
of these four samples, and this gives rise to four new pairs of
coordinates. Repeating this exercise 200 times and plotting the four
resulting data sets as scatter plots produces these four new
diagrams.

Despite the completely different appearance of the four parent
distributions and the four sample distributions, the distributions of
their sums all look very similar. They consist of an elliptical point
cloud that is dense in the middle and thins out towards the edges. The
number of points per unit area is accurately described by a bivariate
Gaussian distribution, which has a three dimensional bell shape.

---

The bivariate normal distribution is not described by two, but by five
parameters mu_x and mu_y, which are the means of the x and y signals,
sigma_x and sigma_y, which are the standard deviations of x and y, and
sigma(x,y), which is the covariance of x and y. I'll explain those in
more detail in a second.

The probability density function in this case involves a matrix
equation, where this matrix is the so-called covariance matrix, which
is inverted, and the vertical lines denote the determinants of the
covariance matrix.

If you plug bivariate data into this equation, then it spits out a
number, which is the probability density at that particular point in
x-y space. Contouring these probability densities for x and y produces
this elliptical set of contour lines, which are a map view of the two
dimensional or three dimensional bell curve that fits our cloud of
data points.

---

Going back to the univariate normal distribution, which, as I said
before, is completely controlled by two parameters, we can see that
the mean mu is our location parameter. Reducing it from 0 to -1 shifts
the distribution to the left, increasing mu to +1 shift it to the
right.

Sigma is our dispersion parameter; the standard deviation. It controls
the width of the distribution. Increasing sigma from 1 to 2 produces a
lower, but wider distribution, whereas reducing sigma from 1 to 1 half
produces a narrower but taller distribution.

By definition, the probability density function always integrates to
unity, So the area under each of these curves is the same is always
equal to one.

---

Because the Gaussian distribution is symmetric, its mean equals its
median and its mode. Other useful numerical properties of the Gaussian
distribution include the fact that the interval from the mean minus
one standard deviation to the mean plus one standard deviation covers
68.27% of the area under the probability density function, and the
interval from the mean minus two times the standard deviation to the
mean plus plus two times the standard deviation covers an area of
95.45%. Conversely, exactly 95% of the area under the normal density
is contained within an interval from the mean minus 1.96 to the mean
plus 1.96 standard deviations.

---

The mu and sigma parameters in a bivariate normal distribution have
exactly the same meaning as they do for a univariate normal
distribution. mu_x and sigma_x describe the univariate distribution of
a marginal projection of our bivariate data onto the x axis; and mu_y
and sigma_y describe the univariate normal distribution obtained by
projecting the data onto the y axis.

So if we reduce mu_x, we shift the whole distribution to the left. If
we increase mu_y we move the distribution higher up et
cetera. Similarly, sigma_x and sigma_y control the dispersion of the
marginal distribution. So if we double sigma_y, then we've essentially
doubled the height of our bivariate normal distribution. Finally, the
fifth parameter of our bivariate normal distribution is the
covariance. This controls the degree of correlation between the x and
the y variable in a bivariate various normal distribution.

Non-zero values of the covariance cause the contours of our
distribution to be tilted with respect to the horizontal and vertical
axes of the diagram. Positive values for the covariance result in a
positive slope and negative values in a negative slope. The closer
these covariance values are to -1 or +1, the narrower these contours
will be.

---

The parameters mu and sigma of a Gaussian distribution are generally
unknown. But we can estimate them parameters from the data just like
the binomial parameters p or the Poisson parameter lambda. This can be
done using the method of maximum likelihood.

Suppose that we have n data points x1, x2, etc. to xn. Then we can use
the multiplication rule of probability and formulate the normal
likelihood function as a product of these probability density
functions one for each measurements. We can maximise this likelihood
with respect to mu and sigma. The details of this calculation are
presented in the notes. I won't go over them here. I just want to
discuss the results.

It turns out that the maximum likelihood estimate for mu, which we
denote by mu-hat, is simply the formula for the arithmetic mean, which
we already saw in the session on summary statistics; and the maximum
likelihood estimate for the standard deviation sigma-hat is a formula
that is very similar to the definition of standard deviation, and
which again I had already introduced in our session on summary
statistics.

The only difference between these two equations is that here we divide
by n and here we divide by n minus one. The subtraction of one from n
over here is called the Bessel correction. It accounts for the fact
that, by using an estimate of the mean x-bar rather than the true
value of the mean mu, we introduce an additional source of uncertainty
in the estimates for the standard deviation. This additional
uncertainty is accounted for by subtracting 1 so-called degree of
freedom from the fit to our data.

And finally, the maximum likelihood estimate for the covariance of two
measured variables x and y in a bivariate normal data set, produces a
formula that is again very similar to the sample covariance. Again,
the difference being here that we remove one degree of freedom. We
apply a Bessel correction to account for the fact that we are
estimating these means rather than having these means being given.

================
Error estimation
================

Suppose that the extinction of the dinosaurs has been dated at 65
million years in one location and a meteorite impact has been dated at
64 million years elsewhere. These two numbers are effectively
meaningless in the absence of an estimate of precision. Taken at face
value, the dates imply that the meteorite impact took place one
million years after the mass extinction, which would rule out a causal
relationship between the two events. However, if the statistical
uncertainty of the age estimates is greater than the one million year
difference between them, then such a causal relationship remains very
much plausible. In conclusion, the statistical uncertainty of
geological or statistical estimates is as important as those estimates
themselves.

---

There are two aspects of analytical uncertainty. Accuracy is the
closeness of a statistical estimates to its true and unknown value.
Precision is the closeness of multiple measurements to each
other. These two concepts can be illustrated using a darts board
analogy. In this darts board analogy, the bull's eye represents the
true value for our unknown parameter. This value is estimated by
taking several measurements which are shown as white circles. The
accuracy of the data is the closeness of the average value of our
measurements to the bull's eye. So these are two accurate data sets,
and these are two inaccurate data sets.

The precision is the degree of clustering of our data. Data can either
be precise or that can be imprecise for different outcomes are
possible. The best of which combines high precision with high
accuracy. The worst situation is a combination of high precision with
low accuracy. This is the worst case scenario because the high
precision is giving us false confidence in the quality of the data and
is masking the fact that the results are biased with respect to the
true value.

Accuracy can be assessed by analysing reference materials whose true
parameter values are known through independent means. This procedure
involves little statistics and will not be discussed further in
today's session. Quantifying precision, however, is a more involved
process, which is also known as error propagation, and that's what I
will be talking about for the remainder of this session.

---

Consider a quantity z that is a function g of some measurements x.
For example, z could be the geological age, g could be the age
equation, and x could be some isotopic measurements which are
determined on a mass spectrometer and that can be used with the
principles of radioactivity to get a handle on geologic time.

Suppose that we've got multiple measurements of these isotopic
compositions. Suppose that we've got n of them, and assume that they
all follow a normal distribution. Then we can estimate the mean and
the standard deviation of that normal distribution using the sample
mean and standard deviation summary statistics.

The best estimate for the geological age is then our function g of the
average value of x. So now we know the age of our system. As I
explained in my introduction, it is equally important to also be able
to quantify the uncertainty of this age that is caused by the
variability of our isotopic measurements. Answering that question is
what error propagation is all about.

---

To simplify the situation. Let us consider a functions that are linear
or approximately linear with respect to the measurements x. Here are
shown two examples of such functions, g1 and g2. They have different
slopes, but they are both linear with respect to x. The measurements
are again assumed to follow normal distributions with a mean x-bar and
with a standard deviation s[x]. The best estimate for our inferred
quantity z is the value that corresponds to the mean of our xs. So for
my first function, the estimated value for z is z1-bar and for the
second function it is z2-bar.

Because our functions are linear, deviations of individual
measurements from the mean of the measurements are proportional to
deviations of the inferred quantity from the mean of the inferred
quantities. If the slope of our line is less than one, then the
precision of the inferred quantity is going to be better than the
precision of our measurements. If the slope of the line is steeper
than one, then the uncertainty or the deviations of the measurements
around the mean are magnified and result in a greater uncertainty of
the inferred quantity.

We can formalise this principle mathematically by writing that
deviations of z from the mean are proportional to deviations of x from
the mean of the measurements, where the constant of proportionality is
the derivative of our function g with respect to our measurements x,
which is the slope of the two lines.

Now, recall the definition of the variance, which is the square of the
standard deviations of z. This is the quantity that we want to
propagate. This definition, we saw, is one divided by n-1 minus one
times the sum of the squared deviations from the mean. So if we plug
this equation into here, then we get a new formula which can be
rearranged to produce our first error propagation equation.

It tells us that the uncertainty of z is proportional to the
uncertainty of x, where the constant of proportionality is the
absolute value of the gradient of our function with respect to our
measurements.

---

Next, let us move on to multivariate problems. Suppose that our
estimated quantity z is calculated as a function g of not one, but two
measured quantities x and y, and further suppose that x and y follow a
bivariate normal distribution with parameters that can be estimated
from some data as x-bar, y-bar, s[x], s[y] and s[x,y], which is the
covariance of x and y. We then make the simplifying approximation that
our function, which in this case is curved, is approximately a linear
in the vicinity of our measurements. Then we can estimate the
uncertainty of z in exactly the way the same way as we did for the
univariate case.

We plug the deviations of x and y into the definition of the variance
for z; we rearrange; and we get an expression that gives us the
squared uncertainties of z as a function of the squared uncertainties
of x and y, the covariance of x and y, and the slopes of the function
g with respect to x and to y. This formula is the general error
propagation formula for a bivariate function.

We can write it either explicitly in this form or we can cast it into
a matrix form where we have one matrix, which is called the Jacobean
matrix; we have the transpose of that same Jacobean matrix, including
these partial derivatives; and in the middle we have the so-called
covariance matrix of the measurements, which contain the variances and
the covariance. These two mathematical forms are mathematically
equivalent.

---

Here again, is the generic formula for the propagation of uncertainty
of a function of two measured quantities x and y. This formula may
look a little bit intimidating to you, but let us just demystify it by
applying it to some common functions, beginning with what is perhaps
the most commonly used function of all: the addition operator.

So here we've got an inferred quantity Z, which is equal to the sum of
a constant A plus a constant B times of first measured quantity X plus
a third constancy times a second measured quantity Y. So here the
constants do not have any analytical uncertainty associated with
them.

To give one example, consider the circumference of a circle, which
equals the diameter of the circle as can be measured with a ruler. So
that would be a measurement X, for example, times PI, where PI is
known with infinite precision. So when we propagate the uncertainty of
the circumference of a circle, we do not have to worry about the
uncertainty of PI, which is represented generically here as A, B or
C. Then the derivatives of this formula with respect to the
measurements X and Y are simply equal to the constants B and C.
Plugging these derivatives into our generic error propagation formula
produces this equation.

Now, let us further explore this equation by considering the special
case of two measurements X and Y that were made independently and are
uncorrelated. In that case, the covariance here is zero, and this
third term of our error propagation formula disappears. Further
suppose that our constants a is zero and our constants B and C are
both equal to 1. Then this generic equation simplifies to a simple sum
of two measurements X and Y. In other words, the squared uncertainty
of Z is simply the sum of the squared uncertainties of X and Y. The
variance of Z equals the sum of the variances of X and Y.

---

OK, let's move on to subtraction. Suppose that Z equals A times X
minus B times Y. Then again, the derivatives of Z with respect to X
and Y are equal to these two constants. So A and -B, and our error
propagation formula becomes nearly identical to that for addition,
with the exception of the covariance term, which now has a negative
sign in front of it. If our measurements X and Y are independent, then
again, these covariances disappear and our formula is identical to the
error propagation formula of addition. So if A and B are both 1 and X
and Y are independent, then again, the variance of Z equals the sum of
the variances of X and Y.

---

Next: multiplication. Multiplication can be written in a generic form
as Z equals a constant times X times Y, where X and Y are both
measured quantities with some analytical uncertainty. The derivatives
of Z with respect to X and with respect to Y are A times Y and A times
X respectively. Plugging these values into our error propagation
formula produces this equation, which we can then further rearrange by
dividing both the left hand side and the right hand side by Z
squared. So here we've got the uncertainty of Z divided by Z, which is
also known as the coefficient of variation. And on the right we can
divide by A times X Y squared and that produces this formula.

There's a lot of redundant values in here that cancel out. The Ys here
can cancel out in the numerator and the denominator of the first term;
the Xs can be removed by from the numerator and the denominator of the
second term; and we could do the same for the covariance term. And
then you can see that this formula simplifies to an equation in which
the coefficient of variation of z squared equals the coefficient of
variation of x squared, plus that of y squared and this covariance
term. If the covariance are zero, then again, this third term
disappears and we can see that the relative uncertainties of z squared
equal the sum of the relative uncertainties of X and Y each squared.

---

Addition, subtraction and multiplication are just three examples of
functions, but exactly the same procedure can also be applied to any
other function. In the notes, I provide further details about the
error, propagation of division, exponentiation, logarithms and the
power operator. With these seven functions, you can cover a large
number of common mathematical formulae in the Earth Sciences.

Error propagation for more complicated functions can either be derived
from the generic error propagation formula directly, or can be done by
combining these seven equations together using the so-called chain
rule.

---

For example, consider this equation here, which describes the distance
travelled by an object as a function of time t where d-naught is the
initial position, v-naught is the initial velocity and g is the
acceleration. Suppose that d_0, v_0 and g are known with absolute
precision. And that only t as associated with analytical uncertainty,
for example, because we are using an imprecise stopwatch.

Then this equation does not immediately fit in into the templates
represented by any of my seven equations. However, it is easy to
define two new functions that do fit these templates. For example, we
can rewrite this equation of three terms as a sum of two terms X and
Y, where X groups the first two terms of our original equation and Y
represents the third term of the equation. Then the uncertainty of d
is simply given by the error propagation formula for a sum as a
function of the uncertainties of X and Y.

The uncertainty of X can be propagated using the error propagation
formula for a sum and the uncertainty of Y can be propagated using the
error propagation formula for the powering operation. We then plug
these uncertainties into this equation, and we get this formula, which
gives us the uncertainty of d as a function of the uncertainties of
our measured quantity t.

This chain rule is a powerful way to propagate the uncertainties of
potentially very complicated problems. It acts really like a set of
Russian dolls in which you break a complex function up into simpler
functions for which you can use the elementary error propagation
formulae.

---

The estimated standard deviation of some derived quantity obtained by
error propagation is also known as its standard error. The mean of a
set of numbers is one example of such a derived quantity, and its
estimated uncertainty obtained by error propagation is called the
standard error of the mean. Let us consider a dataset of n values of a
quantity x, so we've got x1, x2, all the way to xn. The x-bar is the
arithmetic mean of these values.

This equation can be written as a sum, and therefore the analytical
uncertainty of x-bar can be propagated using the error propagation
formula for the addition operator. To this end, we need to modify this
formula for addition from 2 measurements to n measurements so that A
is zero, A is 1/n, x is x1, c is 1/n, y is x2. And then we add
additional terms, which each could consist of 1/n times the ith
measurement where i varies from 1 to n. We could then plug this long
equation into the generalised formula for the error propagation of a
sum. And that gives us a standard error of the mean squared as a
function of the sum of these squared terms, where each term contains
the uncertainty of a particular measurement divided by the number of
measurements.

This constant (1/n)^2, can be moved out of the summation and then we
can make the reasonable assumption that all of the measurements in our
dataset were drawn from the same population, which means that the
uncertainty of each of the Xs equals the uncertainty of the entire
dataset, which we denote by the standard deviation of X. So then all
these times become the same.

And this summation turns into n times the variance of X. We've got two
n here. One of them will cancel out so n divided by n squared equals
1/n. And we take the square root of both of these terms and we get the
standard error of the mean being equal to the standard deviation of
the data divided by the square root of n. This is a very important
result. It relates a standard error of the mean to the standard
deviation of the data.

---

To illustrate the effect of the square root rule, consider the
statistics of human height as an example. The distribution of heights
of adult people is approximately normal, with a mean of 165
centimetres and a standard deviation of about 10 centimetres. There
are about five billion adult humans on the planet. Averaging their
heights should produce a value of one hundred and sixty five
centimetres. The standard error of this average height is 10, divided
by the square root of five billion. That is only 1.5 microns.

So even though there is a lot of dispersion amongst the heights of
humans, the standard error of the mean is tiny and the precision of
the average height is tremendous. This is very important because it
means that no matter how imprecise a dataset might be provided that we
average enough measurements, we can always reduce the precision of our
measurements to arbitrarily low values.

===========================
Comparing distributions (1)
===========================

The previous sessions have introduced a plethora of parametric
distributions that allow us to test hypotheses and assess the
precision of experimental results. However, these inferences are only
as strong as the assumptions on which they are based. For example,
when we used a binomial distribution to assess the occurrence of gold
in a prospecting area, we assumed that the gold was randomly
distributed across all the claims. And when we used a Poisson
distribution to model earthquakes, we assumed that the earthquake
catalogue was free of clusters and all aftershocks had been removed
from it. In this session, I will introduce some strategies to test
assumptions like these, both graphically and numerically.

---

A quantile-quantile or Q-Q plot is a graphical method for comparing
two probability distributions by plotting their quantiles against each
other. Recall that kwantiles are the X values corresponding to
particular intervals on the Y axis of an empirical cumulative
distribution function. For example, for the Old Faithful duration
dataset, the 20 percentile or 0.2 quantile is about two minutes and
the 0.6 quantile is about four minutes and a bit. Q-Q plots set out
the quantiles of a sample against those of a theoretical distribution,
or against the quantiles of another sample.

For example here for the old Faithful dataset, on the X axis I have
plotted the quantiles of a normal distribution with the same mean
and standard deviation as the Old Faithful duration data. On the Y
axis are the actual quantiles of the eruption data if the two
distributions being compared or similar that these points should plot
on the line. This is clearly not the case for the eruption date, and
therefore we can conclude that the eruption data do not follow a
normal distribution.

--

This slide uses three Q Q plots to illustrate the central limit
theorem, which was previously discussed in the session about the
normal distribution. On the left is again our original Q-Q plot for
the raw Old Faithful eruption duration data. In the middle is a new
dataset derived from the Old Faithful data by taking 500 sums of two
randomly selected values from this dataset. The resulting distribution
of the sums has a trimodal frequency distribution, which produces a
Q-Q plot that is closer to the one-to-one line but doesn't quite fall
on the one-to-one one line. So it's still not coinciding with the
expected distribution of a Gaussian distribution. On the right is a
Q-Q plot for a third dataset derived again from the raw data by taking
500 sums of randomly selected values. But here the sum is not of two
values but of 10 values from the original data. This Q-Q plot plots
much closer to the one-to-one line, so it's almost indistinguishable
from a normal distribution. Only at the most extreme quantiles, both
the lower and the upper end, are there some minor deviations from the
one-to-one line. This indicates that the sample distribution has
slightly heavier tails than a normal distribution. This phenomenon
will be discussed further in a few minutes. Increasing the number
ofthe values in the some further from 2 to 10 to maybe 20 or more,
would remove this tail effect and will bring the sample
distribution even closer to the normal distribution.

---

Q-Q plots can not only be used to compare sample distributions with
theoretically predicted parametric distribution, but also to compare
one sample with another sample. So, for example, if we go back to our
session on the normal distribution and central limit theorem again, I
had these two bivariate normal distributions generated by taking the
sums of other values. Projecting these distributions onto the X axis
produces, two univariate normal distributions. These distributions
should have slightly different means (one about 50 and the other about
60). Their standard deviations may also differ, but if we plot these
two marginal distributions as a Q-Q plot, then the quantiles plot
along a straight line, confirming that these two distributions of the
sums of the X-1s and the sums of the X2s are identical in shape. So
this Q-Q plot tells us that these two variables follow a distribution
with the same shape, even though their means are visibly
different. The t-test is a method to assess whether the apparent
difference between these two means is statistically significant or
not.

---

Before we compare the means of two samples, it is useful to have a
look at the simpler case of the mean of a single sample. Here is a
dataset of the densities of five gold coins. The densities are
expressed in grammes per cubic centimetre and they vary
significantly. But they have an average of 19.164 grammes per cubic
centimetre, which is less than the density of pure gold, which is
19.30 grammes per cubic centimetre. We might ask ourselves the
question if the five coins are made of pure gold over if they consist
of a mixture of gold with a less dense metal.

---

To answer this question, we assume that the average of the data X-bar
follows a normal distribution with mean mu on standard deviation sigma
divided by the square root of n. You will recognise that this is the
standard error of the mean. Then we can define a parameter z that
follows a standard normal distribution, which is a normal distribution
with mean zero and standard deviation one. Now sigma is generally
unknown, but we can estimate sigma as s. So the sample standard
deviation (one of our summary statistics). We could then replace z
with a new formula and the new parameter t, which does not follow a
standard normal distribution, but a so called Student-t distribution
with n-1 degrees of freedom, where the n-1 plays a similar similar
role as the Bessel correction in the definition of the sample standard
deviation. The subtraction of one from the number of degrees of
freedom accounts for the fact that we use the data to estimate both
the mean and the deviation. The t-distribution forms the basis of a
statistical test that follows the same sequence of steps as all the
other statistical tests that we have seen so far.

---

For a one sided test, the null hypothesis is that the true mean of the
densities of our five coins is in fact 19.3 grammes per cubic
centimetre, and the alternative hypothesis in this case is that the
actual mean is less than 19.3 grammes per cubic centimetre. The test
statistic in this case is the t-statistic, which is calculated using
the average of the data (19.164 grammes per cubic centimetre); the
proposed mean of the densities (the proposed true density, which is
19.3 density of pure gold); the sample standard deviation of the data
using the summary statistic, which is given as 0.948 and the number of
coins n is five. Plugging these values enter this formula produces a
statistic of minus 3.2. Under the null hypothesis, the test statistic t
is expected to follow a Student-t distribution with four degrees of
freedom (5-1).

This table here tabulates some key quantiles for this distribution,
including the 1 percentile, the 2.5 percentile, the 5 percentile et
cetera. Our observed value for the t-statistic of -3.2 corresponds to
the 0.0163 quantile.This tells us that if the nul hypothesis is
correct, that there is a 1.63% probability that the t statistic would
turn out to have a value of this or less. The value of 0.163 is the
p-value for our t-test.

The rejection region, for our t test includes all the t values up to
the 0.05 quantile. That's all the values of less than -2.21 This
rejection region includes our observed value for the t-statistic of
-3.2, and therefore our no hypothesis has been rejected. Equivalently,
the p-value of 0.0163 is less stand the significance cutoff 0.05 again
leading to the rejection of our no hypothesis. So we have demonstrated
that the density of our coins is less than 19.3, or at least the
average density and, therefore, that our coins do not consist of pure
gold.

---

The same procedure is shown here graphically. The observed value for
the t-statistic is shown as dashed vertical lines, that's this value
of -3.2. Here is the probability density function of the
t-distribution with four degrees obf freedom. This looks like a normal
distribution but has heavier tails. Over here is the cumulative
distribution function. The rejection region is shown in white. The
observed value plots inside this rejection regions, so we cannot
accept our null hypothesis. The 5% cutoff for our test has shown as a
horizontal dotted line. The p-value is the intersection between the
cumulative distribution function and the observed value for the
statistic is shown as the horizontal dashed line. This plots below the
horizontal dotted line and therefore again our null hypothesis has
been rejected.

---

We have just completed a so called one sample t-test in which we
compare the average of a single sample with a particular value. We
compared the average density of five coins with the density of pure
gold. Now I'd like to move on to a two sample t-test in which we
compare the means of two samples. So we will compare the mean
densities of two collections of coins, including our original
collection of five coins and a new collection of four coins. The
density of the first collection is the same as before, it's 19.164
grammes per cubic centimetre. The second collection of coins has a
slightly higher density, at 19.275. The question is: is this difference
statistically significant?

---

In this exercise, we will carry out a two sided t-test, so we will
compare two symmetric hypotheses.  Under the null hypothesis, the true
mean of the densities of the first collection of coins and of the
second collection of coins are the same. Under the alternative
hypothesis, these two densities are different. The test statistic is
again a t-statistic, but this time it is defined in a slightly
different way. It includes the average density of the first collection
of coins, which is 19.164; the average density of the second
collection of coins, which is 19.275; the sample sizes of the first
and a second collection, so five and four coins, respectively; and
then the pooled standard deviation, which is a sort of weighted
average of the standard deviations of the first and the second
collection of coins. Plugging all those values in there gives us a
pooled standard deviation of 0.095. All those values, when entered
into this formula produce a t-statistic of -2.4.

---

Under the null hypothesis, this statistic is expected to follow a
student t-distribution with n-1 but n1+n2-2 degrees of freedom where we
subtract two degrees of freedom because we did not just estimate one
average, we had to estimate two averages from our data.


This stable shows some key quatiles for this distribution, including
the 1, 2.5 5 and other percentiles. Because we're doing a two sided
test, we do not only care about the lower tail of the distribution,
but also the upper tail of the distribution, which is shown at the
bottom here. Our observed value for t of -2.014 corresponds to a lower
quantile of 0.042 and an upper quantile of 0.985. The smallest value
of these two is 0.042. Multiplying this with two
gives us a p-value of 0.84.

---

The significance cutoff is the same as always, it's 0.05 and we are
selecting for our two sided hypothesis tests the older quantiles of
less than 0.025, which correspond to t-values of -2.4 or
smaller. Because we are doing a two sided tests, we have to consider
the upper tail as well. 0.025 or greater in the upper tail correspond
to t-values of 2.4 or greater. Note the symmetry of the
t-distribution, which produces a symmetry in the rejection regions. So
our rejection region again groups all those values for t that are less
than minus or greater than plus 2.4. Our observed value of -2.014 does
not fall in this rejection region and therefore we are not able to
reject the null hypothesis. Equivalently, the p-value, which is twice
the smallest value in our lower tail (0.84) is greater than 0.05, is
greater than the significance cutoff, so again we are unable to reject
the null hypothesis, which therefore stands.

---

Showing the same results graphically, on the left, I've got the
probability density function for a t-distribution with seven degrees
of freedom, and on the right the equivalent cumulative distribution
function. The observed t-value of -2.014 is shown on both panels as a
dashed vertical line. The rejection region for this two-sided test
consists of two parts shown in white here. Our observed value for the
statistic falls outside his rejection region, so we are unable to
reject the null hypothesis. Equivalently, the 2.5 and 97.5% marks of
our cumulative probability are marked as horizontal dotted lines. The
quantile of our ccumulative distribution function under the null
hypothesis for the observed value is shown as a horizontal dashed
line.  It plots above our 2.5% cutoff and therefore again, we are
unable to reject the null hypothesis.

---

The t-test is a powerful way to test whethere the mean of a sample
equals a particular value, or the mean of another sample. However,
formalised tests like t-test have limited practical value. Earlier, I
showed that the standard error of the mean height of adults on our
planet is just one 1.4 microns. Similarly, if the average densities is
taken of not five but five million coins, then it would be extremely
unlikely for that mean to be exactly 19.3 grammes per cubic
centimetre.  With a sample size of five million, the power of the
t-tests would be such that even trace amounts of a lighter contaminant
would have a detectable effect on the density. So instead of asking
ourselves whether the coins have the same density as gold, it is more
useful to know what their mean density actually is, in order to
construct a confidence interval for it. We could then use this
information to learn something about the actual composition of the
gold coins.

===========================
Comparing distributions (2)
===========================

In this second session on comparing distributions, I will wrap up our
discussion of the t-test and I will introduce the chi-square test as
another means to verify distributional assumptions.

---

To construct a confidence interval for the unknown sample mean mu, we
use the definition of the t-statistic in a one sample t-test. By
definition, there is a 1-alpha chance that this t-statistic falls
between the alpha/2 and 1-alpha/2 quantile of a t-distribution but the
right degrees of freedom. For example, for a 95% confidence interval,
there is a 95% chance that t falls between the 2.5 and 97.5
percentiles of a t-distribution with the correct numbers of degrees of
freedom.  We can replace t with this ratio and rearrange this equation
in terms of the unknown sample mean mu. Then we get this expression,
which has two sides.

Due to the symmetry of the t-distribution, we can replace the
1-alpha/2 quantile with 1-alpha/2 two quintile. Then this negative
symbol becomes a +, we flip these greater than symbols around and we
can rewrite this formula as a symmetric conference interval for mu,
which is centred around the arithmetic mean of the data and is
calculated as the standard error of the mean so the standard deviation
divided by the square root of the number of numbers in our sample
times a t-factor.

---

For example, for our five coin collection, the average density was
19.164 grammes per cubic centimetre. The sample standard deviation was
0.0948; we had four degrees of freedom, (5 - 1 because we had to
calculate the average) and our 2.5 percentile for the t-distribution
with four degrees of freedom is -2.776. If we plug all those values
into this formula, we get in 95% confidence region for the mean of
19.16 plus or minus 0.12 grammes per cubic centimetre. So even though
the true density of our coins is unknown, we now have a pretty good
handle on that value. Also note that this 95% confidence interval
does not include 19.3, again confirming that our coins are not made of
pure gold.

---

With increasing sample size and therefore degrees of freedom, the
t-distribution converges to a normal distribution, and its 2.5
percentile and 97.5 percentiles converge to a value of 1.962, which is
identical to the 2.5 of 97.5 percentiles of a normal distribution. So
provided that your sample sizes big enough, the 95% confidence
interval for the mean equals the arithmetic mean of our data plus or
minus approximately two times the standard error of the mean.

In the earth sciences and in science in general, it is common practise
to use these so-called 2-sigma confidence intervals for the data. That
is fine as long as you are sure that your sample size is big enough
for this normal approximation to the t-distribution to apply.

---

Here are shown on the left the probability mass functions and on the
right, the cumulative distribution functions of three t-distributions
with three different degrees of freedom. For low degrees of freedom,
the t-distribution has long tails towards low and high values. With
increasing sample size and degrees of freedom, the tails become
shorter and the t-distribution sharper. When the degrees of freedom
exceed 30 the t-distribution is indistinguishable from a standard
normal distribution with mean zero and standard deviation one. In that
case, we can safely approximate the 95% confidence intervals with a
two sigma interval.

---

Here are shown in grey and in white, on the left the probability
density functions and on the right, the cumulative distribution
functions of two normal distributions (whose means are shown by
vertical dotted lines) are offset by two standard errors from the
sample average X-bar, which is shown by vertical dashed lines. These
dotted lines mark the 95% confidence limits for the unknown sample
mean. However, this simple 2-sigma procedure again only works if the
sample size is large enough for this normal approximation to the
t-distribution to work. For sample sizes of less than 30, these
2-sigma confidence intervals must be replaced with a studentised
confidence interval, in which the number 2 is replaced with the
appropriate t-factor.

---

Comparing the means of two data sets his one way to assess their
similarity or dissimilarity. But Anscombe's data set from the
beginning of the module clearly shows that summary statistics like the
mean do not always capture the data adequately. The Chi-square test is
an alternative approach, which uses a histogram to compare the shape
of a sample distribution with theoretical distribution, or with
another sample distribution.

To illustrate this method, let us go back to an earlier example that
we studied in the session on Poisson distributions. Here, again, is
the histogram tallying the number of magnitude five or greater
earthquakes that have occurred in the United States from 1917 to
2016. This histogram represents 100 years worth of information with
values ranging from 1 to 12 events per year. Based on the similarity
of the mean and the variance of this dataset, we have previously
proceeded under the assumption that the data followed a Poisson
distribution. Now the Chi-square test will allow us to test this
assumption a bit more rigorously.

---

The first step in the Chi-square procedure is to count the number of
events in each bin of the histogram. In order for the Chi-square test
to work, none of the bins must be empty, and at least 80% of them
should contain four or more items. We can fulfil these requirements by
pooling the smallest bins. So I've grouped bins zero through two to
get a new count of 11 events, and I have have also grouped bins 10 and
up, which produces a new bin containing six events.

Next, we calculate the expected number of events per bin, using the
probability mass function of the Poisson distribution with lambda
equalling 5.43 events per year, that was the average of our entire
dataset. We can then compute the Chi-square statistic by plugging the
observed and the expected values into this formula.

And finally we compare the value that we obtained like this with a
Chi-square distribution with n-2 degrees of freedom, where 1 degree of
freedom has been removed because we forced the sum of the expected
counts to be the same as the sum of the observed counts, and another
degree of freedom was subtracted because we used the data to estimate
lambda when we predicted the expected counts.

---

The null hypothesis is that the earthquake data follow a Poisson
distribution and the alternative hypothesis is that this is not the
case. We calculate the Chi-square statistic by plugging the observed
and the expected values into this formula and that produces a value of
5.14.

We compare this value to the predicted distribution under the null
hypothesis, which is a Chi-square distribution with n-2 degrees of
freedom. We've got 9 bins, so n=9 and we have seven degrees of
freedom. This table shows some key quantiles of that chi-square
distribution: the 1, 2.5, 5 etcetera percentiles, and also the upper
tail of this distribution, which is the most relevant for our question
of interest. Because datasets that deviate from a Poisson distribution
should be characterised by high values of the Chi-square distribution,
which means the upper tail. So you want to compare the upper tail of
the Chi-square distribution with our observed value for the Chi-square
statistic of 5.14.

---

The significance level is 5% as always. For our rejection region, we
only look at the upper tell because again we're only interested in the
high end, so in the greatest differences between the Poisson
distribution and our sample distribution. The upper five percentile of
the Chi-square distribution corresponds to a Chi-square value of 14.1,
so the rejection region consists of all Chi-square statistics of the
greater than 14.1.  Our observed value of 5.14 does not fall in this
rejection region, and therefore we are unable to reject the null
hypothesis. Equivalently the p-value, which is 74.3% is greater than
our 5% cutoff; again not allowing us to reject the null
hypothesis. This does not prove that our data follow a Poisson
distribution, but we definitely did not have enough evidence to reject
that hypothesis.

---

Here the same procedure is visualised on a probability density
function on the left and a cumulative distribution function on the
right, which shows that the Chi-square distribution with seven degrees
of freedom is an asymmetric distribution (Chi-square values are not
allowed to take on negative values because of the square in the
definition). The observed value of 5.14 is shown as the vertical
dashed line. The rejection region is marked and white. We're only
interested, again, in the upper tail because that groups those
outcomes that are most dissimilar from the Poissonian predictions. Our
observed value does not fall in this rejection region. Therefore, we
are unable to reject a null hypothesis. And similarly, the quantile
for our observed outcome under the null distribution falls below the
upper 5 five percentile of our predicted cumulative distribution
function, again not allowing us to reject that null hypothesis.

---

Recall that the t-test, which we discussed earlier, could either be
used to compare the mean of a single sample with a particular value,
or to compare the means of two samples. In a similar vein, the
chi-square test can be used to compare either one sample to a
theoretical distribution (such as the Poisson distribution which we
just did), or to compare two samples with each other.

For example, let us compare two sets of petrographic observations,
tallying the number of quartz, plagioclase, alkali feldspar and lithic
clasts in a small sample of sand. This type of table is called a two
(because there are two rows) by four (because there are four columns)
contingency table. Note that the two samples (sample A and B) comprise
a different numbers of clasts.

---

If the two samples have the same underlying composition, then the
expected counts for each cell in the contingency table should be equal
to the product of the sum of the row for that particular cell, with
the sum of the column for that particular cell, divided by the sum of
all the cells.

For example, the expected number of quartz grains in the first sample
would be 41 times 35 divided by 123, which equals 11.7. This table
applies this formula to the whole set of counts.

---

The Chi-square test then proceeds in pretty much the same way as it
did for the one sample situation. The 1-sided test involves a null
hypothesis that claims that the two samples have the same composition,
and an alternative hypothesis that claims the opposite. The Chi-square
statistic is obtained by plugging the observed and predicted or
expected values into our Chi-square formula. This produces a value of
0.86 which, under the null hypothesis should belong to a Chi-square
distribution with three degrees of freedom, which equals the number of
rows minus one times the number of columns minus 1.

Some key quantiles for this nul distribution are shown in this table:
the 1 percentile, the 2.5 percentile etcetera. We also want to have a
look at the upper tail, because we are interested in the greatest
differences between the two distributions (the greatest violations of
our null hypothesis). Our observed value of 0.86 corresponds to a
p-value (the percentile under the upper tail) of 0.843.

---

The significance criterion is again 5% leading to the rejection region
corresponding to the upper five percentile of our null distribution,
and to chi square values of greater than 7.81. Our observed value of
0.86 does not fall in this rejection region. Therefore, we are unable
to reject the null hypothesis. Similarly, our p-value is 0.843 which
is much greater than 5%. And therefore, again, we are unable to reject a
null hypothesis that the two samples have the same composition.

---

Showing the procedure graphically, we've got the probability density
function and the cumulative distribution function of a Ch-square
distribution with three degrees of freedom. This is even more skewed
than the distribution with seven degrees of freedom already was. Our
observed value of 0.86 is shown as two dashed vertical lines. The
rejection region is shown in white. Our observation does not fall in
the rejection region, therefore our no hypothesis cannot be
rejected. The 95 percent mark of our distribution is marked as a
horizontal dotted line. The horizontal dashed line is the intersection
between the observed value and the cumulative distribution function
under the null hypothesis. This plots below this 95% cutoff.
Equivalently the p-value, is one minus. This number is much greater
than 5%. Therefore, again, the null hypothesis stands.

---

The Chi-square distribution only covers positive numbers from zero to
plus infinity. Low Chi-square values indicate a good fit of the data
to the proposed distribution. And high Chi-square values indicate bad
fits. This is why the Chi-square tests that we use so far were all one
tailed tests. We wanted to identify the bad fits in order to reject
the null hypothesis. However, it would be wrong to completely ignore
the good fits.

In the session on the binomial distribution and test, I made the case
that, in general, the desired outcome of a statistical test is the
rejection of the null hypothesis. However, in the context of
distributional tests, our life is often much easier if the null
hypothesis is not rejected.

For example, if the data past a Chi-square square test for a Poisson
distribution, then this allows us to model the data with a single
parameter (lambda). If the data fail the Chi-square test, then we may
have to abandon this simplicity of the Poisson distribution and use a
more realistic but more complex alternative.

The desire to see the data pass a hypothesis test leads some
scientists to cherry pick their data. This means that they selectively
remove perceived outliers from the data until the remaining values
pass the test. It is important to remember that even if the null
hypothesis is true, we should still expect roughly 5% of all the
samples to fail that null hypothesis. That is: there is always a 5%
chance of committing a type-1 error. If all the samples in a study
have a p-value of well over 5%, then this should raise suspicion.

For example, over here are shown the expected frequencies for a
Poisson distribution with parameter lambda equaling 3.5, and panels B,
C and D represent three samples which are compared to this null
distribution with their Chi-square statistic and p-value. Under the
null hypothesis, the Chi-square statistic should follow a Chi-square
distribution with five degrees of freedom, which is shown
here. Comparing this distribution with the observed values for the
Chi-square statistics tells us that sample D is significantly
different from the proposed distribution shown in A.

Sample C is somewhat different from the predictions, but the observed
difference falls within the expected range under the random
fluctuation of Poisson variables. Sample B, however, is identical to
the prediction, leading to a Chi-square value that is zero. This
should raise suspicion. It is extremely unlikely for a sample to fit
the prediction this well. This is very dodgy.

===========================
Comparing distributions (3)
===========================

In this session, we will wrap up the discussion of comparing
distributions with another look at the sample size dependence of
hypothesis tests and two non-parametric ways of comparing
distributions.

---

Earlier, I performed a two-sample Chi-square test to compare the
relative frequencies of various types of clasts in two samples of
sand. Let's carry out a similar experiment, but instead of counting a
few sedimentary grains by hands, we now task a machine to classify
about 10,000 grains of sand by image recognition. At first glance,
these two samples look very similar in composition, but let's carry
out a two-sample Chi-square test, just to be sure.

---

Our null hypothesis is once again that samples A and B have identical
compositions, and the alternative hypothesis is that they don't. Given
our 2 x 4 contingency table of observed values, we can predict the
expected values by multiplying the row sum with the column sum and
dividing by the grand total for each of these cells. That gives a new
table of predicted values. Plugging the observed and the predicted
values into the Chi-square formula gives us a chi-square statistic of
10. Under the null hypothesis, this chi-square statistic should belong
to a chi-square distribution with three degrees of freedom. Key
quantiles of this distribution are shown in this table.

We are interested in the upper tail where we see that our observed
value of 10 corresponds to the upper 1.86 percentile of the
distribution. So our p-value here is 0.0186. Significant levels of
five percent lead to a rejection region of values of 7.81 or greater,
it's a value that we also obtained earlier. This time, however, our
observed value of 10 firmly falls inside this rejection region. So we
reject the null hypothesis despite the great similarity at first
glance of these two samples.  Similarly, or equivalently, the p-value
is 0.0186. This is much smaller than the alpha cutoff. So again, we
reject a null hypothesis.

---

These two samples, despite appearing very similar at first glance,
turn out to be significantly different from a statistical
perspective.

---

I hope that by now you can see a theme in this module, in that as
sample size increases, so does our ability to detect even small
departures from the null hypothesis, we saw that if you average the
heights of five billion adult people, the standard error of these
heights is only a little bit more than one micron.

---

If you count 100,000 sand grains, then your ability to detect even
tiny differences between two samples becomes significant.

---

So this decreases the value of formalised statistical hypotheses.
Again, what we really are after is not so much whether there is a
statistically significant difference between two samples, but rather
how big that difference actually is.

---

This is exactly what the so called effect size aims to achieve. The
effect size is a measure of the degree to which the null hypothesis is
false, that is independent of the sample size.

---

Different distributions have different effects sizes, but for the
chi-square distribution, the effect size looks pretty similar to the
statistic itself. Recall that the Chi-square statistic is the sum of
the squared differences between the observed and the expected counts
divided by the expected counts. The effect size looks very
similar. It's again a sum of squared differences between observed and
expected, not counts, but proportion. So we divide the number of
counts by the grand total number of counts and we divide this by the
expected proportions.

A second difference between the statistic and the effect size is the
square root. Together, these two changes allow the effect size to
quantify the degree of disagreement with the null hypothesis
independent of sample size. So we can then categorise the effect sizes
into small, medium and large effects.

---

Applying this calculation to our huge sand counting experiment
produces this table of observed proportions and this stable of
expected proportions. Plugging these values into the formula for the
chi-square effect size yields a value of about 0.007 which indicates a
very small effect indeed, indicating that the difference between these
two samples may be statistically significant, but it is probably not
geologically significant.

---

The t-test and chi-square test make specific parametric assumptions
about the data. The t-test assumes that the population mean follows a
normal distribution. This may not be correct for small samples from
multimodal distributions. For example, when averaging two or three
values from the bimodial geyser dataset, the assumption of normality
is clearly incorrect. The Chi-square test, on the other hand, requires
binning the data into a histogram. This makes it well-suited for
discrete distribution, such as the binomial and Poisson
distributions. However, it is less well adapted to continuous
distributions such as the normal distribution. Furthermore, each bin
in the histogram requires a sufficient number of counts for the
Chi-square assumption to be valid. This requirement may not be
fulfilled for small samples. These limitations can be avoided with
non-parametric tests, which offer greater flexibility than parametric
tests while increasing their robustness to outliers.


---

The Wilcoxon test, which is also known as the Mann-Whitney test, is a
non parametric alternative to the t-test. So we will illustrate it
using the same example of coin densities that was also used to
introduce the t-test. So let's represent these this table of numbers
in a more generic form with letters A for the first sample and B for
the second sample.

---

To calculate the test statistic, we merge the two samples and we rank
them from small to large. So in this case, the smallest item is a4,
the 2nd smallest item is a1, than a2 then b3 etcetera. If the two
samples follow the same distribution then we would expect the values
to be randomly shuffled and evenly distributed on either side of the
median. However, if the two samples follow different distributions
then their values will be unevenly distributed. The test statistic is
given by the sum of the ranks of the smallest sample.

In our case, sample A contains 4 and sample B contains 4 items, so we
calculate the sum of the ranks of sample B, which are 4 plus 6 plus 7
plus 9 equals 26. For sample sizes of 5 and 4, respectively, our test
statistic w takes on values between a minimum of 10 and a maximum of
35 where the minimum corresponds to a situation in which the four
smallest items were all from sample B, and the maximum possible value
of 35 corresponds to the situation in which the largest four items all
come from sample B. The closer that our statistic is to these extremes
(to other 10 or 35) the less likely it is that samples A and B were
drawn from the same distribution. The hypothesis test is then carried
out by comparing our actual value for the statistic with a lookup
table.  To understand how this lookup table is constructed, let us
consider the smallest possible outcome for our statistic, which is 10.

---

Like I said, this outcome for the statistic of 10 corresponds to all
the arrangements in which the smallest four items in our sorted
arrangement of items all belong to sample B. The total number of
arrangements like this are the number of permutations of the first
group, which is 4!, times the number of permutations of sample A,
which is 5!, divided by the total number of permutations amongst all
nine items, which is 9!. So the probability of the statistic taking on
a value of 10 turns out to be 0.8%. The probability of other outcomes
is computed in a similar fashion.

---

The Wilcoxon test then proceeds in pretty much the same way as all the
other tests that we have looked at. In this case, the null hypothesis
is that the median of the first sample equals that of the second
sample and the alternative hypothesis in a two-sided test is that the
two medians are different.

To calculate the test statistic, we merge these two samples and we
rank them. I have marked the coins in the second collection in bold,
so you can recognise them in the merged table. We then take the sum of
the ranks where I have taken the average of the ranks where we have
duplicate values (like there's a 19.17 that appears twice and there's
a 19.31 grammes per cubic centimetre appears twice). We use the same
average rank for both values so that the sum of our ranks is 26 ...

---

... as it was in this generic example over here. That, of course, is
intentional.

---

Under the null hypothesis, the test statistics follows a Wilcoxon rank
sum distribution with n1 (the number of items in the first sample) of
five and n2 (which is a number of samples in the second collection) of
four. So here are some key values for this null distribution. We're
doing a two-sided test, so we want to have a look at both sides of the
distribution. Our observed value is 26.

---

The significance level of 5% corresponds to a rejection region that
includes the values of 10, 11, 29 and 30 for the Wilcoxon statistic.
Our observed value of the statistic of 26 does not fall in this
rejection region, and therefore we are unable to reject the null
hypothesis.

---

Visualising the same procedure graphically, on the left is the
probability mass function and on the right, the cumulative
distribution function of the Wilcoxon test statistic for a comparison
of two samples containing 5 and 4 items, respectively. The two-sided
rejection region is marked in white on the probability mass
function. The observed value of 26 is shown as a vertical dashed line
in both panels plots outside the rejection region, leaving open the
possibility that the two samples might have come from the same
distribution.

Similarly, here we've got 2.5 and 97.5 percent marks on the cumulative
distribution function. The intersection of our cumulative distribution
with the observation plots between these two lines. Again, not
allowing us to reject the null hypothesis.

---

And finally, the Kolmogorov-Smirnov test is a non parametric
alternative to the Chi-square test that does not required binning.
Given two sets of numbers X and Y, the Kolmogorov-Smirnov statistic is
defined as the maximum vertical distance between the empirical
cumulative distribution functions of the two samples. For example, if
we have two samples of detrital zircon uranium lead ages (so these are
individual sand grains that were dated with the uranium-lead method,
one sample being derived from a dune and another from a river sand)
then the maximum vertical distance between their empirical cumulative
distribution functions is 0.3. This is used as a statistic for the
Kolmorov-Smirnov test.

---

The hypothesis tests then proceeds as always: our null hypothesis is
that the two samples were drawn from the same distribution. The
alternative hypothesis claims the opposite. Our test statistic is
again this maximum vertical distance between the two empirical
cumulative distribution functions, so 0.3. Under this null hypothesis,
the Kolmogorov-Smirnov statistic can be compared to a lookup table
that is similar to the one that was used for the Wilcoxon test. This
table shows some key quantiles for this distribution, both tabulating
the left hand side and the right hand side of the distribution for the
observed value of the statistic.

We get some pretty extreme values, which is sort of giving away the
conclusion of this test already. The significance cutoff is 0.05, the
rejection region corresponding to that groups all the values of the
statistic that are greater than 0.174. Our observed statistic of 0.3
falls squarely inside this rejection region, and hence we reject that
null hypothesis.

The p-value is given by this small probability in the upper tail of
the distribution, 4.5e-5 clearly is smaller than our cutoff limit and
therefore again, our null hypothesis is rejected. The two age
distributions clearly do not share a common source.

---

And here is the graphical representation of the same procedure, with
the probability mass function for the Kolmogorov-Smirnov null
distribution on the left. This is a skewed distribution, just like the
Chi-square distribution. And here is the cumulative distribution
function, with the observed value of 0.3 shown as dashed vertical
lines; our rejection region in the upper tail of the null distribution
shown in white. Our observed value plots in this rejection region and
hence our null hypothesis is rejected.

Similarly, the 95 percentile of the distribution marks the boundary of
the rejection region on our cumulative distribution. Our observed
value plots towards the right of that, so again our null hypothesis is
firmly rejected.

---

The Kolmogorov-Smirnov test cannot only be used to compare two samples
with each other, but also to compare one sample with a theoretical
distribution. On this slide, for the sake of illustration I have
compared the dune sample, whose empirical cumulative distribution
function is shown in white, with a normal distribution shown in
grey. This normal distribution has the same mean on standard deviation
as the ages from the dune sample.

The Kolmogorov-Smirnov statistic for this particular comparison is
0.27 which we can again compare with a lookup table. But now a lookup
table for a one sample Kolmogorov-Smirnov. The outcome of this test,
which I won't discuss in detail, is again a rejection of the null
hypothesis.

##########
Regression
##########

This session will introduce the subject of linear regression, which is
the process of fitting a straight line through a set of linearly
correlated data points.

---

I will introduce the process of linear regression using a
geochronological example. Rubidium-87 is a radioactive isotope of
rubidium that decays to strontium-87 with the decay constant
lambda. Strontium-86 is a second isotope of strontium that does not
have a radioactive parent. Together, these three nuclides
(strontium-86, strontium-87 and rubidium-87 formed the basis of a
geochronometer where t is the age of the system in millions of years
and Sr87/Sr86_0 is the non-radiogenic strontium-87 to strontium-86
ratio, so the ratio that was initially present in the sample when time
was zero. When applied to multiple measurements, this formula fits the
general equation for a straight line.

Suppose that we have measured the rubidium-strontium composition in
aliquots, so in eight splits of the same sample. In each of these
splits, we have measured x_i, which is the independent variable (the
rubidium-strontium ratio) and y_i, which is the dependent variable
(the strontium-strontium ratio). Then these data define a line with an
intercept beta_0 (which is the initial strontium-strontium ratio) and
a slope beta_1 (which is a function of the age t). So we can rearrange
this equation to get t as a function of the betas.

---

Visualising the data on a bivariate scatterplot, with the dependent
variable on the y-axis and the independent variable on the x-axis,
reveals a pretty strong linear trend. The best fitting line through
this data is called an isochron line. Again, its slope allows us to
calculate the age t, which is the isochron age. The linear trend is
quite strong but it's not perfect. There is some scatter of the data
around the best fitting line, and we will need to take that scatter
into account when fitting the line. But before we do that, I would
like to define a correlation coefficient that can be used to quantify
the strength of the linear correlation and test its statistical
significance.

---

You may recall from our session on the bivariate normal distribution
that the dispersion of bivariate datasets can be captured with three
parameters: the standard deviations of the X and the Y variable and
the covariance between X and Y. Pearson's correlation coefficient for
a population of bivariate data (rho) is then defined as the ratio of
this covariance to the product of the two standard deviations. The
standard deviations and covariance of the population are unknown, but
can be estimated using the standard deviation and the covariance of a
sample of data where X-bar and Y-bar are the arithmetic means of that
sample.

Then Pearson's correlation coefficient for samples is defined as the
covariance divided by the product of the standard deviations of the
sample. Both rho (which applies to the population) and r (which
applies to sample), take on values between -1 and +1. It's also common
for the degree of correlation to be quantified as r^2, which produces
values from zero to one. r^2 is also known as the coefficient of
determination.

---

Here are shown four synthetic bivariate normal datasets exhibiting
different degrees of correlation between the dependent and independent
variables. Panel a) displays no correlation. Both the correlation
coefficient and the coefficient of determination are zero. Panel b)
shows a weak positive correlation a the correlation coefficient of
0.71 and a coefficient of determination of 0.5. Panel c) shows a
strong positive correlation, r is 0.95 and r^2 is 0.9. And Panel d
shows a strong negative correlation, r being -0.95 and r^2 again being
0.9.

I use the words 'weak' and 'strong' to describe the strength of the
correlation between the Y and the X variables in these four different
panels. Now 'weak' can 'strong' are subjective qualifiers. A more
objective evaluation is possible by observing the fact that this
statistic here (which is the product of the correlation coefficient
with the square root of the number of measurements minus two divided
by the square root of one minus the coefficient of determination),
this statistic follows a t-distribution with n-2 degrees of
freedom.

So using this statistic, we can formally test whether the apparent
correlation between the dependent and independent variables is
statistically significant.

---

So let us apply this idea to our dataset of rubidium-strontium
measurements. Our null hypothesis is that the Sr/Sr ratio is not
correlated with the Rb/Sr ratio, so our population correlation
coefficient is zero. The alternative hypothesis in this two-sided test
is that the data are correlated, so that our correlation coefficient
is not zero.

Plugging the values from this data table into our formula for this
t-statistic produces a value of nearly 14. The table here shows the
some key quantiles for a t-distribution with six degrees of freedom
(8-2). You've got the 1, 2.5, et cetera, percentiles for the lower
tail of the distribution and the upper tail of the distribution (this
is necessary because we're doing a two-sided test).

Under the null hypothesis. Our observed value of 13.98 is extremely
unlikely to happen under the distribution at six degrees of
freedom. Its p-value is two times 0.0000042.

---

The usual significance cutoff of five percent leads to a symmetric
rejection region that groups all the values for t that are less than
-2.4 and greater than +2.4. Our observed value of 13.98 squarely falls
inside this rejection region, leading to a rejection of our null
hypothesis.

Equivalently, the p-value is 0.0000084, which is far less than the
alpha cutoff again, leading to the rejection of the null hypothesis,
which leads to the conclusion that the apparent correlation between
our dependence and independent variable between our Sr/Sr and Rb/Sr is
indeed statistically significant.

---

Visualising the same procedure graphically, on the left we have the
probability density function of a t-distribution with six degrees of
freedom and on the right the cumulative distribution function. The
two-sided rejection region contains the observed value for our
t-statistic of 13.98, which is shown as vertical dashed lines.

On the right hand side, the boundaries of the rejection region are
marked by these two vertical dotted lines, which mark the 2.5 and 97.5
percentiles of our distribution. So the rejection region groups all
the values for t that fall to the left of this left line and to the
right of the second cutoff line.

Again, our observed value plots inside this rejection region, leading
to the conclusion that our apparent correlation is statistically
significant.

---

Once we have established that there is a statistically significant
correlation between the dependent and the independent variable, we can
then go ahead and fit a line through these data with an intercept
beta_0 and a slope of beta_1.

The most frequently used algorithm for doing this line is called the
method of least squares. As the name suggests, this method quantifies
the misfit of the line going through the data as the sum of the
squared residuals, where the residuals epsilon are the vertical
distances between each of the data points and their projections onto
the line.

Applying this idea to our rubidium-strontium data, we can compare the
predicted values for our dependent variables (so these are these
points on the line) as beta_0 + beta_1 times the independent
variables, and then the difference between the observed values of Y
and the predicted values of Y for a given slope and intercept are
these residuals (epsilon), which can either be negative numbers if the
observed value plots below the predicted value, or they can be
positive numbers if the observed value plots above the predicted
value.

If we square the residuals and we sum them, we get a single number
(the sum of squares) that is a positive value, and which we can
minimise in order to get the best fitting slope and intercept.

---

Here are three attempts to fit the rubidium-strontium data. On the
left is an isochron line that is clearly too steep. It has an
intercept of 0.6 and a slope of 0.3, leading to a sum of squares of
0.013.

In the middle is a line that is clearly not steep enough. It has a
slope of zero (so it's a horizontal line) and an intercept of 0.77,
which is the mean of the data. This leads to a sum of squares of 0.01,
which is less than the first panel, but still not good enough.

If we vary the intercept and the slope between the extremes defined by
the first two panels, and we iterate to minimise the sum of squares,
then we converge to a value for the intercept of 0.698 and a slope of
0.01338. The sum of squares is 3e-4. It's much better than the other
two panels, and consequently, this line fits the data much better than
the other two panels did.

---

Instead of minimising the sum of squares by iterating over all
possible values for the intercept beta_0 and the slope beta_1, we can
find the optimal solution much more quickly by taking the partial
derivatives of the sum of squares with respect to beta_0 and beta_1
and setting these partial derivatives to zero.

Solving this system of equations, we can show that the best estimates
for beta_0 and beta_1 (the estimates that minimise the sum of squares)
are given by these two equations, which combine all our measurements
for the independent and the dependent variables.

These equations involve the sums of the independent variables, the sum
of the dependent variables, the sum of the squares of the independent
variables and the sum of the products of the independent and the
dependent variables.

---

Applying these calculations to our rubidium-strontium data, here you
can see the sum of the independent variables; the sum of the dependent
variables; the squares of the independent variables and their sum over
here; the products of the dependent and the independent variables and
their sum. And these values can all be plugged into these equations
here, which give us our best fitting intercept and slope.

---

Least squares regression is just one way to determine the fit
parameters of a straight line. The method of maximum likelihood, which
we previously used to estimate parameters in the binomial, Poisson and
normal distributions is an alternative way, which I will now show is
mathematically equivalent to the method of least squares.

---

The general formulation of a linear regression problem includes the
residuals epsilon, which are the misfits between the observed and the
predicted values for our dependent variable. If we assume that these
residuals follow a normal distribution with a zero mean, then we can
define a likelihood function of our fit parameters given the data,
that is simply the product of all these normal probabilities. We can
then find the best values for beta_0 and beta_1 given the data, by
maximising this likelihood function over all possible values of beta_0
and beta_1.

---

The values of beta_0 and beta_1 that maximise the product of the
likelihoods will also be those values that maximise the sum of the
log-likelihoods. We can write out the sum of the log-likelihoods like
this, but we don't actually care what these values are. All we care
about is which values of beta_0 and beta_1 maximise whatever is
between these square brackets.

So this constant term and this constant denominator are irrelevant to
our problem and we can therefore remove them from the equation. And
then we find that the values of beta_0 and beta_1 that maximise the
product of the likelihoods are the same values that will also maximise
minus the sum of the squared residuals, which are the same as a values
of beta_0 and beta_1 that will minimise plus the sum of descried
residuals.

Now this is our least squares criterion, which demonstrates that, in
this case, the method of least squares and the method of maximum
likelihood or mathematically equivalent.

---

The method of maximum likelihood is very powerful. It allows us to not
only estimate our best fitting intercept and slope, but also to
propagate their uncertainties, to estimate their covariance matrix,
which contains the standard errors and the covariance of these fit
parameters.

The details of these derivations are provided in the notes, and I will
not go into them in this presentation for the sake of brevity and of
simplicity. However, you should know how to use this covariance matrix
of the fit parameters, for example, to construct confidence intervals
for these parameters.

This is done in a very similar way to the way in which we constructed
confidence intervals for the mean of a distribution, by multiplying
the standard errors of our parameter, which are the square root of
these diagonal elements on the covariance matrix with a t factor (a
t-factor for the appropriate cutoff region, for example, two and a
half percent for a 95 percent confidence interval and the appropriate
number of degrees of freedom, which in this case is the number of
samples in our dataset minus two, where the two accounts for the fact
that both the intercept and the slope, so the means of our data are
estimated from the data).

---

Given the covariance matrix of our fit parameters beta, we can not
only estimate the 95 percent confidence regions for beta_0 and beta_1,
but we can also propagate the uncertainty for the dependent variable
y. So in this formula for linear regression, we recognise the equation
for a sum. If we plug this covariance matrix into the error
propagation for a sum, we get the standard error of our dependent
variable Y, which is the square of the uncertainty of the first term,
plus the square of the uncertainty of the second term, plus this
covariant term. The square root of that gives us the standard error of
Y. So our predicted value and 95 percent confidence interval for that
equal our predicted value of y plus and minus this standard error
multiplied with the appropriate t-factor.

---

Evaluating the 95 percent confidence limits of our dependent variable
for the entire range of our independent variables in the
rubidium-strontium dataset produces a confidence envelope whose
curvature reflects the uncertainty that is caused by extrapolating the
data. The width of the confidence envelope is the smallest near the
average of the measurements and increases indefinitely beyond that
towards lower and higher values.

---

With increasing sample size from 8 to 100 to 1000 samples, the 95
percent confidence envelope around the best fit line reduces in width
until we can hardly see it. This is exactly the same phenomenon as
what we saw with the standard error of the mean. The standard error of
the mean heights of five billion adults is barely more than a micron,
whereas the standard deviation of the original data was 10-20
centimetres.

Similarly, the confidence bands around the best fit line for large
datasets is very small. If we want to visualise the scatter of the
data around the best fit line, then we need to augment or inflate the
standard error of the data around the fit with an estimate for the
standard deviation of the data around that fit.

The formula that estimates the standard deviation is given in the
notes, if you plug that into our equation for the confidence interval,
we get these dashed envelopes, which also reduce in size with
increasing sample size, but only to a point and never goes to zero. It
converges to the standard deviation of the data around the best fit
line.

This second confidence region is called a prediction interval as
opposed to the confidence interval for the best fit line.

---

To conclude the session on linear regression, I would like to
highlight a few common mistakes that are made in regression
analysis. The first of which is called p-hacking, and it is closely
related to the problem of multi-sample comparisons that we discussed
earlier in the session on the Poisson distribution.

When we are presented with a multivariate dataset, for example the
concentration of multiple chemical species in many samples, it is
common practise in exploratory data analysis to plot all pairs of
variables against each other in a grid of scatter plots.

When you do this, it is almost inevitable that some of these pairs
will exhibit a statistically significant correlation. So here, for
example, I've got 18 panels of random data that should not be
correlated at all. Yet one of my panels yields a coefficient of
determination of 0.79 and a highly significant p-value. This result is
simply a type one error.

---

A second issue is that of outliers. High values of the correlation
coefficient or the coefficient of determination are often taken as
evidence for correlation. However, these statistics are sensitive to
extreme values. A single outliers such as this one can have a
disproportionately large effect, suggesting a correlation where there
is none.

---

And a third issue is that of spurious correlation. Consider a
trivariate dataset X, Y and Z that is shown here in these three panels
and exhibits no correlation whatsoever. If we plot Y over Z versus X
over Z, or if we plot Z versus X over Z, then we can obtain these very
strong correlations, either positive or negative, which are entirely
spurious and have no scientific meaning whatsoever.

They are caused by the fact that we are using the same variable z in
both the vertical and the horizontal axis of our scatter
plots.

---

Spurious correlations like this are found quite commonly in the Earth
Sciences. We see it, for example, in the zirconium-yttrium versus
zirconium discrimination diagram, which allows igneous petrol adjusts
to differentiate between islands arc, mid-ocean ridge, and
within-plate basalts. But we also find spurious correlations and
geochronology. The rubidium-strontium isochron plot, for example,
contains the same denominator in both the vertical and the horizontal
axis.

This dataset consists of eight aliquots, but each of these aliquots
forms a dataset of its own. Because these measurements were made by
mass spectrometry. A mass spectrometer measures for each
aliquots these two ratios repeatedly, it calculates the mean and the
covariance matrix, and because of the spurious effect, these error
ellipses corresponding to this covariance matrix are all tilted.

---

Karl Pearson, the inventor of the correlation coefficient, showed that
it is possible to predict the expected correlation or 'null
correlation' associated with this spurious effect. So he derived this
general equation for four parameters W, X, Y and Z that gives the null
correlation as a function of the location and dispersion statistics
for the original raw variables, as well as any possible true
correlation of those raw variables.

This equation is very long and complicated, but it's got many special
cases. Let's consider one of these special cases. If we substitute Z
for Y and Y for W, we get the null correlation for the comparison of Y
over Z versus X over Z, which matches our rubidium-strontium isochron
plot.

Then the correlation between Y and Z becomes the correlation between Z
and Z, which is 1. All these other correlations here are zero. And
then this long equation simplifies to a formula that allows us to
calculate the correlation coefficient (the null correlation) of y over
z versus x over z from the uncertainties and the means of the three
original variables.

From this correlation coefficient, we can then compute or infer the
tilt of all the error ellipses for each aliquots in our dataset.

========
Fractals
========

All the previous sessions in this module introduced techniques that
are generally applicable to any field of science, although I did try
to use as many examples that are relevant to the Earth sciences as
possible. Today's session is concerned with a subject that is probably
more relevant to the Earth sciences than to any other field of
science, and that subject is the subject of fractals and chaos.

---

The United States Geological Survey, or USGS, hosts a catalog of
global earthquakes. This has several shows the 20000 most recent
earthquakes with magnitude four and a half or greater, which I
downloaded from the USGS website. This catalogue includes all the
earthquakes, so it hasn't been the clustered.

---

The distribution is positively skewed and has therefore a similar
appearance to the Ca/Mg-ratio distribution that we saw in the session
on plotting and summary statistics. Recall that the skewness of the
Ca/Mg distribution caused the mean and the median to be very
different.

---

This problem was solved by a simple logarithmic transformation. After
taking logs, the skewed distribution of Ca/Mg-ratios became
symmetric. In fact, we could apply a Chi-square or a
Kolmogorov-Smirnov test to show the log of the Ca/Mg-ratios follows a
normal distribution. This type of skewed distribution, which becomes
normal after taking logarithms, is called a log-normal
distribution. However, when we apply the same procedure to the
earthquake data, we see that the distribution of the logs of the
magnitudes is still positively skewed.

This means that the earthquake magnitudes do not follow a log-normal
distribution, and it also means that logarithms cannot fix the
discrepancy between the mean and the median. In fact, the distribution
of earthquake magnitudes does not have a well-defined mean or
median. This phenomenon is very common in geology. In our attempt to
remove this skewness of the earthquake magnitude data, we ignored the
fact that earthquake magnitudes are already a logarithmic quantity to
begin with; a logarithmic quantity that could take negative values
unlike the Ca/Mg ratio data. In fact, we will see the most seismic
events actually do have negative magnitudes. So instead of taking the
logarithm of the earthquake magnitudes, let's just take the logarithm
of the frequencies.

---

This bivariate scatterplot sets out the logarithm of the number of
earthquakes exceeding a certain magnitude normalised to the total
number of earthquakes in the entire dataset, against the magnitudes.

On this semilog plot, the data plots along a very straight line with
an intercept of 5.09 and a slope of -1.15. This linear relationship
and semilog space is called the Gutenberg-Richter Law and is a
mainstay of seismology. It is tremendously useful for predictive
purposes.

Given a record of small seismic events, the Gutenberg-Richter Law
allows accurate predictions of seismic hazards posed by larger
events.

---

For example, given that the 20000 earthquakes in the dataset spanning
a continuous period of 1031 days, we can use this straight line
equation to predict the likelihood that an earthquake of magnitude
nine or greater will happen within the next year.

First, we calculate the expected number of earthquakes per year of
magnitude 4.5 or greater, that's n_0: it's 20000 divided by a 1031
times the number of days per year equals 7080. Second, we plug our
magnitude of nine into this equation as the independent variable and
that produces a log _n over n_0 of -5.26. Third, we rearrange this
equation for n, so we take 10 to the -5.26, times n_0 (so, times 7080)
gives us a 3.9% chance of observing at least one earthquake of
magnitudes nine or greater within the next year, which is a remarkable
prediction given the fact that our dataset does not contain any
earthquakes of that magnitude.

---

These linear trends in log-log space are called power law
relationships. Those power law relationships are found in many other
fields of the geosciences, not just in seismology. For example, in
hydrology, we can study a map of central Finland where water is marked
in white and land is marked in black.

Finland is also known as the land of a thousand lakes, but in fact,
there are far more than one thousand lakes in Finland. The small area
shown in this figure here already contains 2327 of these lakes. Most
of the lakes are small, but there are also a few big ones that cover
an area of more than a thousand square kilometres.

Plotting the number of lakes exceeding a certain size against that
size on a log-log scale yields a linear array of points that look very
similar to the Gutenberg-Richter Law in seismology. Extrapolating this
trend towards the left would reveal that there are millions of small
puddles in Finland.

Other examples of similar parallel relationships in the Earth sciences
include the size-frequency distributions of faults and joints, of
class in a glacial till, of oil fields and oil deposits, of rivers and
their tributaries with mountains and floods, to name just a few.

---

In a famous paper published in the journal Science in 1967,
mathematician Benoit Mandelbrot showed that it is impossible to
unequivocally pin down the circumference of Britain. The answer
depends on the length of the measuring rod. Here are shown five
attempts to measure the length of the British coastline.

Different results are obtained depending on the length of the ruler
that is used for the measurements. The shorter the yardstick shown as
an error bar here, the longer the length of the coastline.

---

However, this seemingly complex phenomenon can be fully captured by a
simple power law equation, plotting the length of the coastline
against the size of the measuring rod on a log-log scale produces a
linear trends with a slope of -0.253. This line can be extrapolated to
lower values to estimate the length that would be measured with an
even smaller measuring rod.

---

For example, if we were to measure the British coast with a 30
centimetre long ruler, then this would produce a result of 42000
kilometres. An alternative and in fact, equivalent way to plot the
data is to divide the measured length of the coastline by the length
of the measuring rod to obtain the number of linear segments that
approximate the British coast.

Plotting this number against the length of the measuring rod on a
log-log scale produces a straight line whose slope is the slope of the
previous plot minus one.

---

And finally, a third way to obtain the power of law relationship is by
so-called box counting, which instead of approximating the British
coast with a set of line segments, covers the coast with a set of
boxes.

The left hand side of this slide shows a box counting approximation of
the British coast using a 16 by 16, a 32 by 32, a 64 by 64 and 128 by
128 grid. White squares overlap with the coastline; black squares do
not.

The legends and these sub-panels specify the number of white squares
relative to the total number of squares. Instead of plotting the
number of line segments against their size, you plot the number of
boxes against their size on a log-log scale. And this produces once
again a linear trend that has a different intercept to the power law
of the line segments, but a very similar slope.

The box counting method is more flexible and therefore more widely
used than the line segment method. For example, applying the same
technique to a river network on the Island of Corsica (off the coast
of southern France) again produces a power law relationship between
the log of the number of boxes against the log of the size of these
boxes.

The slope of this line is quite different from the slope that we
obtained for the British coastline. And it turns out that this slope
is actually quite useful, it encodes some valuable information about
the land form that we are capturing with our box counting
calculations.

So from these examples, it appears that this type of power law
relationship seems to be a law of nature.

---

Next, I will show that it is quite easy to create synthetic artificial
geometrical patterns that have the same statistical characteristics as
these natural landforms that was shown in the previous few slides. The
first synthetic pattern that I would like to introduce is the
so-called Koch curve, which is created by a simple recursive algorithm
in which a first order Koch curve is constructed by dividing a
straight line segment into three segments of equal length; placing an
equilateral triangle on top of the middle segment pointing outwards;
and removing the base of that triangle.

A second order Koch curve is derived from a first order Koch curve by
replacing each straight line segment of the first order curve by a
scaled down version of that first order curve. And a third order Koch
curve is derived from a second order Koch curve by replacing each
straight line segment in the second order curve by the scaled down
versions of a first order Koch curve.

Finally, here is the sixth or the Koch curve, which is generated by
replacing each straight line segments and a fifth order curve with a
scaled down version of again the first order curve.

This procedure can be repeated until infinity, producing an intricate
curve that is endlessly detailed. It is similar in many ways to the
British coastline, which also produces ever more detail as we zoom
into the map.

Measuring the length of a Koch curve presents the same difficulty as
measuring the length of the British coastline. The answer depends on
the length of the measuring rod.

---

Applying the box counting method to the Koch curve shows once again
that the smaller the boxes are, the greater the number that we need to
cover the entire curve. Setting out the number of boxes against the
sizes of the boxes on a log-log scale produces a best fitting power
law with a slope of -1.26, which is remarkably similar to the box
counting results for the British coastline.

The similarity of these numbers suggests that the Koch curve serves as
an artificial coastline.

---

The Koch curve is just one of many synthetic patties that produce this
type of behaviour. The Sierpinski carpet is another example. It is
generated using a recursive algorithm that is built on a grid of eight
white squares that surround a single black square. Each level of
recursion replaces the white squares with the same pattern.

Here are the first four levels of recursion of this algorithm. The end
result is an arrangement of small and large holes that bear some
resemblance to the Finnish lake example. One thing that the Koch curve
and the Sierpinski carpet have in common is their self-similarity: the
level of complexity remains the same regardless of scale.  Whether one
zooms in or out of the picture, the complexity remains the
same.

Because it consists of boxes, the Sierpinski carpet is ideally suited
for box counting. We can either cover the black areas with boxes, or
we could do the same with the white areas. And here is the resulting
plot, which has a slope of -1.89.

---

The slope of the best fitting line through the box counting results on
a log-log plot is also known as the fractal dimension of the
pattern. It is called a dimension because it has all the
characteristics of the spatial dimensions, which can be used to
characterise, for example, a point (which is a zero dimensional object);
a one dimensional straight line; a two dimensional plain or a three
dimensional cube.

But whereas the traditional notions of a dimension are tied to integer
numbers, the dimensionality of fractals is quantified by non-integer
fractions.

---

The fractal dimension of the Koch curve is 1.26. This is a number
between one, which is the dimensionality of a line and two which is
the dimensionality of a plain.

The value of 1.26 reflects the intricate shape of the curve, which
partly fills the two-dimensional plain. It is neither a line nor a
plain. It is somewhere in between.

---

Similarly, the Sierpinski copied has a fractal dimension of 1.89. This
number also falls between the dimensionality of a line and the
plane. But compared to the Koch curve, the Sierpinski carpet is more
similar to a plain than it is to a line, and consequently its fractal
dimension is closer to two than it is to one.

---

Other shapes exist that are fractal dimensions between zero and one or
between two and three. For example, the Cantor set is generated using
a recursive algorithm that is built on the line segment whose middle
third is removed. Each level of recursion replaces each white line by
the same pattern from the top to the bottom.

This figure shows the first five levels of recursion for this
algorithm. Plotting the size distribution of the Cantor set on a
log-log scale shows on the vertical axis the number of linear segments
that exceed the length shown on the horizontal axis.

The data again defines a power law whose slope indicates a fractal
dimension in this case of 0.63. This value falls between zero and one,
reflecting the fact that the Cantor set has a dimensionality that
falls somewhere between that of a point and that of a line.

---

I would now like to move on to a different subject, but a related
subject, namely that of deterministic chaos. I will introduce this
subject using a physical experiment that involves an arrangement of
three magnets and a pendulum with a metal bob that is attracted by
these magnets, with the force of attraction scaling inversely to the
square of the distance between the bob and the magnet.

So the pendulum will swing and it will slow down due to friction,
where the force of friction is proportional to the velocity of the bob
as it travels through space. We will monitor the position of the bob
as it swings back and forth before eventually coming to a standstill
above one of the three magnets.

---

Despite the simplicity of this setup, it can lead to some amazingly
complex behaviour. These figures show the three magnet configuration
in map view. The white line marks the trajectory of the bob on the
pendulum after it was pushed southward from a position towards the
northwest of the three magnets.

After describing a circular motion, it gets attracted by and is
accelerated towards the second magnet before being deviated, slowing
down, rushing towards the third magnet, being deflected towards the
first magnet, returning to the third magnet, and then coming to an
eventual standstill above the second magnet.

The second panel moves the initial position of the bob ever so
slightly to the south compared to the first panel. Initially, the
trajectory is very similar to that of the first experiment. We are
describing this circular motion towards the second magnet. We get
accelerated and then slow down. However, instead of heading towards
the third magnets, now the bob rushes towards the first magnet before
going to the third magnet back to the second magnet, to the third
magnet and coming to a standstill not above the second magnet, but
above the first magnet.

Even though our starting positions were very similar, the outcome is
very different. We can repeat this experiment along a grid of initial
positions, and we can colour code each initial position in white, grey
or black, depending on whether the final position is magnets one, two
or three, and that produces this amazing picture.

---

This picture, again, colour codes the initial positions of the
magnetic pendulum experiments according to its outcomes. White, grey
and black pixels in 512 by 512 pixel image mark the initial positions
that resulted in a final position at the first, second and third
magnet, respectively.

The resulting pattern is simple in the immediate vicinity of the
magnets, but complex at further distances. It has all the
characteristics of a fractal, exhibiting the same level of complexity,
regardless of scale. The pattern is deterministic in the sense that
the same grid of initial conditions produces exactly the same
pattern.

But it is chaotic because even tiny changes in the positions of the
magnets, or the velocity of our pendulum, may produce completely
different patterns.

---

The magnetic pendulum is just one example of a simple system of
coupled equations that produces complex outcomes. It is a geologically
relevant example because the gravitational interaction between the
planets and moons in our solar system produces similar chaotic
behaviour.

An example of this is shown here for a close encounter between the
Earth and another heavenly body that happened not long ago. The
interplay between the multitude of gravitational fields in our Solar
System is responsible for the ejection of meteors from the asteroid
belt, which have been linked to some of the largest mass extinctions
in the history of life.

Gravitational interactions also destabilise the orbital parameters of
planets such as Mars. The obliquity of Mars's spin axis is
chaotic. Its evolution can be predicted thousands of years into the
future, but becomes unpredictable over million year timescales. Rapid
changes in the obliquity of Mars have caused its polar ice caps to
shift over time.

---

Chaos theory originates from the work of atmospheric scientist Edward
Lorenz. Lorenz formulated a simplified mathematical model for
atmospheric convection based on three deterministic equations. Like
the three magnets of the pendulum example, the interactions between
the three Lorenz-equations produces outcomes that are extremely
sensitive to the initial conditions. Lorenz called this the butterfly
effect.

In the magnetic pendulum example, there were just three outcomes, but
in the real world, there are countless numbers of them. The butterfly
effect raises the theoretical possibility that the flap over
butterfly's wings in Brazil may change the initial conditions of the
global atmosphere and thereby cause a tornado in Texas.

Of course, the vast majority of butterfly wing flaps won't have this
outcome, but some of them may. The outcome is impossible to predict
far in advance. This phenomenon limits the ability of meteorologists
to forecast the weather more than 10 days in advance, and it
complicates our ability to predict how the climate and how oceanic
circulation will responds to a changing atmospheric composition.

=====================
Unsupervised learning
=====================

So far, we have been concerned with relatively simple datasets of one
or two dimensions. However, many Earth science datasets span multiple
dimensions. For example, a geochemist may have measured the
concentration of 20 elements in 50 samples, or a palaeoecologist may
have counted the relative abundances of 15 species at 30 sites. In
this session, I will introduce some tools that can help us see
structure in such big datasets without any prior knowledge of
clusters, groupings or trends. This is called unsupervised learning,
as opposed to the supervised learning algorithms that will be
introduced in the next session.

---

Principal component analysis or PCA, is an exploratory data analysis
method that takes a high dimensional dataset of inputs and produces a
lower, typically two dimensional, projection as output. PCA is closely
related to multidimensional scaling or MDS, which will be introduced
in a few minutes time. To explain the mathematical mechanism behind
PCA, let's begin with a simple toy example.

Consider a bivariate dataset of three samples, where the variables are
called A and B, and the samples are referred to as numbers 1, 2 and
3. Displaying these three points on a scatter diagram reveals that two
of the three samples close together, while the third one plots further
away.

Imagine that you live in a one dimensional world and cannot see the
spatial distribution of the three points represented by the matrix X,
then PCA allows us to visualise a two-dimensional data as a
one-dimensional array of numbers.

---

This can be achieved by decomposing X into three matrices M, P and Q,
where M is the arithmetic mean of the two data columns, P are the
principal components or scores, and Q are the so-called loadings of
the variables.

For those of you who are into linear algebray, P and Q are obtained by
an eigen-decomposition of the covariance matrix Sigma of X which,
incidentally, is also the covariance matrix of the centred data Y.

The diagonal matrix lambda contains the eigenvalues, which are
proportional to the fraction of the total variance in the data that is
accounted for by the directions defined by the eigenvectors. This may
sound a bit abstract, but hopefully a figure will clarify things.

---

Here our data matrix X is again shown as a bivariate scatterplot of
three samples in A-B space. The square represents the arithmetic mean
M and the cross is the value of M plus or minus the directions defined
by the eigenvectors (Q) stretched by the square roots (because we want
standard deviations, not variances) of lambda.

The matrix of principal components P contains the projected
coordinates of our samples onto these principal directions.  Then
these projections are shown on the right hand side of the slide.

The first principal component is obtained by projecting the three
samples onto the first principal direction. This tells us that, to a
first approximation, sample 1 is very different from samples 2 and 3,
which are identical to each other in this one-dimensional
simplification.

Principal component 2 then captures the remaining information, which
shows that samples 2 and 3 are in fact not identical, but there's a
small difference between them in the direction perpendicular to the
first principal component. In the second principal direction, sample 1
plots in between samples 2 and 3.

The matrix decomposition of principal components analysis contains
information about both the samples 1, 2 and 3 and about the variables
A and B. This information is contained in the principal components P
and in the vector loadings Q, respectively. We can graphically combine
all this information together in a so-called biplot.

---

The biplot simultaneously visualises the principal components of the
samples as labels and the loadings of the variables as arrows. The
first principal component is obtained by projecting the samples onto
the horizontal axis. It shows that the most important difference in
the dataset is that between sample 1, which is rich in component B and
poor in component A, and samples 2 and 3, which are rich in A and poor
and B. If we look at our original data, we can see that indeed 1 is
rich in B and poor in A, and 2 and 3 are rich in A and poor in B.

The second principal component captures the remaining variance, with
sample 3 being slightly richer in A and B. So these two errors are
pointing slightly towards sample 3 relative to 2. And that's indeed
what we can see here. Sample 3 has more B than sample 2 does, and it
also has more A.

Although the 1-dimensional toy example is useful for illustrating the
inner workings of principal components analysis, the true value of
this technique obviously lies in higher dimensions.

---

As a more realistic example to illustrate the usefulness of principal
components analysis, let us consider a dataset of American crime
statistics in arrests per 100,000 residents for murder, assault and
rape for each of America's 50 states. Also listed is the percentage of
the population for each of these states that live in urban areas.

This dataset contains four columns, so it's a four-dimensional
dataset, which we cannot immediately visualise so easily on a
two-dimensional scatterplot.

---

Applying principal component analysis to this dataset yields four
principal components, the first two of which represents 62 and 25% of
the total variance, respectively. So we can safely discard the third
and the fourth principal components and visualise the first two
principal components as a biplot.

Grey labels mark the different states, whereas the white vectors mark
the different crimes and the percentage of the population that lives
in urban areas. States that have a lot of crime plots on the left hand
side of the diagram. States that have little crime plot towards the
right. Heavily urbanised states plot at the bottom of the diagram,
whereas rural states plot near the top.

States that are close together on the biplot, such as Vermont and West
Virginia, or Arizona and New York, have similar crime and urbanisation
statistics. States that plot at opposite ends of the diagram, such as
Mississippi and California, or North Dakota and Florida, have different
crime and urbanisation statistics.

The vectors for murder, assault and rape all point in approximately
the same direction, towards the left hand side of the buy plot. This
tells us that these crimes are all correlated with each other. So
states that have a lot of assaults, such as Florida, also have a lot
of rape and murder. States that plots on the right hand side of the
diagram, such as North Dakota, have low crime statistics in all
categories. The vector with urban population is perpendicular to the
crime vectors. This tells us that crime and degree of urbanisation are
not correlated in the United States.

---

Multidimensional scaling, or MDS, is a multivariate ordination
technique that is similar in many ways to principal components
analysis, and I will explore those similarities in a bit more detail
in a minute. But first, I would like to introduce this technique with
an example, not an example of crime statistics, but a geographical
example.

Here is a table of pairwise road distances between European
cities. For example, the distance between Athens and Barcelona is 3313
km. The distance from Barcelona to Stockholm is 2868 km. This is a
symmetric table because the distance from Athens to Rome is the same
as a distance from Rome to Athens. It contains positive numbers, and
it has a zero diagonal.

If we plug a matrix of this form into an MDS algorithm, it will spit
out a map of Europe.

---

Here are the first two dimensions of my multidimensional scaling
fit. We can see that it roughly agrees with the map of Europe. We've
got Athens in the southeast, Stockholm in the north, and Gibraltar in
the southwest. The map of Europe seems to be rotated by about 15-20
degrees, but the distances between the different cities appear to be
correct.

We can actually measured these distances on the map, not in
kilometres, but in centimetres. And we can compare those to the input
distances from our input table here...

---

... and that produces a so-called Shepard plot. This is a scatterplot
that compares the fitted to the true distances. It gives us visual
confirmation that our two-dimensional, multidimensional scaling
configuration has adequately captured the multidimensional inputs.

The fit is not perfect, though. There is some scatter, and this
scatter simply reflects the fact that the input distances were road
distances and European roads can be quite windy. So it is impossible
to fit these windy distances with straight line distances on a two
dimensional plotting surface.

We can quantify the scatter of the fit with a parameter called the
stress. The details don't really matter. It's basically a squared
difference between the fit and the true distances, and we can then use
this stress to assess and formalise the goodness of fit from poor to
perfect. The stress in this case was seven percent, which qualifies as
a good fit to the data.

---

Let us now apply multidimensional scaling to our toy example of three
samples and two variables that was used to introduce principal
component analysis earlier on.

So first we turn this 3 x 2 table into a 3 x 3 distance matrix using
the straight line distance (the Euclidean distance). So this produces
asymmetric matrix with positive values and a zero diagonal. Plugging
this matrix into a multidimensional scaling algorithm (of which there
are many, in the simplest form, they are very similar to principal
components analysis, but they can be more sophisticated to add
flexibility to the technique). Anyway, if we enter this distance
matrix into a multidimensional scaling algorithm, it spits out a set
of coordinates, which look very similar to the principal components of
our toy example.

---

So let's have a look at our principal components again. We've got
-4.24, 2.12, 2.12, -0.71, 0.71. If we go to our multidimensional
scaling, we've got 4.24, -2.12, -2.12, -0.71, 0.71. So the values are
the same. The only difference is that all the signs of the first
multidimensional scaling dimension have been flipped compared to those
of the principal components.

So multidimensional scaling analysis is a scale invariant technique of
which the actual orientation doesn't really tell us very much. And
this is also the reason why our map of Europe is slightly rotated
compared to the true map of Europe.

In any case, our configuration of multidimensional scaling components
is identical to the biplot of our principal component analysis, except
for the fact that the horizontal axis has flipped and that we don't
have the vector loadings in our multidimensional scaling
configuration. So we are losing the information about the
variables. We lost that information when we were taking the distances
between our samples.

So compared to principal components analysis, we have lost
something. But in return, multidimensional scaling gives us much
greater flexibility to compare a much wider variety of objects than we
could analyse with principal component analysis.

---

For example, here is a dataset of 13 detrital zircon U-Pb age
spectra. This dataset was obtained by collecting 13 samples of sand
from sand dunes, from windblown dust and from rivers, and separating
the mineral zircon from that sand so it can be dated with the U-Pb
method.

If we do that for a representative number of grains anywhere between
58 and 772 grains per sample, then we can plot the ages, the
crystallisation ages of these grains as kernel density estimates where
we can identify various components at 200 Ma, 2 Ga, etc., which each
characterise a provenance area (the area from which the sound was
originally derived).

The geological question of interest here is to group these samples and
see which ones are similar to each other and which ones are
dissimilar. In order to understand something about the flow of sand
through the sediment rooting system.

Now, we can make this comparison subjectively by squinting our eyes
and looking at these kernel density estimates. But that's not very
scientific. So multidimensional scaling is one way to help us extract
geologically relevant information from this large dataset in a more
objective way.

Before we can analyse this dataset with multidimensional scaling, we
need to first somehow extract a matrix of distances between these
different samples. Those distances will, of course, not be expressed
in kilometres, they will be some form of statistical distance.

We have actually already seen a way to do so in our session on the
subject of comparing distributions.

---

In that session, I compared the detrital zircon U-Pb age distributions
of two samples of sand from a dune and from a river, using the
Kolmogorov-Smirnov statistic, which is the maximum vertical distance
between the empirical cumulative distribution functions of these two
samples.

Well, it turns out that this dune was sample 5 in the dataset that I
showed in the previous slide, and the river was sample Y; it is the
Yellow River. So we can recognise this Kolmogorov-Smirnov distance of
0.3 in this matrix. I have multiplied it with 100 to remove the
decimal places, and I have repeated this exercise for all possible
pairs of samples in my large dataset.

So this produces a 13 x 13 matrix of Kolmogorov-Smirnov distances,
which are all values greater than zero with a zero diagonal that is
symmetric around that diagonal.

---

Plugging this matrix into a multidimensional scaling algorithm
produces a map that shows not the locations of European cities, but of
sampling locations as a means of not visualising their arrangement in
physical space but in, if you will, U-Pb age spectrum space. We can
see that samples that are plot close together have similar U-Pb age
distributions. So L, Y and T should have similar age spectra. They
each have two peaks of Phanerozoic U-Pb ages and very few Precambrian
U-Pb ages.

And in contrast with that, L and 8 plot very far apart on the
multidimensional scaling map. So they should have very different U-Pb
age distribution. So let's have a look at that. L again has two young
peaks and very little going on in the Precambrian. 8 has only 1
Phanerozoic zircon peak, and it has two big Precambrian peaks.

So it appears that our multidimensional scaling map has indeed
captured the essential differences and similarities in this dataset,
in I think a much more objective manner than we could achieve by just
visually inspecting the kernel density estimates.

---

K-means clustering is an unsupervised learning algorithm that tries to
group data based on their similarity. As the name suggests, the method
requires that we pre-specify the number of clusters K that we want to
find in our dataset. Let's explore this algorithm using a simple,
two-dimensional dataset of 150 points. And let's see if we can
classify these points into three groups.

---

The first step in this algorithm is to randomly select three points
from the dataset and designate them as the centroids of the three
clusters. Let's mark these as crosses, triangles and circles. We then
reassign each data point in our dataset to the cluster, whose centroid
is the closest to it, using the straight line Euclidean distance.

---

Second, we calculate a new centroid for each cluster by averaging the
x's and the y's for the crosses, the triangles and the circles. These
new centroid positions are slightly different than the original
positions. We can then use these new centroid to reclassify all the
data points according to their distance to this new value. And we can
repeat this iterative process until the positions of the centroids
don't change anymore. And this is what is shown in this panel
here. This is the final configuration classifying each of a 150 data
points into the three clusters that are the closest to their
respective centroid.

---

The K-means algorithm can easily be generalised from two to more
dimensions, because the Euclidean distance that is used to group the
samples into clusters can easily be generalised to any number of
dimensions as well. The K-means algorithm is an unsupervised learning
algorithm. This means that it is meant to be applied to data for which
we do not know the correct classification.

However, to get an idea of the success rate of the algorithm, it is
useful to apply it to a dataset for which we do know the correct
answer. One dataset that is particularly useful for this purpose was
first introduced to statistics by Ronald Fisher. The dataset contains
the measurements, in centimetres, of the sepal length and width, plus
the petal length and width of 50 flowers from three different species
of iris.

This 4 x 4 grid of scatter plots shows the marginal distributions of
the four dimensions worth of data. We can recognise our data from the
previous bivariate example as being the panel that compares the sepal
to petal widths.

But we can then apply the K-means algorithm to the full dataset of
four dimensional data, and we can compare the clusters that each of
the flowers has been assigned to with the actual species of those
flowers, and that is summarised in this table.

We can see that all the flowers of species Setosa, which are the
circles, have been correctly classified. 48 out of 50 Versicolor
flowers, which are the plus symbols, have been correctly
classified. It's only with Virginica (the triangles, which is like an
intermediate group between these two species), that the K-means
algorithm struggles a bit: it has misclassified 14 out of 50 of these
flowers.

---

The K-means clustering algorithm requires that we pre-specify the
number of clusters. However, it may not always be obvious what this
number should be. Hierarchical clustering is an alternative approach
that builds a hierarchy from the bottom up and does not require us to
pre-specified the number of groups beforehand. Let's illustrate this
algorithm using a simple, two dimensional example consisting of five
samples and two variables which can easily be visualised on a two
dimensional scatterplot, allowing us to keep track of what is going
on.

---

The first step in the algorithm is to place each data point in its own
cluster, and calculate the distances between all the different
clusters using, for example, the Euclidean distance. We then identify
the closest two clusters (the smallest number on the off-diagonals of
this matrix), which tells us that samples 1 and 3 are the nearest to
each other, and we connect these two points. We can also visualise
these results as a tree or dendrogram.

---

Second, we calculate the distances between the remaining four
clusters, where the distance between cluster 1-3 and the other three
samples is calculated as the maximum of the distances from sample 1 to
the other three samples and of sample 3 to the other three
samples.

The smallest number, and the resulting distance matrix, is the
distance between cluster 1-3 from the previous step, and sample 4. So
we include sample 4 into our second order cluster, and we can
visualise this again as a dendrogram, in which the first order cluster
is nested inside the second order one.

---

At the beginning of our third step in the algorithm, we have three
clusters remaining. We calculate the distances between them the same
way as we did in the previous step. We identify the smallest value in
this distance matrix as indicating that samples 2 and 5 are now the
two closest clusters in our dataset. We group these together, and we
get a new dendrogram in which the third cluster does not share any
elements with the first two clusters.

---

And here are the final results of this hierarchical clustering
analysis for the toy example. The y axis has been adjusted to have
units of height, in order to show the actual distance between the
different clusters. The longer the branch, the greater the difference
between the respective clusters.

So we can see that samples 1, 3 and 4 are relatively close together,
but there is a great distance between the cluster defined by 1, 3 and
4, and the cluster defined by samples 2 and 5.

---

We can then apply the same algorithm to the iris datasets to produce a
tree with 150 leaves, each of which represents a single flower.

---

Recall that the height of the tree corresponds to the maximum possible
distance between points belonging to two different clusters. The
height changes rapidly between one and three clusters, indicating that
these correspond to the most significant bifurcations. So let's cut
down the tree at this level.

Given that we know the species of all 150 iris flowers in the dataset,
we can then again assess the performance of the hierarchical
clustering algorithm using a contingency table. This table shows that
the algorithm has done a good job classifying, Setosa and Virginica,
but struggles with Versicolor, of which has misclassified about half
of the flowers.

===================
Supervised learning
===================

This session will introduce two supervised machine learning
algorithms. In contrast with the unsupervised learning algorithms of
the previous session, which did not require prior knowledge about the
data, the supervised learning algorithms of this session use training
data to classify samples into predefined categories. Once the
classification is complete, the same decision rules can then be used
to assign a new sample of unknown affinity to one of these
categories.

The first of the two supervised learning algorithms that we will study
is discriminant analysis. This is a technique that is similar in some
ways to principal component analysis. The second technique is the
decision tree, which is similar in other ways to the hierarchical
clustering algorithm of the previous session.

---

Consider a dataset uppercase X containing a large number of
n-dimensional data were n in this bivariate example is two (we have
two variables X and Y). These data belong to one of K classes where K
and our example is three: we've got triangles, circles and crosses. We
are now trying to decide which of these classes a new sample of
unknown affinity belongs to.

Let's call this sample lowercase x. This question is answered by
Bayes' Rule. The decision D is the class G that has the highest
posterior probability given the data lowercase x. The posterior
probability can be calculated using Bayes' theorem, which we have
already discussed in the session on probability. Here, P(X|G) is the
so-called likelihood of the data given the class, and P(G) is the
prior probability of the class, which we will assume to be uniform.
So we will assume that prior to our calculations, we give the new
sample an equal chance of belonging to the first, second or the third
class. Each of these probabilities is one third.  So if we plug this
equation into this decision rule, then this decision reduces to a
simple comparison of likelihoods.

---

We then make the simplifying assumption of multivariate normality, or
in the case of our simple toy example, bivariate normality, where mu_K
and sigma_K are the means and the covariance matrices of the K-th
group, where K goes from one to three for triangles, circles and
crosses.

The decision is then, again, the group that maximises this bivariate
normal likelihood, or equivalently, it's also the group that maximises
the log of these three likelihoods and that can be written as a sum of
terms. This formula then becomes the basis of so-called quadratic
discriminant analysis.

---

The panel on the right hand side of this slide shows the curved
decision boundaries for the quadratic discriminant analysis of our toy
example. For any point that plots to the left or above this decision
line here, the likelihood that it belongs to the group of circles
exceeds its likelihood for the group of triangles and the group of
crosses.

If a sample falls in the upper right quadrangle of our discrimination
diagram, then it is most likely to belong to the group of triangles,
and less likely to belong to the circles or the crosses. And finally,
if a new sample falls in the area of the crosses, then the likelihood
for this particular group exceeds that of the other two groups.

So classifying a new sample of unknown affinity is simply a matter of
plotting it on this diagram and seeing in which field it
falls.

---

Usually the vectors mu_K and the matrices sigma_K are unknown and must
be estimated from the training data. If you make the additional
assumption that all the classes share the same covariance structure,
so all the sigma_k's equal sigma and therefore all these error
ellipses have the same size and orientation, then the decision rule
for quadratic discriminate analysis simplifies to an equation that is
linear in the data x.

This decision rule forms the basis of linear discriminant analysis,
which is so named because the decision boundaries between the
different classes become straight lines.

---

Linear discriminant analysis can also be used to reduce the
dimensionality of a dataset, in a similar way to principal component
analysis. Recall that, in the previous session, we used a simple toy
example to show how principal component analysis can be used to
project a two-dimensional dataset onto a one-dimensional line. We can
use the same approach to illustrate how linear discriminant analysis
can achieve the same effect, although with a different aim.

Consider an equal mixture of two bivariate normal datasets, then
principal component analysis is an unsupervised learning technique, so
it doesn't know which of these samples are circles and which of them
are crosses. It lumps them all together, and it extracts the major
axis of the best fitting error ellipse to the merged dataset as its
first principal components.

In contrast with this, linear discriminant analysis, which is a
supervised learning technique, does know which of the samples are
circles and which are crosses. It fits two error ellipses to the data
and extracts a function (the so-called first linear discriminant) that
maximises the distance between these two groups. In this example, this
produces a line that is perpendicular to the first principal
component.

---

Applying linear discriminant analysis to the four-dimensional iris
dataset produces four linear discriminant functions that maximise the
variance between the different species relative to the variance within
each species.

Like the first two principal components of PCA, and the first two
dimensions of MDS in the previous session, also the first two linear
discriminant functions achieve a dimension reduction. But whereas PCA
and MDS aim to visualise the total variability of the data, linear
discriminant analysis aims to highlight the predefined clusters within
the dataset, which in the case of the iris dataset, are the three
different species.

---

The first two linear discriminants of the four-dimensional iris data
represent a two-dimensional projection of these data that maximises
the differences between the three different species of flowers
contained in them. These linear discriminants are defined on the left
hand side of this slide.

Now, suppose that we have a new flower with a sepal length of six
centimetres, a sepal width of three centimetres, a petal length of
five centimetres, and a petal width of one and a half
centimetres. What species does this flower most likely belong to?

Well, we can simply plug the measurements into these two linear
discriminants, plot the results on our discrimination diagram, and
that reveals that the most likely species for the new flower is
Versicolor.

We can be more precise by explicitly calculating the posterior
probability that comes out of our Bayes' Rule, and this tells us that
the probability that the new flower belongs to species Virsicolor is
81 percent, and there is a 19 percent chance that it belongs to
Virginica.

---

Discriminant analysis is a parametric learning algorithm, which
assumes that the different classes in a dataset are grouped in
multivariate normal clusters. Here is an example of a dataset for
which this assumption is invalid. The white circles are split into two
data clouds. The black circles form a C-shape that surrounds one of
the modes of this white population. Unfortunately, discriminant
analysis is unable to handle this bimodal situation.

---

Decision trees are a non-parametric learning algorithm that is better
able to handle complex multimodal datasets like this. It uses
recursive binary partitions to describe the data, using a recursive
algorithm that approximates the data by a piecewise constant function.

The first step of this algorithm exhaustively searches all possible
split points and variables, where in our toy example here there are
two variables, but there are many split points that can be searched
and we find the one that minimises the impurity Q, where p(empty
circle) and p(full circle) are the proportions of class one and class
two in one half of the partition. This particular definition of
impurity is called the Gini Index of Diversity.

Here are three randomly selected candidate splits. The best one is the
one that minimises Q. It's a partition that splits on the variable X
at a value of -4.902. The left hand side of this first partition is
pure, so that in the second half of our tree building exercise, we
only need to evaluate the right hand side.

---

So here is that second partition.  It splits on the variable Y. That's
a value of 1.923. In contrast with the first partition, the second
partition does not produce pure nodes. So in subsequent iterations of
the recursive algorithm, we will need to further subdivide the two
halves of this second partition.

One advantage of this recursive binary partitioning scheme is that its
results can be visualised on a dendrogram that is similar to the
dendrograms that we used to visualise the unsupervised hierarchical
clustering algorithm.

---

The recursive partitioning process can be repeated until all the nodes
of the decision tree are pure, which means that they only contain
either an empty circle or a full circle. This maximum sized tree
contains 35 terminal nodes and perfectly describes the training
data. In other words, it has zero bias. However, for the purpose of
prediction, this tree is not optimal because it overfits the training
data, causing high variance.

---

One way to estimate the misclassification rate is by using a separate
set of test data whose affinities (first or second class, empty or
full circles) are known, but which were not used to train the decision
tree. Another method is the method of cross-validation. This method
works as follows.

First, we divide the training data into 10 equal parts. We remove one
of these parts and use the remaining nine to create a decision
tree. Then we enter the fraction removed into the tree and count the
number of misclassified samples in it. We then repeat this step, but
this time removing a second fraction. We repeat this process until all
10 parts have been assessed.

---

The three with optimal predictive power is smaller than the largest
possible tree and can be found by pruning the tree. We do this using a
so-called cost-complexity criterion, which balances the size of the
tree against the purity of the nodes using a tuning parameter
alpha.

You don't need to worry about the actual details of this
implementation, but the basic process is essentially such that by
varying this parameter alpha, we can create smaller sub trees of our
largest possible tree. We can then feed each of these sub trees from
small to big into the cross validation procedure, and we can calculate
the misclassification rate for all these different trees.

We can then plot this cross-validation error against the size of the
tree, or equivalently against the cross complexity criterion, and that
produces a curve that quickly drops down with increasing tree size
until a point where it starts increasing because the biggest trees are
actually not very good for predicting new samples.

We then choose a tree size near the minimum of this cross-validation
plot as our optimal tree size.

---

For our bivariate toy example, this procedure produces a tree with
four splits and five terminal nodes. This optimal tree misclassifies
only 19 of the 300 samples and the training data. The 10-fold
cross-validation error, however, is three times higher at 18
percent. But this cross-validation error is a much better estimate of
the future performance of this tree on new data than the number of
misclassified training data is for the entire dataset.

---

Like the discriminant analysis of the beginning of this session, also
decision trees can be applied to, and are most useful for, datasets
that comprise more than two dimensions. The decision tree on the
right-hand side of this slide applies the method to Fisher's iris
data, which contain four variables instead of the two for the toy
example, and three classes (or species) instead of the two of, again,
the toy example.

Even though there are four variables in the data sets, the optimal
tree actually only uses two of them. But despite this relatively small
amounts of information, the misclassification rates of the training
data is very low at four percent. Only 6 of the 150 flowers in the
training data were misclassified.

The 10-fold cross-validation error is slightly higher again, at six
percent, but still very low.

---

Let's now consider again our new flower with its sepal length of six
centimetres, sepal width of three centimetres, petal length of five
centimetres and petal width of one and a half centimetres. Which
species is the flower?

Well, the petal length of the new flower is greater than the 2.45
centimetre cut off of the first split, and its petals with is less
than the 1.75 centimetres of the second split. Therefore, the flower
ends up in the second terminal node of the tree, suggesting that it
belongs to the species Versicolor. This is the same conclusion as we
reached in the linear discriminant analysis.

============
Compositions
============

The normal distribution plays a key role in linear regression,
principal component analysis, discriminant analysis and many other
standard statistical techniques. Although Gaussian distributions are
very common in nature, it is dangerous to assume normality for all
datasets. In fact, more often than not, the normality assumption is
invalid for geological data. Ignoring this non-normality can lead to
counter-intuitive and plainly wrong results. In this session, we will
discuss the statistical treatment of compositional data, which include
the chemical compositions of rocks, minerals, water and so forth, a
data type that is very common in the Earth Sciences. We will see that
this data type cannot be safely analysed using standard statistical
techniques that are based on the normal distribution.

---

Before we get into the details of proper compositional data analysis,
we will first illustrate the dangers of blindly assuming normality by
first considering the simple case of ratios of strictly positive data,
which are also quite common in the Earth Sciences.

Take, for example, the ratio of apatite to tourmaline in heavy mineral
analysis, which has been used to indicate the duration of transport
and storage prior to deposition of sediments. Or consider the ratio of
strontium-87 to rubidium-87 in geochronology. We've already seen that
ratios can lead to spurious correlations in regression
analysis. However, in this session, I will show that even basic
summary statistics such as the mean and standard deviation can produce
counterintuitive results when they are applied to ratios. Consider two
datasets, A and B of 10 random values between zero and one.

---

Forming two sets of ratios by dividing A by B and by dividing B by A
gives rise to two new sets of 10 values.

---

Simple arithmetic dictates that the reciprocal of the ratios of A over
B should equal the ratio of B over A. So if A over B is 1.3, then 1
divided by 1.3 equals 0.78, which equals B over A. If A over B is 2.1,
then one divided by 2.1 is 0.47, which again is B over A,
etc.

---

However, if we calculate the mean of the A over B ratios, which is 1.2
and the mean of the B over A ratios, which is 2.3, then we get the
very counterintuitive result that 1 divided by the average A over B
ratio does not equal the average of the B over A ratios.

And conversely, the reciprocal of the average B over A ratio does not
equal the average of the A over B ratios.

---

The solution to the ratio averaging conundrum is to simply take
logarithms. If you calculate the logarithm of the ratios of A over B
and the logarithms of the B over A ratios and we average those, then
we get values of -0.12 and +0.12, respectively. Exponentiating these
mean log ratios produces values of 0.88 and 1.13. These are the
geometric means as opposed to the arithmetic means of the A over B and
B over A ratios.

We then find that one divide it by the geometric mean of A over B
equals the geometric mean of B over A and the reciprocal of the
geometric mean of b over A equals the geometric mean of A over B.

This is an altogether more satisfying result than what we got from the
arithmetic mean.

---

Like the ratios that we just discussed, the chemical compositions of
rocks and minerals are also expressed as strictly positive
numbers. They, however, do not span the entire range of positive
values, but are restricted to a narrow subset of that space, ranging
from zero to one if fractions are used, or from zero to 100 if we use
percentage notation.

Compositions are further restricted by a constant sum constraint,
where the sum of the concentrations of N components must add up to
100%, or to unity.  If n=3, we have a ternary system. These three
component systems can be visualised on a ternary diagram, an example
of which is shown on this slide.

Ternary diagrams are very popular in geology. Well known examples are
the Q-F-L diagram of sedimentary photography; the A-CN-K diagram that
is shown here and is used in weathering studies; and the A-F-M, Q-A-P
and Q-P-F diagrams in igneous petrology.

Treating the ternary data space as a regular Euclidean space with
Gaussian statistics can lead to very wrong results, as I will now
illustrate by further exploring this particular example here.

---

The circles on this diagram represent 20 samples, 20 measurements of
aluminium, calcium and sodium and potassium that have been
renormalised to unity to plot them on the ternary diagram. In an
attempt to summarise this dataset, we calculate the arithmetic mean of
the three variables, and this gives rise to a new vector of three
numbers, which we can plot on the ternary diagram along with the
data. And that is this white square.

Now this arithmetic mean plots outside the data cloud, so it is a
pretty meaningless value and a pretty poor summary statistic for the
location of our data.

---

A second commonly used summary statistic is the standard deviation,
which is a measure of dispersion. Let's calculate the standard
deviations of our three variables. So the standard deviation of the
aluminium oxide, of the calcium plus sodium, and of the potassium.

Then we get three new values. And if the data -if our compositions-
follow a normal distribution, then we would expect roughly 95 percent
of those data to fall in a two sigma interval around the mean.

That is shown here, so if the data follow a normal distribution, then
95% of the aluminium oxide concentrations should fall in an interval
of 0.763 (which is the arithmetic mean) plus or minus two times
0.0975, so two times the standard deviation. And we can do the same
for the calcium and sodium and for the potassium.

We can then mark the upper and lower bands of these three confidence
regions on the ternary diagram as a confidence polygon. This procedure
used to be very common in the Earth Sciences and you can still see it
occasionally in geological papers, but it can lead to nonsensical
results, where the confidence polygon partially plots outside the
ternary diagram in physically impossible negative data space.

The synthetic example shows that even the simplest summary statistics
such as the arithmetic mean and the standard deviation do not work as
expected when they are applied to compositional data. The same is true
for more advanced statistical operations that are based on the normal
distribution.

This includes linear regression, principal components analysis and
discriminant analysis. These problems had long been known to
geologists. A geologist called Felix Chayes published on them in the
1940s and in the 1960s. But a comprehensive solution to the
compositional data conundrum was not found until the 1980s by Scottish
statistician John Aitchison.

---

The solution to the compositional data problem is closely related to
the solution of the ratio averaging problem that we discussed earlier
on. The trick is to map the data from the three component ternary data
space to a bivariate Euclidean data space by means of an additive
logratio transformation.

So instead of plotting X, Y and Z, we plot the log of X/Z and log of
Y/Z. After performing our statistical analysis of choice (for example:
calculating the arithmetic mean, constructing confidence regions,
performing linear regression, etc.), we could then map the results of
our analysis back to the ternary diagram using an inverse logratio
transformation, as is shown at the bottom of the slide.

We then find that the arithmetic mean of the logratio compositions,
when mapped back to the ternary diagram, plots nicely inside our data
cloud in a much more meaningful position. The 95% confidence region
around the mean of the logratios, when we map that back to the ternary
diagram, turns into a boomerang-shaped confidence region that tightly
hugs the data, and does not spill over into physically impossible
negative space outside the ternary diagram.

So in both cases, we find that calculating the summary statistics in
logratio space produces much more sensible results than if we did it
on the raw compositional data. The logratio approach makes a lot of
intuitive sense, because even though our ternary dataset comprises
three variables, the very fact that we can plot a ternary diagram on a
two-dimensional sheet of paper or computer screen tells us that there
really are only two dimensions worth of information in the data, and
the bivariate logratio space does much more justice to this
information content than the ternary diagram does.

The logratio approach can easily be generalised from three to more
components. Consider, for example, a four component system. Such a
system no longer resides within a ternary diagram. It sits within an
odd space of tetrahedral shape. We can liberate the data from that
tetrahedral space by mapping it to a three dimensional Euclidean
space, using again a logratio transformation.

After performing the statistical analysis on the transformed data, we
can then map the results again back to our tetrahedron if we like.

---

Let us now move on to more sophisticated statistical techniques,
beginning with principal component analysis.  Let's first illustrate
this concept, again, using a simple toy example that is similar to the
toy example that we used in the session on unsupervised learning
techniques.

Consider a trivariate dataset X, where we've got three variables A, B
and C, and we've got three samples 1, 2 and 3. This is a compositional
dataset where the columns for each row add up to 100% and can be
plotted on a ternary diagram. Here I have multiplied the third
component C with 3 in order to avoid an unsightly overlap between the
two labels of samples two and three, which have very similar
compositions.

---

Unfortunately we cannot directly apply principal component analysis to
compositional data. For example, the first step in the principal
component analysis involves the calculation of an arithmetic
mean. However, earlier in this session we saw that the arithmetic mean
of compositional data can produce nonsensical results.

---

Although the additive logratio transformation solves the closure
problem, it is not suitable for principal component analysis because
the axes of alr-coordinate spaces are not orthogonal to each other.

There are six different ways to form logratios in the ternary data
space. These six logratios define three coordinate axes that intersect
each other at 60 degree angles. As a consequence, distances in
additive logratio space depend on the choice of logratios.

For example, calculate the distance between the alr-coordinates of
samples 2 and 3, first using b as a common denominator, and then using
c as a common denominator. The first distance yields a value of 1.74,
whereas the second distance is 2.46.

---

The fact that distances are not unequivocally defined in alr-space
spells trouble for principal component analysi.  Recall the
equivalence of PCA and classical multidimensional scaling, which was
discussed in the session on unsupervised learning. MDS is based on
dissimilarity matrices, and so if distances are not well defined, then
neither are the MDS configuration and, hence, the principal
components.

---

This issue can be solved by using a different type of logratio
transformation, the so-called centred logratio transformation, in
which we take the logarithm of the ratio of each component divided by
the geometric mean of all the components for each sample. For our
example, this centred logratio transformation maps our original data,
not from a 3 x 3 to a 3 x 2 matrix, but to a another 3 by 3
matrix. However, again, our data are allowed to take on any value from
minus infinity to plus infinity. So we have again liberated our data
from the confines of the ternary data space.

---

Subjecting the centred logratio transformed data to the same eigen
decomposition that was used in the toy example for the unsupervised
learning session, we can see that even though there are three
principal components instead of two, the scores of the third principal
component are all zero. This means that all the information is
contained in the first two principal components, and we can safely
ignore the third one.

---

The compositional biplot of the toy data using the centred logratio
transformation tells us that sample 1 is enriched in component B
relative to samples 2 and 3. This is consistent with our ternary
diagram, in which sample 1 is located in the corner of component B,
whereas samples 2 and 3 are richer and component A and are plotting
closer to that corner of the ternary diagram.

The difference between samples 2 and 3 is due to a small difference in
component C, which is also consistent with our observations in the
ternary diagram.

---

Let's move away from the simple toy example and have a look at a more
realistic geological example. Here is a table of 10 major oxide
compositions of 16 different samples of dune sand from the Namib Sand
Sea in Southwest Africa.

We cannot plot this multivariate dataset on a single two-dimensional
diagram. Principal component analysis is therefore the method of
choice, to project the data onto a two-dimensional subspace, hopefully
see some structure in the data, and perhaps infer the effect of the
different compositions on the different samples.

Unfortunately, we cannot directly apply principal components analysis
to these compositional data. For example, the first step in the
principal component matrix decomposition is the calculation of an
arithmetic mean. However, earlier in the session, I've shown that the
arithmetic mean of compositional data can produce nonsensical
results.

That, of course, is why we have to use the logratio transformation
-the log ratio approach- which solves this issue and allows us to
apply principal component analysis to a dataset like this.

---

And here are the results of this analysis shown as a compositional
bplot, with the samples shown in grey and the components shown in
white. Samples that plot close together, such as N1 and N14 have
similar compositions, and samples that plot far apart like N5 and N12
have different compositions. The arrows indicate that sample N8 is
enriched in Mn relative to sample T8.

The arrows for magnesium and calcium plot in the same direction,
suggesting that these two oxide concentrations are proportional to
each other. The arrow for K plots in the opposite direction,
suggesting that this oxide is anticorrelated with both Mg and Ca.

---

The so-called links between Ti, Fe, Mn, Mg and Ca are all
collinear. This tells us that the subcomposition defined by these
oxides forms a one-dimensional pattern. Consequently, the logratio of
Mn to Mg is strongly correlated with the logratio of Ti to Ca.

---

In contrast with this, the link between K and Ca is perpendicular to
that between Ti and phosphorus. This means that the corresponding
subcompositions are independent. Consequently, the logratio of
phosphorus to Ti is uncorrelated with the logratio of K to Ca.

---

To summarise what we've seen so far, we can safely use summary
statistics such as the mean and standard deviation, as well as
unsupervised learning techniques, such as PCA, to compositional data
after performing a logratio transformation. Exactly the same procedure
can be applied to supervised learning techniques, such as linear
discriminant analysis.

Let's illustrate this with an example from igneous petrology. This
ternary diagram contains over 500 igneous rock compositions from
Iceland and the Cascade Mountains on a ternary A-F-M diagram where A
stands for the alkaline metals sodium plus potassium, F stands for
iron oxide and M stands for magnesium oxide.

The black dots define a Fenner trend marking the tholeitic suite of
igneous rocks from Iceland. The white dots find a Bowen trend marking
the calc-alkaline suite of rocks from the Cascades.

---

Subjecting the data to an additive logratio transformation liberates
the data from the confines of the ternary diagram and maps them to a
Euclidean dataspace in which logratios are free to take any value from
minus infinity to plus infinity. In this space, the logratios of M/F
and A/F for the tholeitic and calc-alkaline suites are clustered into
two distinct clouds of roughly equal size that have all the hallmarks
of bivariate normal distributions with shared covariance matrix.
Thus, the transformed data seem well-suited for linear discriminant
analysis.

---

Applying linear discriminant analysis to the AFM data and visualising
the results in both logratio space and on the ternary diagram,
produces a linear decision boundary in logratio space and a curved
decision boundary in the ternary diagram, which effectively separates
the tholeitic and calc-alkaline rocks.

================
Directional data
================

Strike and dip; azimuth and elevation; latitude and longitude are all
examples of directional data, which are ubiquitous in the Earth
sciences. Just like the compositional data that were discussed in the
previous session, also the statistical analysis of directional data is
fraught with dangers. We cannot just blindly apply conventional
statistical techniques (which assume normality) and use them to
analyse directional data.

---

Consider, for example, this data set of 30 orientations of glacial
striations from Madagascar. These values span a range from 0 to 360
degrees where zero is, in fact, equal to 360 degrees. We can plot
these data on a circle. That shows that the values are roughly centred
around zero but they exhibit significant scatter from the northwest at
276 degrees to the northeast at 79 degrees.

---

Computing the arithmetic mean value of these angles by taking their
sum and dividing that sum by 30 yields a value of 189.2 degrees, which
is shown as an arrow on our circle plot. Even though the individual
striations are all pointing towards the north, the arithmetic mean is
pointing in exactly the opposite direction towards the south, which is
clearly a nonsensical results.

---

The problem is that directional data are wrapped around a circle, so
the difference between a one degree angle and a 359 degree angle is
not 358 degrees, but only two degrees. So the usual rules of
arithmetic do not apply to angular measurements. Better results for
the difference between two angles 'theta' and 'phi' are obtained using
a rearranged version of this basic triggered trigonometric identity.
If you plug the angles off 359 and a 1 degree into this formula, you
get the correct difference between the two angles of two degrees.

---

For the same reason, the arithmetic mean of a 1 degree angle and a 359
degree angle is 180 degrees, which is the opposite of what it should
be. It should be zero degrees. A more sensible definition of the mean
direction is again obtained using trigonometry, by taking the vector
sum of all the component directions.

So if we have here, for example, 1,2,3,4 angular measurements theta_i
(where i goes from 1 to 4) and theta is measured relative to the
horizontal line, then the vector sum of the four angles is shown as
this fat long white line. Its direction theta-bar is a more suitable
average of these directional measurements than the arithmetic mean.

---

Applying this formula to our data set of 30 glacial striation
measurements yields a vector mean direction of 357.6 degrees, which is
pointing towards the North, a much better results than the arithmetic
mean, which was pointing towards the South.

---

Dividing the length of the vector sum (which is shown as a dashed
white line on this slide) by the number of measurements (which is four
in this case) yields this thick white arrow whose length R-bar is a
measure of angular concentration.

---

R-bar increases with decreasing angular dispersion and vice versa. So
if the angular measurements are dispersed around the circle then R-bar
is small. If the angular measurements all point more or less in the
same direction, than R-bar is large. It takes on values between 0
(which indicates uniformly distributed angular measurement) and 1
(which is the extreme case, in which all the angular measurements
point in the same direction).

---

Alternatively, we could also create a dispersion (rather than
concentration) parameter by using R-bar to define the circular
standard deviation. Unlike R-bar the circular standard deviation does
not range from 0 to 1, but from zero to infinity, where large values
represent samples that are dispersed, and small values for the
circular standard deviation reflect samples who's measurements are
highly concentrated.

---

The normal distribution is not appropriate for directional data, for
the same reason why it did not apply to compositional data. The tails
of the normal distribution go from minus infinity to plus infinity and
does not fit within the constraints data space of angles. The von
Mises distribution, which is shown here, does not suffer from this
problem.

Like the normal distribution, also the von Mises distribution is
controlled by two parameters: a location parameter mu (which basically
represents the mean angle) and a concentration parameter kappa (which
serves a similar purpose to, but is not identical to the concentration
parameter R-bar that was introduced as a summary statistic in the
previous slides).

Here are shown four examples of von Mises distributions (with
different values for the mean mu and the concentration parameter
kappa) wrapped around a circle that is shown in grey. The probability
density is proportional to the distance from this circle to the white
line. We can see that mu controls the angular location of the bulge,
whose width is controlled by the concentration parameter kappa, which
ranges from zero to infinity.

Given some data, mu is easy to estimate using the vector sum
methodology that I introduced earlier. Kappa is not easy to estimate,
but R-bar is, as I explained in the previous slides.

---

We can then use a lookup table to convert the values of R-bar (which
go from 0 to 1) to values of kappa (which go from zero to
infinity).

---

Applying this routine to the glacial striation measurements yields an
R-bar value of 0.78, which can be converted using the lookup table to
a kappa value of 2.8. If we then combine this value with our vector
mean of 357.6 degrees, then we can evaluate the probability density of
the von Mises distribution for this data set. And that is shown here,
not wrapped around the circle before but stretched out along a linear
interval from minus pi (so from minus 108 degrees), to plus pi (to plus
180 degrees from north).

---

The principles of directional statistics can be generalised from the
one dimensional circle to a two dimensional spherical surface in three
dimensional space. The coordinates of data in this space can be
expressed either as a latitude and longitude; strike and dip; or dip
and azimuth; and they can be converted to three-dimensional Cartesian
coordinates (X,Y,Z) using these trigonometric expressions.

---

Spherical data can be visualised on a two-dimensional surface by
stereographic map projection, using either a Wulff equal angle or a
Schmidt equal area stereonet, where the Wulff net is commonly used in
structural geology and the Schmidt net is more popular in
crystallography.

In a geographical context, a stereo yet is the view that you get if
you look through a semi-sphere with north being on top and the
latitudinal lines shown a dashed small circles, the longitudal lines
as dashed great circles. The solid circles on these two stereonets
mark 10 degree cones at 10 degree and 90 degree dips and azimuths of
0, 90, 180 and 270 degrees.

As the name suggests, the Wulff equal angle net preserves the shape of
these circles, whereas the Schmidt equal area net preserves their
size.

---

And here, the Wulff and Schmidt net are used to visualise or display
the African continent. The Wulff net shows Africa in its right shape
on the Schmidt net shows its right size. Importantly, no
two-dimensional projection can achieve both of these goals at the same
time.

---

Besides visualising geographical coordinates, stereonets can also be
used (and are in fact more frequently used) to display azimuth and dip
measurements. For example, in structural geology, we can measure the
azimuth and the dip of slickensides on a fault plane, or in
palaeomagnetic studies, we can measure the declination on inclination
of the remnant magnetic field that is recorded in ancient sedimentary
beds or in igneous rocks.

So here is a data set of 10 palaeomagnetic measurements. In this case,
we use a Schmidt net to show here the first measurement by rotating
the Schmidt net by 47.9 degrees and then going down 10 -> 20 -> 28.6
degrees to visualise this first measurement.  Repeating that for all
10 measurements gives a cloud of data which can then be used for plate
tectonic reconstructions.

---

And finally, we can also visualise strike and dip measurements on a
stereonet, as is shown here for a data set of 10 fault planes, where
the circular segments mark the intersection of these fault planes with
the bottom half of a sphere, shown in equal area projection; and these
white circles mark the intersection of the poles of these planes which
are perpendicular lines to the fault planes that also intersect
the stereo net surface.

---

Averaging spherical data is done in essentially the same way as was
used for directional data. So we take the vector sum of all our
measurements. We first convert them to Cartesian coordinates (X,Y,Z)
on the sphere using these trigonometric expressions. Then we average
the Xs, the Ys and the Zs and we map them back to latitude, longitude,
strike, dip or dip and azimuth measurements using these inverse
trigonometry expressions.

---

And this slide applies this method to our palaeomagnetic data and to
the structural data, where the vector mean is shown as a white square
and a white circle here, whith the data shown in grey. You can see
that these vector means nicely capture the central part of these data
clouds.

---

It is important to make a distinction between directions such as
'north' or 'east' and orientations such as 'north–south' and
'east-west'. The glacial striations that we discussed earlier are a
clear example of directional data: on average, the striations point
towards the north, suggesting a southerly ice source. In contrast,
consider a dataset of 20 pebble orientations on a beach, which are
aligned in a NE–SW direction. Half of the measurements were recorded
as northeasterly and the other half as southwesterly. Both of angles
are equivalent. The white circle marks the vector mean, which points
in an unrepresentative northwesterly direction.

---

A better way to average orientational data is to maximise the moment
of inertia. Suppose that the circle is a ring, and that the tick marks
have a physical weight attached to them. If we were to spin this ring
around the y-axis, the it would wobble significantly. The wobble can
be eliminated by changing the spin axis to the direction that
minimises the moment of inertia of the spinning ring. This orientation
is more representative for the pebble orientations. Note that the
perpendicular direction, which maximises the moment of inertia, also
allows the ring to spin without wobbling. But it, of course, does not
represent a good average orientation.

---

The moment of inertia can be minimised by calculating the Cartesian
coordinates of the angular data (marked here as x and y, which are
given by the cosines and sines of the orientations, respectively), and
calculating their covariance matrix.

Subjecting the covariance matrix to an eigendecomposition (which is
the same procedure that is used in Principal Component Analysis)
yields two eigenvalues and eigenvectors.

The first eigenvector defines a direction theta-bar of
atan(-0.64/-0.77) = 0.70 = 40 degrees. It accounts for 98.9% of the
data variance, as indicated by the first eigenvalue.

---

The same procedure can be used to average orientational data on a
sphere. For example, consider a structural dataset for a subhorizontal
bed. Suppose that some measured surfaces dip gently to the north and
others gently to the south.

Calculating the vector mean yields a nonsensical (subvertical!)
result. This is shown as a black line and filled square. The large
open square marks the direction that minimises the moment of inertia
of the poles. This is a much more sensible orientational average.

================
Directional data
================

The spatial dimension is of crucial importance to the Earth
sciences. Geostatistics is a branch of statistics that explicitly
deals with this type of data and aims at explicitly modelling spatial
relationships between data points. Originally developed in the context
of mining geology, the field of geostatistics has applications in
petroleum geology, in environmental sciences, in oceanography,
hydrology, pathology, ecology, epidemiology and many more disciplines.

---

In this session, we will use an environmental dataset to introduce
some fundamental geostatistical concepts. So here is a table with the
locations and zinc concentrations of 155 samples of topsoil, which
were collected in a floodplain of the River Meuse near the village of
Stein in the Netherlands. X and Y are the easting and northing in
metres, using the Dutch topographical map coordinate system. The
column labelled 'Zn' contains zinc concentrations in parts per million,
measured in composite samples of a 15 by 15 metre area for each
location.

---

And here is that same dataset shown in map view. So each of these 155
points represents a sample, and the colours (the grey scales) mark the
not the zinc concentration, but the logarithm of the zinc
concentrations, which suppresses the extreme positive outliers and
produces a less skewed distribution of values.

Now, consider the position marked by this question mark, where no
measurements were taken. The question that the field of geostatistics
then addresses is: "can we use the surrounding data to estimate the
zinc concentration at this new location", or equivalently "can we
estimate the logarithm of the zinc concentrations of the surrounding
data to estimate the values in between the measurements"?

---

We will solve this problem by developing an algorithm that implements
Waldo Tobler's so-called First Law of Geography, which states that
everything is related to everything else, but near things are more
related than distant things.

For example, if you are in a hilly landscape, then you will find that
your altitude is very likely to be similar to points that are located
within a few metres of your current horizontal position and less
likely to be similar to the elevation of points that are thousands of
kilometres away from you.

---

So in the first step of our spatial interpolation algorithm, I will
begin by quantifying the spatial correlation between the data, by
first measuring the inter-sample distances between the 155 samples. So
that produces a Euclidean distance map that is very similar to the
distance matrices that we use for multidimensional scaling.

Here I am only showing the upper triangle in this matrix because the
lower triangle contains exactly the same values. The values in this
matrix are positive numbers that have units of metres.

---

Next, we compute the differences between the zinc concentrations or
the logarithm of the zinc concentrations for all the samples,
producing another 155 x 155 matrix, which contains, for example, here,
the difference of the logarithm of the zinc concentrations of sample 8
and sample 2, of sample 10 and sample 5, etc.

This is also a symmetric matrix. So here I only show the lower parts
because the upper part is exactly the same.

---

We can then merge these two triangle matrices into one big table,
where we've got the Euclidean distances in metres above the diagonal
and the log contrasts of the zinc concentrations below the
diagonal.

And then we can select all the pairs of samples that are located
within a distance of 150 metres or less. And these sample pairs are
boxed both above the diagonal and below the diagonal. We can then use
the log contrasts of these proximal samples to quantify how closely
correlated with each other they are.

---

We can do this using the so-called semivariance which, as the name
suggests, follows a similar definition to that of the variance. So
we've got the sum of the squared differences between not a sample and
its mean, but between two samples. The pairs of samples that are
located within a distance h of each other. Where h, in this case, is
150 metres or less, n(h) is the number of pairs that are within that
proximity of each other.

For the 155 sample dataset, there are 66 pairs of samples that are
within 150 metres from each other. Only nine of these pairs are shown
in the slide due to space constraints, but the sum of the squared
values of all these boxes normalised to two times the number of box
values (so 2 x 66) equals 0.1, which is the semivariance for distances
of up to 150 metres.

---

Next, we repeat this calculation by increasing h from 0-150 to 150-300
metres. There are 530 pairs of samples that are within that range of
distances from each other. They are boxed in the upper right triangle
of this matrix.

The corresponding log contrasts are also boxed below the diagonal. If
we take the sum of the squares of these 530 log contrast and we
normalised that to two times the number of pairs, then we get a
semivariance of 0.28, which is greater than the semivariance for the
interval from 0-150 metres.

And that really quantifies Tobler's First Law of Geography, which
tells us that proximal samples are more strongly correlated (have less
variability) than distant samples.

---

Continuing these calculations for progressively increasing values of
the lag h produces a so-called empirical semivariogram. The
semivariance is low at first and then gradually rises with increasing
lag before levelling off to a plateau. Because the semivariance is
inversely proportional to the degree of spatial correlation, the
semivariogram tells us that there is relatively little variability
amongst samples that are located close together and more variability
amongst samples that are far apart. In other words, the semivariogram
formalises Tobler's First Law of Geography.

---

Now that we have quantified the degree of spatial variability or,
equivalently, the degree of spatial correlation, the next step in our
spatial interpolation exercise is to parameterise the
semivariogram. There are several equations to do this, such as the
spherical, exponential and Gaussian equations. Each of these formulas
give us the semivariance gamma as a function of the lag h and three
parameters C_n, C_s and C_r, which correspond to the nugget, the sill
and the range.

---

This diagram illustrates the range, sill and nugget for a spherical
semivariogram model, but the same three parameters also control the
shape of the exponential and the Gaussian models. As the name
suggests, the range quantifies the distance within which samples are
most strongly correlated with each other and within which the most
valuable information can be gleaned for the purpose of
interpolation.

The sill quantifies the long range background variability of the
dataset, and the nugget quantifies the excess variability at a lag of
zero. Now you might think that at a distance of zero, there shouldn't
be any variability. However, in most real datasets, a nugget does
exist and this may be caused either by the analytical uncertainty or,
in the case of the Meuse dataset, it likely reflects the fact that the
zinc concentration measurements were averaged over a 15 by 15 metre
area, and the variability within these 15 by 50 metre plots is not
captured by the semivariogram.

---

So now that we have quantified and parameterised the spatial
correlation within our dataset, we can now finally move on to the
actual spatial interpolation itself. We will do so using a method
called kriging interpolation, which is so named after the South
African mining geologist Dani Gerhardus Kriege, who laid the
foundations for the method in his master's thesis.

Given n measurements of some quantity z that is measured at locations
xi and yi, where i goes from 1 to n, kriging interpolation allows us
to estimate the parameter value at a new location (x_0, y_0) by simply
taking the weighted mean of all the measurements where the weights are
a function of the semivariances between all the different samples and
the estimated semivariances at the new location, using a semivariogram
model between each of our measured locations and the new location.

---

So, for example, if we go to our location at an Easting of 179850
metres and a Northing of 331650 metres, and we fit a spherical
semivariogram model to the data sets, then we get a nugget of 0.35, a
sill of 0.64 and a range of 892 metres.

---

Then we can use this model to estimate the semivariances between the
new location and all the measured locations, as well as the
semivariances between all the different measurements in our
dataset.

If you plug all those values into this equation, then you get the
weights which add up to one. Multiplying each of these weights with
the measurements gives us a value of 5.01, which is our estimate for
the logarithm of the zinc concentration at the new location.

Exponentiating this value gives us an actual zinc concentration of 150
parts per million. Now you may wonder what this lambda number is over
here. This is the so-called Lagrange multiplier. It's a mathematical
trick that is used to solve constrained optimisation problems. You
don't need to worry about the implementation details of this.

It basically allows us to enforce a unit sum for all the 155
weights. We don't need this lambda value to estimate the zinc
concentration at our new location, but we will use it to calculate the
uncertainty for that estimate.

So that's the application of kriging interpolation to a single
location. We can repeat this exercise for multiple locations. We can
repeat it for an entire grid of locations, and we can contour that
grid, which is shown on the next slide.

---

On this slide, we see that areas where there are measurements of low
zinc concentrations are surrounded by contour values that are
reflecting low zinc concentrations, and high zinc concentration
measurements are also surrounded by contours of high zinc
concentrations. So Tobler's First Law of Geography seems to be
honoured in this contour map.

I have only contoured the area in the immediate vicinity of the actual
measurements because kriging is an interpolation method. It is not
well-suited for extrapolation.

---

Then, as promised, we could also estimate the uncertainty, the
variance of the kriging estimate for the new location as a weighted
average of the semivariances between each of the measurements in our
dataset and the new location, plus that mysterious Lagrange
multiplier. Applying that to our location at 179850 and 331650 gives
us an estimated variance of 0.22.

So this is the estimated variance of the logarithm of the zinc
concentration at location (x_0, y0_0). Now if we go back to our
session on error propagation, the variance of a logarithmic quantity
equals the square of the coefficient of variation (which is the
standard error divided by the value).

So what that tells us is that the standard error (so the square root
of the variance) of the logarithm of the zinc concentration should be
equal to the relative uncertainty of the zinc concentration, where the
zinc concentration is expressed in ppm.

For (x_0,y_0), the variance is 0.22. The quare root of that is
0.47. So that tells us that the relative uncertainty for the zinc
concentration estimate at this location is 47%.  And again, we can
repeat this exercise not for just one location, but for the whole grid
of locations that we can contour.

---

The results of that are shown over here. So we've got the coefficient
of variation, the relative uncertainty of the zinc concentrations for
the entire field area. You can see that the uncertainty estimates are
the lowest at the actual sample locations. You can see them as these
bright stars here, and then the uncertainties increase near the edges
of the area where we don't have data and in which kriging has to
extrapolate, which is something that it is not very good at.
